{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jazoza/cultural-data-analysis/blob/main/05_CDA_compare_visualize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cultural Data Analysis\n",
        "\n",
        "Introduction to working with datasets"
      ],
      "metadata": {
        "id": "7CC34uuNzNxY"
      },
      "id": "7CC34uuNzNxY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76a4e150-cc6f-4878-a32b-e78a1d6426ae",
      "metadata": {
        "id": "76a4e150-cc6f-4878-a32b-e78a1d6426ae"
      },
      "outputs": [],
      "source": [
        "# import necessary libraries\n",
        "import os, re, csv\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7010dda9-acc5-4d90-b175-90012564d13c",
      "metadata": {
        "id": "7010dda9-acc5-4d90-b175-90012564d13c"
      },
      "source": [
        "## Loading the dataset: heritage homes webistes\n",
        "\n",
        "The dataset is stored in a shared google drive:\n",
        "https://drive.google.com/drive/folders/11Shm0edDOiWrOe56fzJQRZi-v_BPSW8E?usp=drive_link\n",
        "\n",
        "Add it to your drive.\n",
        "\n",
        "To access it, load your gdrive in 'Files' (see left pane of the notebook in google colab) and navigate to the shared folder. You may need to click on 'refresh' to make it appear on the list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d42429d3-63fe-4b79-b341-160057e5dcbc",
      "metadata": {
        "id": "d42429d3-63fe-4b79-b341-160057e5dcbc"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/jazoza/cultural-data-analysis"
      ],
      "metadata": {
        "id": "QYiHwjcORrPC"
      },
      "id": "QYiHwjcORrPC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Import all datasets (4 countries)\n",
        "\n",
        "You will have all datasets available for analysis and comparison, mapped in the following way:\n",
        "\n",
        "> df0 - Dutch dataset\n",
        "\n",
        "> df1 - UK dataset\n",
        "\n",
        "> df2 - German dataset\n",
        "\n",
        "> df3 - French dataset"
      ],
      "metadata": {
        "id": "_GcR-p4NfFHw"
      },
      "id": "_GcR-p4NfFHw"
    },
    {
      "cell_type": "code",
      "source": [
        "# Country code: change here between 'NL' and 'UK'\n",
        "cc_list = ['NL', 'UK', 'DE', 'FR']"
      ],
      "metadata": {
        "id": "5JIG2eWEfWoF"
      },
      "id": "5JIG2eWEfWoF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gdrive_path = '/content/gdrive/MyDrive/CDA/'"
      ],
      "metadata": {
        "id": "bbjhZ8nKZtZC"
      },
      "id": "bbjhZ8nKZtZC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import scraped json data into 4 separate dataframes\n",
        "df0=pd.read_json(gdrive_path+cc_list[0]+'_dataset_website-content-crawler.json')\n",
        "# select columns for analysis: url, text, metadata\n",
        "df0=df0[['url','text','metadata']]\n",
        "\n",
        "df1=pd.read_json(gdrive_path+cc_list[1]+'_dataset_website-content-crawler.json')\n",
        "# select columns for analysis: url, text, metadata\n",
        "df1=df1[['url','text','metadata']]\n",
        "\n",
        "df2=pd.read_json(gdrive_path+cc_list[2]+'_dataset_website-content-crawler.json')\n",
        "# select columns for analysis: url, text, metadata\n",
        "df2=df2[['url','text','metadata']]\n",
        "\n",
        "df3=pd.read_json(gdrive_path+cc_list[3]+'_dataset_website-content-crawler.json')\n",
        "# select columns for analysis: url, text, metadata\n",
        "df3=df3[['url','text','metadata']]\n",
        "\n",
        "df0.head()"
      ],
      "metadata": {
        "id": "yCPPY_4I2pIZ"
      },
      "id": "yCPPY_4I2pIZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to extract the main domain from the url in the dataset\n",
        "def extract_main_domain(url):\n",
        "    if not isinstance(str(url), str):\n",
        "        print('NOT VALID',url)\n",
        "        return None\n",
        "    match = re.findall('(?:\\\\w+\\\\.)*\\\\w+\\\\.\\\\w*', str(url)) #'www\\.?([^/]+)'\n",
        "    return match[0].lstrip('www.') if match else None"
      ],
      "metadata": {
        "id": "txb3nj2mff6G"
      },
      "id": "txb3nj2mff6G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8697b51f-50a5-4091-9cc1-0aed1308b27d",
      "metadata": {
        "id": "8697b51f-50a5-4091-9cc1-0aed1308b27d"
      },
      "outputs": [],
      "source": [
        "# Add a new column 'domain' and fill it by applying the extract_main_domain function to the 'url' column\n",
        "\n",
        "# first, create a mapping of dataframes which could be addressed in a loop\n",
        "df_dict = {'0':df0, '1':df1, '2':df2, '3':df3}\n",
        "\n",
        "# then, loop through the df_dict to update each dataframe\n",
        "for k, v in df_dict.items():\n",
        "  cc_column = cc_list[int(k[-1])]+' domains'\n",
        "  cc = cc_list[int(k[-1])]\n",
        "  # print(cc_column, cc)\n",
        "  urls = pd.read_csv(gdrive_path+'url_lists/'+cc_list[int(k[-1])]+'_urls.csv')[cc_column].values.tolist()\n",
        "  domains = {extract_main_domain(url) for url in urls if extract_main_domain(url) is not None}\n",
        "  matching_links = [link for link in v.url if extract_main_domain(link) in domains]\n",
        "  # update the dataframe\n",
        "  v['domain'] = v['url'].apply(extract_main_domain)\n",
        "\n",
        "# check one of the dataframes\n",
        "df1.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import stopwords\n",
        "\n",
        "Import stopwords dictionaries for the 4 langauges we work with. It is good to import all of them in our case, because many websites have sections is English, German or French even when this is not the main language of the website."
      ],
      "metadata": {
        "id": "Rr6hiPQ-4z5O"
      },
      "id": "Rr6hiPQ-4z5O"
    },
    {
      "cell_type": "code",
      "source": [
        "# load a list of 'stopwords' function\n",
        "def get_stopwords_list(stop_file_path):\n",
        "    \"\"\"load stop words \"\"\"\n",
        "    with open(stop_file_path, 'r', encoding=\"utf-8\") as f:\n",
        "        stopwords = f.readlines()\n",
        "        stop_set = set(m.strip() for m in stopwords)\n",
        "        return list(frozenset(stop_set))"
      ],
      "metadata": {
        "id": "_Px9Aoim4-pq"
      },
      "id": "_Px9Aoim4-pq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47db9deb-8836-47fb-9f74-28a023bcb5d7",
      "metadata": {
        "id": "47db9deb-8836-47fb-9f74-28a023bcb5d7"
      },
      "outputs": [],
      "source": [
        "# Get the stopwords list for all languages (using cc_list previously defined)\n",
        "# cc_list = ['NL', 'UK', 'DE', 'FR'] # remove the hashtag from this line to uncomment this code and make it run\n",
        "\n",
        "stopwords = [] # empty list to which a list of stopwords will be appended in loop\n",
        "\n",
        "for i in range(len(cc_list)):\n",
        "  stopwords_cc_path = \"/content/cultural-data-analysis/stopwords_archive/\"+cc_list[i]+\".txt\"\n",
        "  stopwords_cc = get_stopwords_list(stopwords_cc_path)\n",
        "  #print(len(stopwords_cc)) # print how many words are in the list\n",
        "  stopwords.extend(stopwords_cc)\n",
        "\n",
        "#print(len(stopwords)) # print how many words are in all stopwords lists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc9d2331-fe01-4e94-b984-00ac834a771c",
      "metadata": {
        "id": "cc9d2331-fe01-4e94-b984-00ac834a771c"
      },
      "outputs": [],
      "source": [
        "# you may need to include additional words which you notice as too frequent\n",
        "special_stop_words = ['nbsp', 'nl', 'fr', 'de', 'uk', 'com', 'www', 'lit', ' '] # these might appear frequently as 'terms' in the corpus, so it's good to filter them\n",
        "stopwords_ext = stopwords+special_stop_words"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08144921-d7be-45ed-8795-1c085fb2640b",
      "metadata": {
        "id": "08144921-d7be-45ed-8795-1c085fb2640b"
      },
      "source": [
        "## 1. Visualize term frequency: bar chart"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CALCULATE TERM FREQUENCY WITHOUT STOP-WORDS\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "df = df0 # define the datraframe (df0-NL; df1-UK; df2-DE; df3-FR)\n",
        "\n",
        "cvec_stopped = CountVectorizer(stop_words=stopwords_ext, token_pattern=r'(?u)\\b[A-Za-z]{2,}\\b') # token pattern recognizes only words which are made of letters, and longer than 1 character\n",
        "cvec_stopped.fit(df.text)\n",
        "document_matrix = cvec_stopped.transform(df.text)\n",
        "term_batches = np.linspace(0,document_matrix.shape[0],10).astype(int)\n",
        "i=0\n",
        "df_stopped = []\n",
        "while i < len(term_batches)-1:\n",
        "    batch_result = np.sum(document_matrix[term_batches[i]:term_batches[i+1]].toarray(),axis=0)\n",
        "    df_stopped.append(batch_result)\n",
        "    print(term_batches[i+1],\"entries' term frequency calculated\")\n",
        "    i += 1\n",
        "\n",
        "terms_stopped = np.sum(df_stopped,axis=0)\n",
        "#print(terms_stopped.shape)\n",
        "term_freq_df_stopped = pd.DataFrame([terms_stopped],columns=cvec_stopped.get_feature_names_out()).transpose()\n",
        "term_freq_df_stopped.columns = ['terms']"
      ],
      "metadata": {
        "id": "rKDwducX7kqD"
      },
      "id": "rKDwducX7kqD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Filter and order top N words in descending order\n",
        "# change the value in .head(N) to include more or less terms\n",
        "topN = term_freq_df_stopped.sort_values(by='terms',\n",
        "                                                ascending=False).head(50)\n",
        "\n",
        "# Create the bar chart\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(topN.index, topN['terms'], color='skyblue')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Most Frequent Words (excluding stopwords)')\n",
        "plt.xticks(rotation=45, ha='right') # Rotate x-axis labels for readability\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8eX91yiEg7UE"
      },
      "id": "8eX91yiEg7UE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# Create the bar chart using seaborn\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=topN.index, y=topN['terms'], palette='viridis',\n",
        "            hue=topN.index, legend=False)\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Top 10 Most Frequent Words (excluding stopwords)')\n",
        "plt.xticks(rotation=45, ha='right') # Rotate x-axis labels for readability\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JiwCfaFFjTAJ"
      },
      "id": "JiwCfaFFjTAJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Visualize comparative frequency\n",
        "\n",
        "Compare the frequency of female- and male-connotated titles"
      ],
      "metadata": {
        "id": "S99Dd9_ml8NG"
      },
      "id": "S99Dd9_ml8NG"
    },
    {
      "cell_type": "code",
      "source": [
        "# define lists of nobility titles for each county\n",
        "# lists are defined as tupples: value pairs of title and 0/1\n",
        "# where 0 = male titles, 1 = female title\n",
        "dutch_nobility_titles = [('ridder',0), ('jonkvrouw',1),\n",
        "                        ('baron',0), ('barones',1),\n",
        "                         ('graaf',0), ('gravin',1),\n",
        "                          ('hertog',0), ('hertogin',1),\n",
        "                          ('prins',0), ('prinses',1)]\n",
        "\n",
        "uk_nobility_titles = [('sir',0), ('lady',1), ('knight',0),\n",
        "                        ('baron',0), ('baroness',1),\n",
        "                         ('duke',0), ('duchess',1),\n",
        "                          ('prince',0), ('princess',1),\n",
        "                           ('king',0), ('queen',1)]\n",
        "\n",
        "german_nobility_titles = [ ('ritter',0),\n",
        "                         ('graf',0), ('gräfin',1),\n",
        "                          ('fürst',0),('fürstin',1),\n",
        "                          ('herzog',0), ('herzogin',1),\n",
        "                           ('prinz',0), ('prinzessin',1),\n",
        "                            ('könig', 0),('königin', 1)]\n",
        "\n",
        "french_nobility_titles = [('chevalier',0), ('dame',1), ('baron',0), ('baronne',1),\n",
        "                          ('duc',0), ('duchesse',1), ('marquis',0), ('marquise',1),\n",
        "                           ('prince',0), ('princesse',1), ('roi',0), ('reine',1)]\n"
      ],
      "metadata": {
        "id": "fjampXpel7xd"
      },
      "id": "fjampXpel7xd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# change country for nobility titles\n",
        "# the list of titles should match the country dataframe:\n",
        "# df0 - dutch_nobility_titles\n",
        "# df1 - uk_nobility_titles\n",
        "# df2 - german_nobility_titles\n",
        "# df3 - french_nobility_titles\n",
        "\n",
        "nobility_titles = german_nobility_titles # define the list of nobility titles\n",
        "\n",
        "df = df2 # define the datraframe (df0-NL; df1-UK; df2-DE; df3-FR)\n",
        "\n",
        "nobility = [term for term, gender in nobility_titles]\n",
        "genders = [gender for term, gender in nobility_titles]\n",
        "\n",
        "for term in nobility:\n",
        "    df[term] = df['text'].apply(lambda x: x.lower().count(term) if isinstance(x, str) else 0)\n",
        "\n",
        "# Filter for rows where 'kasteel' appears at least once\n",
        "mask = (df[nobility] > 0).any(axis=1)\n",
        "\n",
        "# Filter the DataFrame based on the boolean mask.\n",
        "df_filtered = df[mask]\n",
        "\n",
        "# Create the five-column table\n",
        "filter_list = ['domain','url'] + nobility\n",
        "result_df = df_filtered[filter_list]\n",
        "\n",
        "# Calculate the sum of values for each specified column\n",
        "column_sums = result_df[nobility].sum()\n",
        "\n",
        "# Create a bar chart with pink bars\n",
        "fig, ax = plt.subplots(figsize=(10,6))\n",
        "colors = ['#e877f0' if gender == 0 else '#9f01aa' for gender in genders]\n",
        "\n",
        "# Add horizontal lines at 1/8ths of the vertical bars height\n",
        "max_bar_height = column_sums.values.max()\n",
        "step_size = max_bar_height / 8\n",
        "# Ensure at least one line is drawn if max_bar_height is very small\n",
        "if step_size == 0 and max_bar_height > 0:\n",
        "    y_values = [max_bar_height]\n",
        "elif step_size > 0:\n",
        "    y_values = np.arange(step_size, max_bar_height + step_size, step_size)\n",
        "else:\n",
        "    y_values = []\n",
        "\n",
        "for value in y_values:\n",
        "  plt.axhline(y=value, color='grey', linestyle='--', linewidth=0.5)\n",
        "\n",
        "plt.bar(column_sums.index, column_sums.values, color=colors)\n",
        "\n",
        "# Customize the plot\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.tight_layout()\n",
        "\n",
        "ax.set_ylabel('Sum of term frequencies')\n",
        "ax.set_title('Frequency of terms related to nobility titles in 2024 corpus heritage houses websites')\n",
        "\n",
        "# Add two columns from the df dataframe on the left\n",
        "plt.text(-0.8, -160, f\"total websites: {len(df.domain.unique())}\", fontsize=10)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pLgPPJH5mgkJ"
      },
      "id": "pLgPPJH5mgkJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Visualize term relations: scatter plot\n",
        "\n"
      ],
      "metadata": {
        "id": "sChgA3losQqC"
      },
      "id": "sChgA3losQqC"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "Cxth7eBisYC_"
      },
      "id": "Cxth7eBisYC_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "U7PIPa25sbIa"
      },
      "id": "U7PIPa25sbIa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "df = df0\n",
        "\n",
        "# X is a list of tokenized texts (i.e. list of lists of tokens)\n",
        "X = [word_tokenize(item) for item in df.text.tolist()] # replace df0 with a dataframe you are analysing\n",
        "#print(X[0:3])\n",
        "model = gensim.models.Word2Vec(X, min_count=6, vector_size=200) # min_count: how many times a word appears in the corpus; size: number of dimensions\n",
        "\n"
      ],
      "metadata": {
        "id": "OQbsQB5LsObd"
      },
      "id": "OQbsQB5LsObd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize keywords with t-SNE\n",
        "\n",
        "Visualize to N number, or choose keywords that correspond to your analysis and visualize how they and their closest terms are distributed in the discourse.\n",
        "Use t-SNE to visualize the relations.\n",
        "\n",
        "* [t-Distributed Stochastic Neighbor Embedding (t-SNE)](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) is a technique for dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets.\n"
      ],
      "metadata": {
        "id": "kPpqk0TYab3j"
      },
      "id": "kPpqk0TYab3j"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7096871"
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Get the top N words from term_freq_df_stopped\n",
        "topN_df = term_freq_df_stopped.sort_values(by='terms', ascending=False).head(1000)\n",
        "topN_words_list = topN_df.index.tolist()\n",
        "\n",
        "# Filter words that are present in the Word2Vec model's vocabulary\n",
        "filtered_top_words = [word for word in topN_words_list if word in model.wv.key_to_index]\n",
        "\n",
        "# Get the word embeddings for these filtered words\n",
        "embeddings_for_tsne = [model.wv[word] for word in filtered_top_words]\n",
        "\n",
        "# Check if there are embeddings to visualize\n",
        "if not embeddings_for_tsne:\n",
        "    print(\"No valid words found in the model's vocabulary from the top 100 words to visualize.\")\n",
        "else:\n",
        "    embeddings_array = np.array(embeddings_for_tsne)\n",
        "\n",
        "    # Apply t-SNE for Dimensionality Reduction\n",
        "    tsne_model_2d = TSNE(perplexity=15, n_components=2, init='pca', n_iter=3500, random_state=32)\n",
        "    embeddings_2d = tsne_model_2d.fit_transform(embeddings_array)\n",
        "\n",
        "    # Visualize the 2D word embeddings\n",
        "    plt.figure(figsize=(16, 9))\n",
        "    plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.7, c='blue')\n",
        "\n",
        "    for i, word in enumerate(filtered_top_words): ####\n",
        "        plt.annotate(word, alpha=0.8, xy=(embeddings_2d[i, 0], embeddings_2d[i, 1]),\n",
        "                     xytext=(5, 2), textcoords='offset points', ha='right', va='bottom', size=8)\n",
        "\n",
        "    plt.title('t-SNE Visualization of Top 100 Most Frequent Words')\n",
        "    plt.xlabel('t-SNE Dimension 1')\n",
        "    plt.ylabel('t-SNE Dimension 2')\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "id": "c7096871",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Visualise relations between specific words"
      ],
      "metadata": {
        "id": "_Q3-Dr2AtL9J"
      },
      "id": "_Q3-Dr2AtL9J"
    },
    {
      "cell_type": "code",
      "source": [
        "search_terms = ['jonkvrouw', 'ridder']   # selected words\n",
        "top_k = 20                        # how many nearest neighbors"
      ],
      "metadata": {
        "id": "IgQerdJTx8ET"
      },
      "id": "IgQerdJTx8ET",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a set of random colours\n",
        "\n",
        "import random\n",
        "\n",
        "def rgb_to_hex(rgb_string):\n",
        "    r, g, b = map(int, rgb_string[4:-1].split(\",\"))\n",
        "    return '#%02x%02x%02x' % (r, g, b)\n",
        "\n",
        "colors = {term: rgb_to_hex(\n",
        "    f\"rgb({random.randint(0,255)}, {random.randint(0,255)}, {random.randint(0,255)})\"\n",
        ") for term in search_terms}"
      ],
      "metadata": {
        "id": "KMfUhNYmzOQZ"
      },
      "id": "KMfUhNYmzOQZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Collect words & embeddings\n",
        "cluster_words = []\n",
        "cluster_colors = []\n",
        "\n",
        "for term, color in zip(search_terms, colors.values()):\n",
        "    if term not in model.wv.key_to_index:\n",
        "        print(f\"'{term}' not in vocabulary, skipping.\")\n",
        "        continue\n",
        "\n",
        "    # Get top K similar words\n",
        "    similar = model.wv.most_similar(term, topn=top_k)\n",
        "\n",
        "    # Add main terms to the two lists (words, colors)\n",
        "    cluster_words.append(term)\n",
        "    cluster_colors.append(color)\n",
        "\n",
        "    # Add similar words to the two lists (words, colors)\n",
        "    for word, sim in similar:\n",
        "        cluster_words.append(word)\n",
        "        cluster_colors.append(color)\n",
        "\n",
        "embeddings = np.array([model.wv[word] for word in cluster_words])\n",
        "\n",
        "# TSNE 2D projection (dimension reduction)\n",
        "tsne = TSNE(n_components=2, perplexity=10, random_state=42, init='pca', n_iter=3000)\n",
        "emb_2D = tsne.fit_transform(embeddings)\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(14, 9))\n",
        "\n",
        "plt.scatter(\n",
        "    emb_2D[:, 0],\n",
        "    emb_2D[:, 1],\n",
        "    c=cluster_colors,\n",
        "    alpha=0.8,\n",
        "    s=60\n",
        ")\n",
        "\n",
        "# Annotate the points\n",
        "for i, word in enumerate(cluster_words):\n",
        "    plt.annotate(\n",
        "        word,\n",
        "        (emb_2D[i, 0], emb_2D[i, 1]),\n",
        "        textcoords=\"offset points\",\n",
        "        xytext=(4, 2),\n",
        "        ha='left',\n",
        "        fontsize=9\n",
        "    )\n",
        "\n",
        "plt.title(f\"Closest Words to Search Terms {search_terms}\", fontsize=14)\n",
        "plt.xlabel(\"t-SNE Dimension 1\")\n",
        "plt.ylabel(\"t-SNE Dimension 2\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "h52kChPJ6-tv"
      },
      "id": "h52kChPJ6-tv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Topic Modeling: scatter plot\n",
        "\n",
        "Topic model produced with pre-processed corpus, to include only meaningful words"
      ],
      "metadata": {
        "id": "TF6gJ1PpgrG8"
      },
      "id": "TF6gJ1PpgrG8"
    },
    {
      "cell_type": "code",
      "source": [
        "# download the suitable language pipeline\n",
        "# Dutch: nl_core_news_sm\n",
        "# French: fr_core_news_sm\n",
        "# German: de_core_news_sm\n",
        "# English: en_core_web_sm (available by default)\n",
        "!python -m spacy download nl_core_news_sm"
      ],
      "metadata": {
        "id": "OqM4cHBm6jGk"
      },
      "id": "OqM4cHBm6jGk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "df = df0 # CHANGE TO THE df YOU ARE ANALYSING, affects next line\n",
        "nlp = spacy.load(\"nl_core_news_sm\", disable=[\"ner\", \"parser\"])\n",
        "\n",
        "# PREPROCESS: lemmatize, remove stopwords, remove punctuation, keep nouns/adjectives/verbs only, build bigrams.\n",
        "def preprocess_text(doc):\n",
        "    doc = nlp(doc.lower())\n",
        "    tokens = [\n",
        "        token.lemma_\n",
        "        for token in doc\n",
        "        if token.is_alpha\n",
        "        and not token.is_stop\n",
        "        and token.pos_ in {\"NOUN\", \"ADJ\", \"VERB\"}\n",
        "    ]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "df[\"clean_text\"] = df[\"text\"].apply(preprocess_text)\n",
        "documents = df[\"clean_text\"].tolist()\n",
        "\n",
        "# Vectorizer\n",
        "vectorizer = CountVectorizer(\n",
        "    max_df=0.80,\n",
        "    min_df=3,\n",
        "    max_features=3000,\n",
        "    ngram_range=(1, 2)\n",
        ")\n",
        "\n",
        "dtm = vectorizer.fit_transform(documents)\n",
        "features = vectorizer.get_feature_names_out()\n",
        "\n",
        "lda = LatentDirichletAllocation(\n",
        "    n_components=5,\n",
        "    max_iter=50,\n",
        "    learning_method='batch',\n",
        "    learning_decay=0.7,\n",
        "    doc_topic_prior=0.3,\n",
        "    topic_word_prior=0.3,\n",
        "    random_state=0\n",
        ")\n",
        "\n",
        "lda.fit(dtm)\n",
        "\n",
        "def display_unique_topics(model, feature_names, num_top_words):\n",
        "    used_words = set()  # store words already assigned to a topic\n",
        "\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        # sort terms by importance for the topic\n",
        "        sorted_indices = topic.argsort()[::-1]\n",
        "\n",
        "        topic_terms = []\n",
        "        for idx in sorted_indices:\n",
        "            term = feature_names[idx]\n",
        "            if term not in used_words:      # ensure uniqueness\n",
        "                topic_terms.append(term)\n",
        "                used_words.add(term)        # mark as used\n",
        "            if len(topic_terms) == num_top_words:\n",
        "                break\n",
        "\n",
        "        print(f\"\\nTopic {topic_idx}\")\n",
        "        print(\", \".join(topic_terms))\n",
        "\n",
        "# print topics\n",
        "display_unique_topics(lda, features, 10)"
      ],
      "metadata": {
        "id": "y0V0l_03ouQW"
      },
      "id": "y0V0l_03ouQW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PARAMETERS\n",
        "num_words = 10   # top N words per topic\n",
        "\n",
        "# 1. EXTRACT TOP WORDS PER TOPIC and store their first assigned topic ID\n",
        "word_to_topic_map = {}\n",
        "all_words_from_topics = [] # To maintain order for unique_words\n",
        "\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "for topic_idx, topic in enumerate(lda.components_):\n",
        "    top_indices = topic.argsort()[:-num_words - 1:-1]\n",
        "    for idx in top_indices:\n",
        "        word = feature_names[idx]\n",
        "        if word not in word_to_topic_map: # Only store the first topic_idx for a word\n",
        "            word_to_topic_map[word] = topic_idx\n",
        "        all_words_from_topics.append(word)\n",
        "\n",
        "# Ensure unique words (preserving first occurrence order)\n",
        "unique_words = []\n",
        "seen_words = set()\n",
        "for word in all_words_from_topics:\n",
        "    if word not in seen_words:\n",
        "        unique_words.append(word)\n",
        "        seen_words.add(word)\n",
        "\n",
        "# Create topic_ids list corresponding to unique_words for plotting\n",
        "topic_ids_for_plotting = [word_to_topic_map[word] for word in unique_words]\n",
        "\n",
        "# reusing frequency table from term frequency counting\n",
        "freq_lookup = term_freq_df_stopped[\"terms\"].to_dict()\n",
        "# Convert missing frequencies to 1 (small bubble)\n",
        "bubble_sizes = np.array([freq_lookup.get(w, 1) for w in unique_words]) * 20\n",
        "\n",
        "# use topic distribution over topics as embedding (from LDA)\n",
        "word_vectors = []\n",
        "# Ensure that word_vectors are built in the same order as unique_words\n",
        "for w in unique_words:\n",
        "    idx = np.where(feature_names == w)[0]\n",
        "    if len(idx) == 0:\n",
        "        # This case should ideally not happen if word is in feature_names and model.wv\n",
        "        continue\n",
        "    word_vectors.append(lda.components_[:, idx[0]])\n",
        "\n",
        "word_vectors = np.array(word_vectors)\n",
        "\n",
        "# t-SNE dimensionality reduction\n",
        "tsne = TSNE(n_components=2, perplexity=20, n_iter=2000, random_state=42)\n",
        "coords = tsne.fit_transform(word_vectors)\n",
        "\n",
        "# visualizing\n",
        "plt.figure(figsize=(14, 10))\n",
        "scatter = plt.scatter(\n",
        "    coords[:, 0], coords[:, 1],\n",
        "    s=bubble_sizes,\n",
        "    c=topic_ids_for_plotting, # Use the correctly aligned topic_ids\n",
        "    cmap=\"tab10\",\n",
        "    alpha=0.4,\n",
        "    edgecolor=None\n",
        ")\n",
        "\n",
        "# LABELS\n",
        "for i, word in enumerate(unique_words):\n",
        "    plt.annotate(\n",
        "        word,\n",
        "        (coords[i, 0], coords[i, 1]),\n",
        "        xytext=(4, 2),\n",
        "        textcoords=\"offset points\",\n",
        "        fontsize=9\n",
        "    )\n",
        "\n",
        "plt.title(\"LDA Topic Word Clusters (Bubble Size = term_freq_df_stopped Frequency)\", fontsize=14)\n",
        "plt.xlabel(\"t-SNE Dimension 1\")\n",
        "plt.ylabel(\"t-SNE Dimension 2\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_ol3ham84CK1"
      },
      "id": "_ol3ham84CK1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88b65cad"
      },
      "source": [
        "## 5. Visualize word collocation: network graph"
      ],
      "id": "88b65cad"
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "8jqz8iFKlizo"
      },
      "id": "8jqz8iFKlizo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download the suitable language pipeline\n",
        "# Dutch: nl_core_news_sm\n",
        "# French: fr_core_news_sm\n",
        "# German: de_core_news_sm\n",
        "# English: en_core_web_sm (available by default)\n",
        "!python -m spacy download nl_core_news_sm"
      ],
      "metadata": {
        "id": "rzZrWgAmmIVZ"
      },
      "id": "rzZrWgAmmIVZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('nl_core_news_sm') # change to FR/DE/EN code module, see names above"
      ],
      "metadata": {
        "id": "mZVm0X3FmI2h"
      },
      "id": "mZVm0X3FmI2h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "#function to clean and lemmatize text\n",
        "def clean_documents(text):\n",
        "    # Ensure text is a string to prevent errors with non-string inputs\n",
        "    text = str(text)\n",
        "\n",
        "    # Remove all non-alphabetic characters (incl. punctuation, numbers, etc.)\n",
        "    cleaned_text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
        "\n",
        "    # Replace multiple spaces with a single space and strip leading/trailing spaces.\n",
        "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
        "\n",
        "    # If the text becomes empty after cleaning (e.g., if it was only punctuation), return an empty list\n",
        "    if not cleaned_text:\n",
        "        return []\n",
        "\n",
        "    # lemmatize the text with spacy\n",
        "    doc = nlp(cleaned_text, disable=['parser','ner'])\n",
        "    # Get lemmas, convert to lowercase, and filter out any empty strings that might arise\n",
        "    # token.lemma_.strip() ensures no whitespace-only strings are included\n",
        "    lemma = [token.lemma_.lower() for token in doc if token.lemma_.strip()]\n",
        "    return lemma"
      ],
      "metadata": {
        "id": "lE7MoWzmmNV-"
      },
      "id": "lE7MoWzmmNV-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatize the text in .text column\n",
        "df = df0\n",
        "lemmatized = df.text.map(clean_documents) # CHANGE df0 TO THE DATAFRAME YOU ARE ANALYSING\n",
        "lemmatized.head()"
      ],
      "metadata": {
        "id": "FLbkz0ddmPUA"
      },
      "id": "FLbkz0ddmPUA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten the list of lists into one list + remove stopwords\n",
        "unlist_documents = [word for sublist in lemmatized for word in sublist if word not in stopwords_ext]"
      ],
      "metadata": {
        "id": "5fiyhgAvmRn-"
      },
      "id": "5fiyhgAvmRn-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "id": "vxwBhty_mUV4"
      },
      "id": "vxwBhty_mUV4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initiate bigrams and trigrams\n",
        "bigrams = nltk.collocations.BigramAssocMeasures()\n",
        "trigrams = nltk.collocations.TrigramAssocMeasures()"
      ],
      "metadata": {
        "id": "_r3CMycvmXLr"
      },
      "id": "_r3CMycvmXLr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# identify all collocations in the flat list of words from all documents\n",
        "bigramFinder = nltk.collocations.BigramCollocationFinder.from_words(unlist_documents)\n",
        "trigramFinder = nltk.collocations.TrigramCollocationFinder.from_words(unlist_documents)"
      ],
      "metadata": {
        "id": "aSsdo2hCmZx0"
      },
      "id": "aSsdo2hCmZx0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute basic bigrams frequency\n",
        "bigram_freq = bigramFinder.ngram_fd.items()\n",
        "bigramFreqTable = pd.DataFrame(list(bigram_freq), columns=['bigram','freq']).sort_values(by='freq', ascending=False)\n",
        "bigramFreqTable.head().reset_index(drop=True)"
      ],
      "metadata": {
        "id": "W9XCfughYim0"
      },
      "execution_count": null,
      "outputs": [],
      "id": "W9XCfughYim0"
    },
    {
      "cell_type": "code",
      "source": [
        "# compute basic trigrams frequency\n",
        "trigram_freq = trigramFinder.ngram_fd.items()\n",
        "trigramFreqTable = pd.DataFrame(list(trigram_freq), columns=['trigram','freq']).sort_values(by='freq', ascending=False)\n",
        "trigramFreqTable[:10]"
      ],
      "metadata": {
        "id": "pYTAOfQGhfY3"
      },
      "execution_count": null,
      "outputs": [],
      "id": "pYTAOfQGhfY3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fee7258d"
      },
      "source": [
        "Search for a specific term in bigram & trigram frequency table:"
      ],
      "id": "fee7258d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f75d31ba"
      },
      "source": [
        "search_term = 'kasteel'"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "f75d31ba"
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_bigrams_for_term = bigramFreqTable[bigramFreqTable['bigram'].apply(lambda x: search_term in x)]\n",
        "\n",
        "display(filtered_bigrams_for_term.head(10))"
      ],
      "metadata": {
        "id": "YjifGsX2OlGU"
      },
      "execution_count": null,
      "outputs": [],
      "id": "YjifGsX2OlGU"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a0e9c0a"
      },
      "source": [
        "filtered_trigrams_for_term = trigramFreqTable[trigramFreqTable['trigram'].apply(lambda x: search_term in x)]\n",
        "\n",
        "display(filtered_trigrams_for_term.head(10))"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "3a0e9c0a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize the network of collocations relative to a term"
      ],
      "metadata": {
        "id": "-ptGGdsWOboy"
      },
      "id": "-ptGGdsWOboy"
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "\n",
        "search_term_filtered_bi = bigramFreqTable[bigramFreqTable['bigram'].astype(str).str.contains(search_term)]\n",
        "top_10_rows = search_term_filtered_bi.sort_values('freq', ascending=False).head(10)\n",
        "\n",
        "graph = nx.Graph()\n",
        "\n",
        "# Add the keyword node\n",
        "graph.add_node(search_term)\n",
        "\n",
        "# Add nodes for the top 10 entries and connect them to the keyword\n",
        "for index, row in top_10_rows.iterrows():\n",
        "  bigram = row['bigram']\n",
        "  freq = row['freq']\n",
        "  graph.add_node(bigram)\n",
        "  graph.add_edge(search_term, bigram, weight=freq)\n",
        "\n",
        "# Get the positions of the nodes using spring layout\n",
        "pos = nx.spring_layout(graph, k=0.5, iterations=100)  # Increase iterations for better node distribution\n",
        "\n",
        "# Draw the graph with node size and edge length proportional to frequency\n",
        "nx.draw(graph, pos, with_labels=True, node_color='white', node_size=[graph.degree(node) * 100 for node in graph],\n",
        "        edge_color=[graph[u][v]['weight'] for u, v in graph.edges()],\n",
        "        width=[graph[u][v]['weight'] / 150 for u, v in graph.edges()],\n",
        "        font_size=10)\n",
        "\n",
        "plt.title(f\"{search_term} bigrams\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yiS0Pq3Kmla5"
      },
      "id": "yiS0Pq3Kmla5",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}