{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02cbadff-c1c8-481c-bf8e-2675d6613a36",
   "metadata": {},
   "source": [
    "### Humanities Data Analysis: Case studies with Python\n",
    "--------------------------------------------------\n",
    "Folgert Karsdorp, Mike Kestemont & Allen Riddell\n",
    "Chapter 2: Parsing and Manipulating Structured Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bade3d-af77-4c12-a508-e6227aa78835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "tf = tarfile.open('data/folger.tar.gz', 'r')\n",
    "tf.extractall('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893c96d7-aff2-46fa-8891-09c2d20c8cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/folger/txt/1H4.txt'\n",
    "stream = open(file_path)\n",
    "contents = stream.read()\n",
    "stream.close()\n",
    "\n",
    "print(contents[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d092a6e-6945-4e1c-8e21-3f0381565330",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path) as stream:\n",
    "    contents = stream.read()\n",
    "\n",
    "print(contents[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fd274f-12d2-49f6-a535-df7719490191",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/anna-karenina.txt', encoding='koi8-r') as stream:\n",
    "    # Use stream.readline() to retrieve the next line from a file,\n",
    "    # in this case the 1st one:\n",
    "    line = stream.readline()\n",
    "\n",
    "print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bf9363-a1fe-4375-8a29-2fd6eb97fca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = 'data/folger_shakespeare_collection.csv'\n",
    "with open(csv_file) as stream:\n",
    "    # call stream.readlines() to read all lines in the CSV file as a list.\n",
    "    lines = stream.readlines()\n",
    "\n",
    "print(lines[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975d3ae4-2315-4d29-b76b-b54fbee2c231",
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = []\n",
    "for line in open(csv_file):\n",
    "    entries.append(line.strip().split(','))\n",
    "\n",
    "for entry in entries[:3]:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7757a3-ae23-4591-b5d2-c0fb3694eb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "entries = []\n",
    "with open(csv_file) as stream:\n",
    "    reader = csv.reader(stream, delimiter=',')\n",
    "    for fname, author, title, editor, publisher, pubplace, date in reader:\n",
    "        entries.append((fname, title))\n",
    "\n",
    "for entry in entries[:5]:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d661257-44c6-406b-b23d-6c0225f983ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = []\n",
    "with open(csv_file) as stream:\n",
    "    reader = csv.reader(stream, delimiter=',')\n",
    "    for fname, _, title, *_ in reader:\n",
    "        entries.append((fname, title))\n",
    "\n",
    "for entry in entries[:5]:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64df8800-30ae-40bf-898e-80568bdeadd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, _, c, _, _ = range(5)\n",
    "print(a, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ef8b93-1853-4ac8-be9f-536df3bb1bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, *l = range(5)\n",
    "print(a, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc68fd1-11e1-4678-8617-41190931dce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = range(5)\n",
    "a, l = seq[0], seq[1:]\n",
    "print(a, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fc3559-9135-4b51-aebc-140d4184d007",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, *l, b = range(5)\n",
    "print(a, l, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7753b70e-c696-461f-9541-69677786c4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = []\n",
    "\n",
    "with open(csv_file) as stream:\n",
    "    reader = csv.DictReader(stream, delimiter=',')\n",
    "    for row in reader:\n",
    "        entries.append(row)\n",
    "\n",
    "for entry in entries[:5]:\n",
    "    print(entry['fname'], entry['title'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337c3c27-4791-4ecf-a824-b240e3c43049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2 as PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2e002a-18eb-4783-b10e-016b8b3a5043",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/folger/pdf/1H4.pdf'\n",
    "pdf = PDF.PdfFileReader(file_path, overwriteWarnings=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cae66b3-64cb-43b1-83fd-7f3d76464407",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pages = pdf.getNumPages()\n",
    "print(f'PDF has {n_pages} pages.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d009224b-62d6-4408-8ec3-67c08418be1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = pdf.getPage(1)\n",
    "content = page.extractText()\n",
    "print(content[:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f0efc1-a569-4fed-ae30-80c7b79bbdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf2txt(fname, page_numbers=None, concatenate=False):\n",
    "    \"\"\"Convert text from a PDF file into a string or list of strings.\n",
    "\n",
    "    Arguments:\n",
    "        fname: a string pointing to the filename of the PDF file\n",
    "        page_numbers: an integer or sequence of integers pointing to the\n",
    "            pages to extract. If None (default), all pages are extracted.\n",
    "        concatenate: a boolean indicating whether to concatenate the\n",
    "            extracted pages into a single string. When False, a list of\n",
    "            strings is returned.\n",
    "\n",
    "    Returns:\n",
    "        A string or list of strings representing the text extracted\n",
    "        from the supplied PDF file.\n",
    "\n",
    "    \"\"\"\n",
    "    pdf = PDF.PdfFileReader(fname, overwriteWarnings=False)\n",
    "    if page_numbers is None:\n",
    "        page_numbers = range(pdf.getNumPages())\n",
    "    elif isinstance(page_numbers, int):\n",
    "        page_numbers = [page_numbers]\n",
    "    texts = [pdf.getPage(n).extractText() for n in page_numbers]\n",
    "    return '\\n'.join(texts) if concatenate else texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc581c85-6f9d-4920-8748-2ff29bde0ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pdf2txt(file_path, concatenate=True)\n",
    "sample = pdf2txt(file_path, page_numbers=[1, 4, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1ce2f5-409c-4c0a-9714-8fe1add7e229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "line = {\n",
    "    'line_id': 12664,\n",
    "    'play_name': 'Alls well that ends well',\n",
    "    'speech_number': 1,\n",
    "    'line_number': '1.1.1',\n",
    "    'speaker': 'COUNTESS',\n",
    "    'text_entry': 'In delivering my son from me, I bury a second husband.'\n",
    "}\n",
    "\n",
    "print(json.dumps(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4d96d9-c1c7-4d1f-9013-f32b1f6f0173",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "with open('shakespeare.json', 'w') as f:\n",
    "    json.dump(line, f)\n",
    "\n",
    "\n",
    "# %%\n",
    "with open('data/macbeth.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(data[3:5])\n",
    "\n",
    "\n",
    "# %%\n",
    "import collections\n",
    "\n",
    "languages = collections.Counter()\n",
    "for entry in data:\n",
    "    languages[entry['lang']] += 1\n",
    "\n",
    "print(languages.most_common())\n",
    "\n",
    "\n",
    "# %%\n",
    "with open('data/sonnets/18.xml') as stream:\n",
    "    xml = stream.read()\n",
    "\n",
    "print(xml)\n",
    "\n",
    "\n",
    "# %%\n",
    "import lxml.etree\n",
    "\n",
    "\n",
    "# %%\n",
    "tree = lxml.etree.parse('data/sonnets/18.xml')\n",
    "print(tree)\n",
    "\n",
    "\n",
    "# %%\n",
    "# decoding is needed to transform the bytes object into an actual string\n",
    "print(lxml.etree.tostring(tree).decode())\n",
    "\n",
    "\n",
    "# %%\n",
    "for rhyme in tree.iterfind('//rhyme'):\n",
    "    print(f'element: {rhyme.tag} -> {rhyme.text}')\n",
    "\n",
    "\n",
    "# %%\n",
    "root = tree.getroot()\n",
    "print(root.tag)\n",
    "\n",
    "\n",
    "# %%\n",
    "print(root.attrib['year'])\n",
    "\n",
    "\n",
    "# %%\n",
    "print(len(root))\n",
    "\n",
    "\n",
    "# %%\n",
    "children = [child.tag for child in root]\n",
    "\n",
    "\n",
    "# %%\n",
    "print('\\n'.join(child.text or '' for child in root))\n",
    "\n",
    "\n",
    "# %%\n",
    "print(''.join(root[0].itertext()))\n",
    "\n",
    "\n",
    "# %%\n",
    "for node in root:\n",
    "    if node.tag == 'line':\n",
    "        print(f\"line {node.attrib['n']: >2}: {''.join(node.itertext())}\")\n",
    "\n",
    "\n",
    "# %%\n",
    "with open('data/sonnets/116.txt') as stream:\n",
    "    text = stream.read()\n",
    "\n",
    "print(text)\n",
    "\n",
    "\n",
    "# %%\n",
    "root = lxml.etree.Element('sonnet')\n",
    "root.attrib['author'] = 'William Shakespeare'\n",
    "root.attrib['year'] = '1609'\n",
    "\n",
    "\n",
    "# %%\n",
    "tree = lxml.etree.ElementTree(root)\n",
    "stringified = lxml.etree.tostring(tree)\n",
    "print(stringified)\n",
    "\n",
    "\n",
    "# %%\n",
    "print(type(stringified))\n",
    "\n",
    "\n",
    "# %%\n",
    "print(stringified.decode('utf-8'))\n",
    "\n",
    "\n",
    "# %%\n",
    "for nb, line in enumerate(open('data/sonnets/116.txt')):\n",
    "    node = lxml.etree.Element('line')\n",
    "    node.attrib['n'] = str(nb + 1)\n",
    "    node.text = line.strip()\n",
    "    root.append(node)\n",
    "    # voltas typically, but not always occur between the octave and sextet\n",
    "    if nb == 8:\n",
    "        node = lxml.etree.Element('volta')\n",
    "        root.append(node)\n",
    "\n",
    "\n",
    "# %%\n",
    "print(lxml.etree.tostring(tree, pretty_print=True).decode())\n",
    "\n",
    "\n",
    "# %%\n",
    "# Loop over all nodes in the tree\n",
    "for node in root:\n",
    "    # Leave the volta node alone. A continue statement instructs\n",
    "    # Python to move on to the next item in the loop.\n",
    "    if node.tag == 'volta':\n",
    "        continue\n",
    "    # We chop off and store verse-final punctuation:\n",
    "    punctuation = ''\n",
    "    if node.text[-1] in ',:;.':\n",
    "        punctuation = node.text[-1]\n",
    "        node.text = node.text[:-1]\n",
    "    # Make a list of words using the split method\n",
    "    words = node.text.split()\n",
    "    # We split rhyme words and other words:\n",
    "    other_words, rhyme = words[:-1], words[-1]\n",
    "    # Replace the node's text with all text except the rhyme word\n",
    "    node.text = ' '.join(other_words) + ' '\n",
    "    # We create the rhyme element, with punctuation (if any) in its tail\n",
    "    elt = lxml.etree.Element('rhyme')\n",
    "    elt.text = rhyme\n",
    "    elt.tail = punctuation\n",
    "    # We add the rhyme to the line:\n",
    "    node.append(elt)\n",
    "\n",
    "tree = lxml.etree.ElementTree(root)\n",
    "print(lxml.etree.tostring(tree, pretty_print=True).decode())\n",
    "\n",
    "\n",
    "# %%\n",
    "with open('data/sonnets/116.xml', 'w') as f:\n",
    "    f.write(\n",
    "        lxml.etree.tostring(\n",
    "            root, xml_declaration=True, pretty_print=True, encoding='utf-8').decode())\n",
    "\n",
    "\n",
    "# %%\n",
    "root = lxml.etree.Element('sonnet')\n",
    "# Add an author attribute to the root node\n",
    "root.attrib['author'] = 'William Shakespeare'\n",
    "# Add a year attribute to the root node\n",
    "root.attrib['year'] = '1609'\n",
    "\n",
    "for nb, line in enumerate(open('data/sonnets/116.txt')):\n",
    "    line_node = lxml.etree.Element('line')\n",
    "    # Add a line number attribute to each line node\n",
    "    line_node.attrib['n'] = str(nb + 1)\n",
    "\n",
    "    # Make different nodes for words and non-words\n",
    "    word = ''\n",
    "    for char in line.strip():\n",
    "        if char.isalpha():\n",
    "            word += char\n",
    "        else:\n",
    "            word_node = lxml.etree.Element('w')\n",
    "            word_node.text = word\n",
    "            line_node.append(word_node)\n",
    "            word = ''\n",
    "\n",
    "            char_node = lxml.etree.Element('c')\n",
    "            char_node.text = char\n",
    "            line_node.append(char_node)\n",
    "\n",
    "    # don't forget last word:\n",
    "    if word:\n",
    "        word_node = lxml.etree.Element('w')\n",
    "        word_node.text = word\n",
    "        line_node.append(word_node)\n",
    "\n",
    "    rhyme_node = lxml.etree.Element('rhyme')\n",
    "    # We use xpath to find the final w-element in the line\n",
    "    # and wrap it in a line element\n",
    "    rhyme_node.append(line_node.xpath('//w')[-1])\n",
    "    line_node.replace(line_node.xpath('//w')[-1], rhyme_node)\n",
    "\n",
    "    root.append(line_node)\n",
    "\n",
    "    # Add the volta node\n",
    "    if nb == 8:\n",
    "        node = lxml.etree.Element('volta')\n",
    "        root.append(node)\n",
    "\n",
    "tree = lxml.etree.ElementTree(root)\n",
    "xml_string = lxml.etree.tostring(tree, pretty_print=True).decode()\n",
    "# Print a snippet of the tree:\n",
    "print(xml_string[:xml_string.find(\"</line>\") + 8] + '  ...')\n",
    "\n",
    "\n",
    "# %%\n",
    "tree = lxml.etree.parse('data/folger/xml/Oth.xml')\n",
    "print(tree.getroot().find('.//{http://www.tei-c.org/ns/1.0}title').text)\n",
    "\n",
    "\n",
    "# %%\n",
    "print(tree.getroot().find('title'))\n",
    "\n",
    "\n",
    "# %%\n",
    "NSMAP = {'tei': 'http://www.tei-c.org/ns/1.0'}\n",
    "print(tree.getroot().find('.//tei:title', namespaces=NSMAP).text)\n",
    "\n",
    "\n",
    "# %%\n",
    "import bs4 as bs\n",
    "\n",
    "html_doc = \"\"\"\n",
    "<html>\n",
    "  <head>\n",
    "    <title>Henry IV, Part I</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <div>\n",
    "      <p class=\"speaker\">KING</p>\n",
    "      <p id=\"line-1.1.1\">\n",
    "        <a id=\"ftln-0001\">FTLN 0001</a>\n",
    "        So shaken as we are, so wan with care,\n",
    "      </p>\n",
    "      <p id=\"line-1.1.2\">\n",
    "        <a id=\"ftln-0002\">FTLN 0002</a>\n",
    "        Find we a time for frighted peace to pant\n",
    "      </p>\n",
    "      <p id=\"line-1.1.3\">\n",
    "        <a id=\"ftln-0003\">FTLN 0003</a>\n",
    "        And breathe short-winded accents of new broils\n",
    "      </p>\n",
    "      <p id=\"line-1.1.4\">\n",
    "        <a id=\"ftln-0004\">FTLN 0004</a>\n",
    "        To be commenced in strands afar remote.\n",
    "      </p>\n",
    "    </div>\n",
    "  </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "html = bs.BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "\n",
    "# %%\n",
    "# print the documents <title> (from head)\n",
    "print(html.title)\n",
    "\n",
    "\n",
    "# %%\n",
    "# print the first <p> element and its content\n",
    "print(html.p)\n",
    "\n",
    "\n",
    "# %%\n",
    "# print the text of a particular element, e.g. the <title>\n",
    "print(html.title.text)\n",
    "\n",
    "\n",
    "# %%\n",
    "# print the parent tag (and its content) of the first <p> element\n",
    "print(html.p.parent)\n",
    "\n",
    "\n",
    "# %%\n",
    "# print the parent tag name of the first <p> element\n",
    "print(html.p.parent.name)\n",
    "\n",
    "\n",
    "# %%\n",
    "# find all occurrences of the <a> element\n",
    "print(html.find_all('a'))\n",
    "\n",
    "\n",
    "# %%\n",
    "# find a <p> element with a specific ID\n",
    "print(html.find('p', id='line-1.1.3'))\n",
    "\n",
    "\n",
    "# %%\n",
    "def html2txt(fpath):\n",
    "    \"\"\"Convert text from a HTML file into a string.\n",
    "\n",
    "    Arguments:\n",
    "        fpath: a string pointing to the filename of the HTML file\n",
    "\n",
    "    Returns:\n",
    "        A string representing the text extracted from the supplied\n",
    "        HTML file.\n",
    "\n",
    "    \"\"\"\n",
    "    with open(fpath) as f:\n",
    "        html = bs.BeautifulSoup(f, 'html.parser')\n",
    "    return html.get_text()\n",
    "\n",
    "\n",
    "# %%\n",
    "fp = 'data/folger/html/1H4.html'\n",
    "text = html2txt(fp)\n",
    "start = text.find('Henry V')\n",
    "print(text[start:start + 500])\n",
    "\n",
    "\n",
    "# %%\n",
    "with open(fp) as f:\n",
    "    html = bs.BeautifulSoup(f, 'html.parser')\n",
    "toc = html.find('table', attrs={'class': 'contents'})\n",
    "\n",
    "\n",
    "# %%\n",
    "def toc_hrefs(html):\n",
    "    \"\"\"Return a list of hrefs from a document's table of contents.\"\"\"\n",
    "    toc = html.find('table', attrs={'class': 'contents'})\n",
    "    hrefs = []\n",
    "    for tr in toc.find_all('tr'):\n",
    "        for td in tr.find_all('td'):\n",
    "            for a in td.find_all('a'):\n",
    "                hrefs.append(a.get('href'))\n",
    "    return hrefs\n",
    "\n",
    "\n",
    "# %%\n",
    "items = toc_hrefs(html)\n",
    "print(items[:5])\n",
    "\n",
    "\n",
    "# %%\n",
    "def get_href_div(html, href):\n",
    "    \"\"\"Retrieve the <div> element corresponding to the given href.\"\"\"\n",
    "    href = href.lstrip('#')\n",
    "    div = html.find('div', attrs={'id': href})\n",
    "    if div is None:\n",
    "        div = html.find('a', attrs={'name': href}).findNext('div')\n",
    "    return div\n",
    "\n",
    "\n",
    "# %%\n",
    "def html2txt(fname, concatenate=False):\n",
    "    \"\"\"Convert text from a HTML file into a string or sequence of strings.\n",
    "\n",
    "    Arguments:\n",
    "        fpath: a string pointing to the filename of the HTML file.\n",
    "        concatenate: a boolean indicating whether to concatenate the\n",
    "            extracted texts into a single string. If False, a list of\n",
    "            strings representing the individual sections is returned.\n",
    "\n",
    "    Returns:\n",
    "        A string or list of strings representing the text extracted\n",
    "        from the supplied HTML file.\n",
    "\n",
    "    \"\"\"\n",
    "    with open(fname) as f:\n",
    "        html = bs.BeautifulSoup(f, 'html.parser')\n",
    "    # Use a concise list comprehension to create the list of texts.\n",
    "    # The same list could be constructed using an ordinary for-loop:\n",
    "    #    texts = []\n",
    "    #    for href in toc_hrefs(html):\n",
    "    #        text = get_href_div(html, href).get_text()\n",
    "    #        texts.append(text)\n",
    "    texts = [get_href_div(html, href).get_text() for href in toc_hrefs(html)]\n",
    "    return '\\n'.join(texts) if concatenate else texts\n",
    "\n",
    "\n",
    "# %%\n",
    "texts = html2txt(fp)\n",
    "print(texts[6][:200])\n",
    "\n",
    "\n",
    "# %%\n",
    "import urllib.request\n",
    "\n",
    "page = urllib.request.urlopen('https://en.wikipedia.org/wiki/William_Shakespeare')\n",
    "html = page.read()\n",
    "\n",
    "\n",
    "# %%\n",
    "import bs4\n",
    "\n",
    "soup = bs4.BeautifulSoup(html, 'html.parser')\n",
    "print(soup.get_text().strip()[:300])\n",
    "\n",
    "\n",
    "# %%\n",
    "import re\n",
    "\n",
    "for script in soup(['script', 'style']):\n",
    "    script.extract()\n",
    "text = soup.get_text()\n",
    "text = re.sub('\\s*\\n+\\s*', '\\n', text)  # remove multiple linebreaks:\n",
    "print(text[:300])\n",
    "\n",
    "\n",
    "# %%\n",
    "links = soup.find_all('a')\n",
    "print(links[9].prettify())\n",
    "\n",
    "\n",
    "# %%\n",
    "V = {1, 2, 3, 4, 5}\n",
    "E = {(1, 2), (1, 4), (2, 5), (3, 4), (4, 5)}\n",
    "\n",
    "\n",
    "# %%\n",
    "import networkx as nx\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(V)\n",
    "G.add_edges_from(E)\n",
    "\n",
    "\n",
    "# %%\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nx.draw_networkx(G, font_color=\"white\")\n",
    "plt.axis('off');\n",
    "\n",
    "\n",
    "# %%\n",
    "NSMAP = {'tei': 'http://www.tei-c.org/ns/1.0'}\n",
    "\n",
    "\n",
    "def character_network(tree):\n",
    "    \"\"\"Construct a character interaction network.\n",
    "\n",
    "    Construct a character interaction network for Shakespeare texts in\n",
    "    the Folger Digital Text collection. Character interaction networks\n",
    "    are constructed on the basis of successive speaker turns in the texts,\n",
    "    and edges between speakers are created when their utterances follow\n",
    "    one another.\n",
    "\n",
    "    Arguments:\n",
    "        tree: An lxml.ElementTree instance representing one of the XML\n",
    "            files in the Folger Shakespeare collection.\n",
    "\n",
    "    Returns:\n",
    "        A character interaction network represented as a weighted,\n",
    "        undirected NetworkX Graph.\n",
    "\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    # extract a list of speaker turns for each scene in a play\n",
    "    for scene in tree.iterfind('.//tei:div2[@type=\"scene\"]', NSMAP):\n",
    "        speakers = scene.findall('.//tei:sp', NSMAP)\n",
    "        # iterate over the sequence of speaker turns...\n",
    "        for i in range(len(speakers) - 1):\n",
    "            # ... and extract pairs of adjacent speakers\n",
    "            try:\n",
    "                speaker_i = speakers[i].attrib['who'].split('_')[0].replace('#', '')\n",
    "                speaker_j = speakers[i + 1].attrib['who'].split('_')[0].replace('#', '')\n",
    "                # if the interaction between two speakers has already\n",
    "                # been attested, update their interaction count\n",
    "                if G.has_edge(speaker_i, speaker_j):\n",
    "                    G[speaker_i][speaker_j]['weight'] += 1\n",
    "                # else add an edge between speaker i and j to the graph\n",
    "                else:\n",
    "                    G.add_edge(speaker_i, speaker_j, weight=1)\n",
    "            except KeyError:\n",
    "                continue\n",
    "    return G\n",
    "\n",
    "\n",
    "# %%\n",
    "tree = lxml.etree.parse('data/folger/xml/Ham.xml')\n",
    "G = character_network(tree.getroot())\n",
    "\n",
    "\n",
    "# %%\n",
    "print(f\"N nodes = {G.number_of_nodes()}, N edges = {G.number_of_edges()}\")\n",
    "\n",
    "\n",
    "# %%\n",
    "import collections\n",
    "\n",
    "interactions = collections.Counter()\n",
    "\n",
    "for speaker_i, speaker_j, data in G.edges(data=True):\n",
    "    interaction_count = data['weight']\n",
    "    interactions[speaker_i] += interaction_count\n",
    "    interactions[speaker_j] += interaction_count\n",
    "\n",
    "nodesizes = [interactions[speaker] * 5 for speaker in G]\n",
    "\n",
    "\n",
    "# %%\n",
    "# Create an empty figure of size 15x15\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "# Compute the positions of the nodes using the spring layout algorithm\n",
    "pos = nx.spring_layout(G, k=0.5, iterations=200)\n",
    "# Then, add the edges to the visualization\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.4)\n",
    "# Subsequently, add the weighted nodes to the visualization\n",
    "nx.draw_networkx_nodes(G, pos, node_size=nodesizes, alpha=0.4)\n",
    "# Finally, add the labels (i.e. the speaker IDs) to the visualization\n",
    "nx.draw_networkx_labels(G, pos, fontsize=14)\n",
    "plt.axis('off');\n",
    "\n",
    "\n",
    "# %%\n",
    "from copy import deepcopy\n",
    "G0 = deepcopy(G)\n",
    "\n",
    "for u, v, d in G0.edges(data=True):\n",
    "    d['weight'] = 1\n",
    "\n",
    "nodesizes = [interactions[speaker] * 5 for speaker in G0]\n",
    "\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "pos = nx.spring_layout(G0, k=0.5, iterations=200)\n",
    "nx.draw_networkx_edges(G0, pos, alpha=0.4)\n",
    "nx.draw_networkx_nodes(G0, pos, node_size=nodesizes, alpha=0.4)\n",
    "nx.draw_networkx_labels(G0, pos, fontsize=14)\n",
    "plt.axis('off');\n",
    "\n",
    "\n",
    "# %%\n",
    "G0.remove_node('Hamlet')\n",
    "\n",
    "\n",
    "# %%\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "pos = nx.spring_layout(G0, k=0.5, iterations=200)\n",
    "nodesizes = [interactions[speaker] * 5 for speaker in G0]\n",
    "nx.draw_networkx_edges(G0, pos, alpha=0.4)\n",
    "nx.draw_networkx_nodes(G0, pos, node_size=nodesizes, alpha=0.4)\n",
    "nx.draw_networkx_labels(G0, pos, fontsize=14)\n",
    "plt.axis('off');\n",
    "\n",
    "\n",
    "# %%\n",
    "import json\n",
    "from networkx.readwrite import json_graph\n",
    "\n",
    "with open('hamlet.json', 'w') as f:\n",
    "    json.dump(json_graph.node_link_data(G), f)\n",
    "\n",
    "with open('hamlet.json') as f:\n",
    "    d = json.load(f)\n",
    "\n",
    "G = json_graph.node_link_graph(d)\n",
    "print(f\"Graph with {len(G.nodes())} nodes and {len(G.edges())} edges.\")\n",
    "\n",
    "\n",
    "# %%\n",
    "# Undocumented code snippet used in chapter (e.g., for figure generation)\n",
    "import functools\n",
    "from copy import deepcopy\n",
    "G1 = deepcopy(G)\n",
    "\n",
    "for u, v, d in G.edges(data=True):\n",
    "    if d[\"weight\"] < 10:\n",
    "        G1.remove_edge(u, v)\n",
    "\n",
    "G1 = nx.relabel_nodes(G1, {\"SOLDIERS.FORTINBRAS.Captain\": \"Fortinbras.Captain\"})\n",
    "# rename verbose name for Fortinbras' Captain\n",
    "#SOLDIERS.FORTINBRAS.Captain\n",
    "\n",
    "subgraphs = [G1.subgraph(c).copy() for c in nx.connected_components(G1)]\n",
    "# functools.reduce is similar to foldl in Haskell and fold_left in OCaml\n",
    "def larger_graph(graph1, graph2):\n",
    "    return graph2 if len(graph2.nodes()) > len(graph1.nodes()) else graph1\n",
    "G1 = functools.reduce(larger_graph, subgraphs, subgraphs[0])\n",
    "\n",
    "fig = plt.figure(figsize=(9, 6))\n",
    "pos = nx.spring_layout(G1, k=0.5, iterations=2000, seed=1)\n",
    "nx.draw_networkx_edges(G1, pos, alpha=0.4)\n",
    "nx.draw_networkx_nodes(G1, pos, node_size=[degree * 100 for _, degree in G1.degree()], alpha=0.4)\n",
    "nx.draw_networkx_labels(G1, pos, fontsize=14)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig('img/hamlet-minimum-10-interactions.png')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
