{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Cultural Data Analysis\n",
        "\n",
        "Introduction to working with datasets"
      ],
      "metadata": {
        "id": "7CC34uuNzNxY"
      },
      "id": "7CC34uuNzNxY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76a4e150-cc6f-4878-a32b-e78a1d6426ae",
      "metadata": {
        "id": "76a4e150-cc6f-4878-a32b-e78a1d6426ae"
      },
      "outputs": [],
      "source": [
        "# import necessary libraries\n",
        "import os, re, csv\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7010dda9-acc5-4d90-b175-90012564d13c",
      "metadata": {
        "id": "7010dda9-acc5-4d90-b175-90012564d13c"
      },
      "source": [
        "## Loading the dataset: heritage homes webistes\n",
        "\n",
        "The dataset is stored in a shared google drive:\n",
        "https://drive.google.com/drive/folders/11Shm0edDOiWrOe56fzJQRZi-v_BPSW8E?usp=drive_link\n",
        "\n",
        "Add it to your drive.\n",
        "\n",
        "To access it, load your gdrive in 'Files' (see left pane of the notebook in google colab) and navigate to the shared folder. You may need to click on 'refresh' to make it appear on the list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d42429d3-63fe-4b79-b341-160057e5dcbc",
      "metadata": {
        "id": "d42429d3-63fe-4b79-b341-160057e5dcbc"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/jazoza/cultural-data-analysis"
      ],
      "metadata": {
        "id": "QYiHwjcORrPC"
      },
      "id": "QYiHwjcORrPC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Import all datasets (4 countries)\n",
        "\n",
        "You will have all datasets available for analysis and comparison, mapped in the following way:\n",
        "\n",
        "> df0 - Dutch dataset\n",
        "\n",
        "> df1 - UK dataset\n",
        "\n",
        "> df2 - German dataset\n",
        "\n",
        "> df3 - French dataset"
      ],
      "metadata": {
        "id": "_GcR-p4NfFHw"
      },
      "id": "_GcR-p4NfFHw"
    },
    {
      "cell_type": "code",
      "source": [
        "# Country code: change here between 'NL' and 'UK'\n",
        "cc_list = ['NL', 'UK', 'DE', 'FR']"
      ],
      "metadata": {
        "id": "5JIG2eWEfWoF"
      },
      "id": "5JIG2eWEfWoF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gdrive_path = '/content/gdrive/MyDrive/CDA/'"
      ],
      "metadata": {
        "id": "bbjhZ8nKZtZC"
      },
      "id": "bbjhZ8nKZtZC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import scraped json data into 4 separate dataframes\n",
        "df0=pd.read_json(gdrive_path+cc_list[0]+'_dataset_website-content-crawler.json')\n",
        "# select columns for analysis: url, text, metadata\n",
        "df0=df0[['url','text','metadata']]\n",
        "\n",
        "df1=pd.read_json(gdrive_path+cc_list[1]+'_dataset_website-content-crawler.json')\n",
        "# select columns for analysis: url, text, metadata\n",
        "df1=df1[['url','text','metadata']]\n",
        "\n",
        "df2=pd.read_json(gdrive_path+cc_list[2]+'_dataset_website-content-crawler.json')\n",
        "# select columns for analysis: url, text, metadata\n",
        "df2=df2[['url','text','metadata']]\n",
        "\n",
        "df3=pd.read_json(gdrive_path+cc_list[3]+'_dataset_website-content-crawler.json')\n",
        "# select columns for analysis: url, text, metadata\n",
        "df3=df3[['url','text','metadata']]\n",
        "\n",
        "df0.head()"
      ],
      "metadata": {
        "id": "yCPPY_4I2pIZ"
      },
      "id": "yCPPY_4I2pIZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to extract the main domain from the url in the dataset\n",
        "def extract_main_domain(url):\n",
        "    if not isinstance(str(url), str):\n",
        "        print('NOT VALID',url)\n",
        "        return None\n",
        "    match = re.findall('(?:\\\\w+\\\\.)*\\\\w+\\\\.\\\\w*', str(url)) #'www\\.?([^/]+)'\n",
        "    return match[0].lstrip('www.') if match else None"
      ],
      "metadata": {
        "id": "txb3nj2mff6G"
      },
      "id": "txb3nj2mff6G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8697b51f-50a5-4091-9cc1-0aed1308b27d",
      "metadata": {
        "id": "8697b51f-50a5-4091-9cc1-0aed1308b27d"
      },
      "outputs": [],
      "source": [
        "# Add a new column 'domain' and fill it by applying the extract_main_domain function to the 'url' column\n",
        "\n",
        "# first, create a mapping of dataframes which could be addressed in a loop\n",
        "df_dict = {'0':df0, '1':df1, '2':df2, '3':df3}\n",
        "\n",
        "# then, loop through the df_dict to update each dataframe\n",
        "for k, v in df_dict.items():\n",
        "  cc_column = cc_list[int(k[-1])]+' domains'\n",
        "  cc = cc_list[int(k[-1])]\n",
        "  # print(cc_column, cc)\n",
        "  urls = pd.read_csv(gdrive_path+'url_lists/'+cc_list[int(k[-1])]+'_urls.csv')[cc_column].values.tolist()\n",
        "  domains = {extract_main_domain(url) for url in urls if extract_main_domain(url) is not None}\n",
        "  matching_links = [link for link in v.url if extract_main_domain(link) in domains]\n",
        "  # update the dataframe\n",
        "  v['domain'] = v['url'].apply(extract_main_domain)\n",
        "\n",
        "# check one of the dataframes\n",
        "df1.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import stopwords\n",
        "\n",
        "Import stopwords dictionaries for the 4 langauges we work with. It is good to import all of them in our case, because many websites have sections is English, German or French even when this is not the main language of the website."
      ],
      "metadata": {
        "id": "Rr6hiPQ-4z5O"
      },
      "id": "Rr6hiPQ-4z5O"
    },
    {
      "cell_type": "code",
      "source": [
        "# load a list of 'stopwords' function\n",
        "def get_stopwords_list(stop_file_path):\n",
        "    \"\"\"load stop words \"\"\"\n",
        "    with open(stop_file_path, 'r', encoding=\"utf-8\") as f:\n",
        "        stopwords = f.readlines()\n",
        "        stop_set = set(m.strip() for m in stopwords)\n",
        "        return list(frozenset(stop_set))"
      ],
      "metadata": {
        "id": "_Px9Aoim4-pq"
      },
      "id": "_Px9Aoim4-pq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47db9deb-8836-47fb-9f74-28a023bcb5d7",
      "metadata": {
        "id": "47db9deb-8836-47fb-9f74-28a023bcb5d7"
      },
      "outputs": [],
      "source": [
        "# Get the stopwords list for all languages (using cc_list previously defined)\n",
        "# cc_list = ['NL', 'UK', 'DE', 'FR'] # remove the hashtag from this line to uncomment this code and make it run\n",
        "\n",
        "stopwords = [] # empty list to which a list of stopwords will be appended in loop\n",
        "\n",
        "for i in range(len(cc_list)):\n",
        "  stopwords_cc_path = \"/content/cultural-data-analysis/stopwords_archive/\"+cc_list[i]+\".txt\"\n",
        "  stopwords_cc = get_stopwords_list(stopwords_cc_path)\n",
        "  #print(len(stopwords_cc)) # print how many words are in the list\n",
        "  stopwords.extend(stopwords_cc)\n",
        "\n",
        "#print(len(stopwords)) # print how many words are in all stopwords lists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc9d2331-fe01-4e94-b984-00ac834a771c",
      "metadata": {
        "id": "cc9d2331-fe01-4e94-b984-00ac834a771c"
      },
      "outputs": [],
      "source": [
        "# you may need to include additional words which you notice as too frequent\n",
        "special_stop_words = ['nbsp', 'nl', 'fr', 'de', 'uk', 'com', 'www', 'lit', ' '] # these might appear frequently as 'terms' in the corpus, so it's good to filter them\n",
        "stopwords_ext = stopwords+special_stop_words"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08144921-d7be-45ed-8795-1c085fb2640b",
      "metadata": {
        "id": "08144921-d7be-45ed-8795-1c085fb2640b"
      },
      "source": [
        "## 1. Visualize term frequency: bar chart"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CALCULATE TERM FREQUENCY WITHOUT STOP-WORDS\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "#cvec_stopped = CountVectorizer(max_df=0.5, token_pattern=r'(?u)\\b[A-Za-z]{2,}\\b') # max_df could in theory automatically filter stopwords\n",
        "cvec_stopped = CountVectorizer(stop_words=stopwords_ext, token_pattern=r'(?u)\\b[A-Za-z]{2,}\\b') # token pattern recognizes only words which are made of letters, and longer than 1 character\n",
        "cvec_stopped.fit(df0.text) #### CHANGE df0 TO THE DATAFRAME YOU ANALYSE\n",
        "document_matrix = cvec_stopped.transform(df0.text) #### CHANGE df0 TO THE DATAFRAME YOU ANALYSE\n",
        "term_batches = np.linspace(0,document_matrix.shape[0],10).astype(int)\n",
        "i=0\n",
        "df_stopped = []\n",
        "while i < len(term_batches)-1:\n",
        "    batch_result = np.sum(document_matrix[term_batches[i]:term_batches[i+1]].toarray(),axis=0)\n",
        "    df_stopped.append(batch_result)\n",
        "    print(term_batches[i+1],\"entries' term frequency calculated\")\n",
        "    i += 1\n",
        "\n",
        "terms_stopped = np.sum(df_stopped,axis=0)\n",
        "#print(terms_stopped.shape)\n",
        "term_freq_df_stopped = pd.DataFrame([terms_stopped],columns=cvec_stopped.get_feature_names_out()).transpose()\n",
        "term_freq_df_stopped.columns = ['terms']"
      ],
      "metadata": {
        "id": "rKDwducX7kqD"
      },
      "id": "rKDwducX7kqD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Filter and order top N words in descending order\n",
        "# change the value in .head(N) to include more or less terms\n",
        "topN = term_freq_df_stopped.sort_values(by='terms',\n",
        "                                                ascending=False).head(50)\n",
        "\n",
        "# Create the bar chart\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(topN.index, topN['terms'], color='skyblue')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Most Frequent Words (excluding stopwords)')\n",
        "plt.xticks(rotation=45, ha='right') # Rotate x-axis labels for readability\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8eX91yiEg7UE"
      },
      "id": "8eX91yiEg7UE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# Create the bar chart using seaborn\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=topN.index, y=topN['terms'], palette='viridis',\n",
        "            hue=topN.index, legend=False)\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Top 10 Most Frequent Words (excluding stopwords)')\n",
        "plt.xticks(rotation=45, ha='right') # Rotate x-axis labels for readability\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JiwCfaFFjTAJ"
      },
      "id": "JiwCfaFFjTAJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Visualize comparative frequency\n",
        "\n",
        "Compare the frequency of female- and male-connotated titles"
      ],
      "metadata": {
        "id": "S99Dd9_ml8NG"
      },
      "id": "S99Dd9_ml8NG"
    },
    {
      "cell_type": "code",
      "source": [
        "# define lists of nobility titles for each county\n",
        "# lists are defined as tupples: value pairs of title and 0/1\n",
        "# where 0 = male titles, 1 = female title\n",
        "dutch_nobility_titles = [('ridder',0), ('jonkvrouw',1),\n",
        "                        ('baron',0), ('barones',1),\n",
        "                         ('graaf',0), ('gravin',1),\n",
        "                          ('hertog',0), ('hertogin',1),\n",
        "                          ('prins',0), ('prinses',1)]\n",
        "\n",
        "uk_nobility_titles = [('sir',0), ('lady',1), ('knight',0),\n",
        "                        ('baron',0), ('baroness',1),\n",
        "                         ('duke',0), ('duchess',1),\n",
        "                          ('prince',0), ('princess',1),\n",
        "                           ('king',0), ('queen',1)]\n",
        "\n",
        "german_nobility_titles = [ ('ritter',0),\n",
        "                         ('graf',0), ('gräfin',1),\n",
        "                          ('fürst',0),('fürstin',1),\n",
        "                          ('herzog',0), ('herzogin',1),\n",
        "                           ('prinz',0), ('prinzessin',1),\n",
        "                            ('könig', 0),('königin', 1)]\n",
        "\n",
        "french_nobility_titles = [('chevalier',0), ('dame',1), ('baron',0), ('baronne',1),\n",
        "                          ('duc',0), ('duchesse',1), ('marquis',0), ('marquise',1),\n",
        "                           ('prince',0), ('princesse',1), ('roi',0), ('reine',1)]\n"
      ],
      "metadata": {
        "id": "fjampXpel7xd"
      },
      "id": "fjampXpel7xd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# change country for nobility titles\n",
        "# the list of titles should match the country dataframe\n",
        "# df0 - dutch_nobility_titles\n",
        "# df1 - uk_nobility_titles\n",
        "# df2 - german_nobility_titles\n",
        "# df3 - french_nobility_titles\n",
        "\n",
        "nobility_titles = german_nobility_titles\n",
        "df = df2\n",
        "\n",
        "nobility = [term for term, gender in nobility_titles]\n",
        "genders = [gender for term, gender in nobility_titles]\n",
        "\n",
        "for term in nobility:\n",
        "    df[term] = df['text'].apply(lambda x: x.lower().count(term) if isinstance(x, str) else 0)\n",
        "\n",
        "# Filter for rows where 'kasteel' appears at least once\n",
        "mask = (df[nobility] > 0).any(axis=1)\n",
        "\n",
        "# Filter the DataFrame based on the boolean mask.\n",
        "df_filtered = df[mask]\n",
        "\n",
        "# Create the five-column table\n",
        "filter_list = ['domain','url'] + nobility\n",
        "result_df = df_filtered[filter_list]\n",
        "\n",
        "# Calculate the sum of values for each specified column\n",
        "column_sums = result_df[nobility].sum()\n",
        "\n",
        "# Create a bar chart with pink bars\n",
        "fig, ax = plt.subplots(figsize=(10,6))\n",
        "colors = ['#e877f0' if gender == 0 else '#9f01aa' for gender in genders]\n",
        "\n",
        "# Add horizontal lines at 1/8ths of the vertical bars height\n",
        "max_bar_height = column_sums.values.max()\n",
        "step_size = max_bar_height / 8\n",
        "# Ensure at least one line is drawn if max_bar_height is very small\n",
        "if step_size == 0 and max_bar_height > 0:\n",
        "    y_values = [max_bar_height]\n",
        "elif step_size > 0:\n",
        "    y_values = np.arange(step_size, max_bar_height + step_size, step_size)\n",
        "else:\n",
        "    y_values = []\n",
        "\n",
        "for value in y_values:\n",
        "  plt.axhline(y=value, color='grey', linestyle='--', linewidth=0.5)\n",
        "\n",
        "plt.bar(column_sums.index, column_sums.values, color=colors)\n",
        "\n",
        "# Customize the plot\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.tight_layout()\n",
        "\n",
        "ax.set_ylabel('Sum of term frequencies')\n",
        "ax.set_title('Frequency of terms related to nobility titles in 2024 corpus heritage houses websites')\n",
        "\n",
        "# Add two columns from the df dataframe on the left\n",
        "plt.text(-0.8, -160, f\"total websites: {len(df.domain.unique())}\", fontsize=10)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pLgPPJH5mgkJ"
      },
      "id": "pLgPPJH5mgkJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Visualize term relations: scatter plot\n",
        "\n"
      ],
      "metadata": {
        "id": "sChgA3losQqC"
      },
      "id": "sChgA3losQqC"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "Cxth7eBisYC_"
      },
      "id": "Cxth7eBisYC_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "U7PIPa25sbIa"
      },
      "id": "U7PIPa25sbIa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "df = df0\n",
        "\n",
        "# X is a list of tokenized texts (i.e. list of lists of tokens)\n",
        "X = [word_tokenize(item) for item in df.text.tolist()] # replace df0 with a dataframe you are analysing\n",
        "#print(X[0:3])\n",
        "model = gensim.models.Word2Vec(X, min_count=6, vector_size=200) # min_count: how many times a word appears in the corpus; size: number of dimensions\n",
        "\n"
      ],
      "metadata": {
        "id": "OQbsQB5LsObd"
      },
      "id": "OQbsQB5LsObd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize keywords with t-SNE\n",
        "\n",
        "Visualize to N number, or choose keywords that correspond to your analysis and visualize how they and their closest terms are distributed in the discourse.\n",
        "Use t-SNE to visualize the relations.\n",
        "\n",
        "* [t-Distributed Stochastic Neighbor Embedding (t-SNE)](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) is a technique for dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets.\n"
      ],
      "metadata": {
        "id": "kPpqk0TYab3j"
      },
      "id": "kPpqk0TYab3j"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7096871"
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.cm as cm\n",
        "import seaborn as sns\n",
        "\n",
        "# Get the top N words from term_freq_df_stopped\n",
        "topN_df = term_freq_df_stopped.sort_values(by='terms', ascending=False).head(1000)\n",
        "topN_words_list = topN_df.index.tolist()\n",
        "\n",
        "# Filter words that are present in the Word2Vec model's vocabulary\n",
        "filtered_top_words = [word for word in topN_words_list if word in model.wv.key_to_index]\n",
        "\n",
        "# Get the word embeddings for these filtered words\n",
        "embeddings_for_tsne = [model.wv[word] for word in filtered_top_words]\n",
        "\n",
        "# Check if there are embeddings to visualize\n",
        "if not embeddings_for_tsne:\n",
        "    print(\"No valid words found in the model's vocabulary from the top 100 words to visualize.\")\n",
        "else:\n",
        "    embeddings_array = np.array(embeddings_for_tsne)\n",
        "\n",
        "    # Apply t-SNE for Dimensionality Reduction\n",
        "    tsne_model_2d = TSNE(perplexity=15, n_components=2, init='pca', n_iter=3500, random_state=32)\n",
        "    embeddings_2d = tsne_model_2d.fit_transform(embeddings_array)\n",
        "\n",
        "    # Use seaborn style for plots\n",
        "    sns.set_style(\"whitegrid\")\n",
        "\n",
        "    # Visualize the 2D word embeddings\n",
        "    plt.figure(figsize=(16, 9))\n",
        "    plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.7, c='blue')\n",
        "\n",
        "    for i, word in enumerate(filtered_top_words):\n",
        "        plt.annotate(word, alpha=0.8, xy=(embeddings_2d[i, 0], embeddings_2d[i, 1]),\n",
        "                     xytext=(5, 2), textcoords='offset points', ha='right', va='bottom', size=8)\n",
        "\n",
        "    plt.title('t-SNE Visualization of Top 100 Most Frequent Words')\n",
        "    plt.xlabel('t-SNE Dimension 1')\n",
        "    plt.ylabel('t-SNE Dimension 2')\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "id": "c7096871",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Visualise relations between specific words"
      ],
      "metadata": {
        "id": "_Q3-Dr2AtL9J"
      },
      "id": "_Q3-Dr2AtL9J"
    },
    {
      "cell_type": "code",
      "source": [
        "search_terms = ['man', 'vrouw']"
      ],
      "metadata": {
        "id": "IgQerdJTx8ET"
      },
      "id": "IgQerdJTx8ET",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "here\n"
      ],
      "metadata": {
        "id": "h52kChPJ6-tv"
      },
      "id": "h52kChPJ6-tv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Topic Modeling\n"
      ],
      "metadata": {
        "id": "TF6gJ1PpgrG8"
      },
      "id": "TF6gJ1PpgrG8"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "def display_topics(H, W, feature_names, documents, no_top_words, no_top_documents):\n",
        "    for topic_idx, topic in enumerate(H):\n",
        "        print(f\"Topic {topic_idx}:\")\n",
        "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
        "\n",
        "documents = df['text'].tolist()\n",
        "\n",
        "vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words=stopwords_ext) # Modify stopwords if needed\n",
        "dtm = vectorizer.fit_transform(documents)\n",
        "\n",
        "num_topics = 10\n",
        "lda = LatentDirichletAllocation(n_components=num_topics, random_state=0)\n",
        "lda.fit(dtm)\n",
        "\n",
        "\n",
        "no_top_words = 10\n",
        "no_top_documents = 10\n",
        "\n",
        "display_topics(lda.components_, lda.transform(dtm), vectorizer.get_feature_names_out(), documents, no_top_words, no_top_documents)"
      ],
      "metadata": {
        "id": "y0V0l_03ouQW"
      },
      "id": "y0V0l_03ouQW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visualize topics"
      ],
      "metadata": {
        "id": "8WPGBwAnqDEU"
      },
      "id": "8WPGBwAnqDEU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88b65cad"
      },
      "source": [
        "## 5. Visualize word collocation"
      ],
      "id": "88b65cad"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}