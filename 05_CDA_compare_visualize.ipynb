{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Cultural Data Analysis\n",
        "\n",
        "Introduction to working with datasets"
      ],
      "metadata": {
        "id": "7CC34uuNzNxY"
      },
      "id": "7CC34uuNzNxY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76a4e150-cc6f-4878-a32b-e78a1d6426ae",
      "metadata": {
        "id": "76a4e150-cc6f-4878-a32b-e78a1d6426ae"
      },
      "outputs": [],
      "source": [
        "# import necessary libraries\n",
        "import os, re, csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gensim, nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from collections import Counter\n",
        "from itertools import islice\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "import string\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "Dx2o0IVXXqbu"
      },
      "id": "Dx2o0IVXXqbu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "7010dda9-acc5-4d90-b175-90012564d13c",
      "metadata": {
        "id": "7010dda9-acc5-4d90-b175-90012564d13c"
      },
      "source": [
        "## Loading the dataset: heritage homes webistes\n",
        "\n",
        "The dataset is stored in a shared google drive:\n",
        "https://drive.google.com/drive/folders/11Shm0edDOiWrOe56fzJQRZi-v_BPSW8E?usp=drive_link\n",
        "\n",
        "Add it to your drive.\n",
        "\n",
        "To access it, load your gdrive in 'Files' (see left pane of the notebook in google colab) and navigate to the shared folder. You may need to click on 'refresh' to make it appear on the list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d42429d3-63fe-4b79-b341-160057e5dcbc",
      "metadata": {
        "id": "d42429d3-63fe-4b79-b341-160057e5dcbc"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Country code: change here between 'NL' and 'UK'\n",
        "cc = 'FR'"
      ],
      "metadata": {
        "id": "QYiHwjcORrPC"
      },
      "id": "QYiHwjcORrPC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gdrive_path = '/content/gdrive/MyDrive/CDA/'"
      ],
      "metadata": {
        "id": "bbjhZ8nKZtZC"
      },
      "id": "bbjhZ8nKZtZC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data_file = gdrive_path+cc+'_dataset_website-content-crawler.json'"
      ],
      "metadata": {
        "id": "yCPPY_4I2pIZ"
      },
      "id": "yCPPY_4I2pIZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8697b51f-50a5-4091-9cc1-0aed1308b27d",
      "metadata": {
        "id": "8697b51f-50a5-4091-9cc1-0aed1308b27d"
      },
      "outputs": [],
      "source": [
        "# Import json data from Aipfy scraping\n",
        "df=pd.read_json(raw_data_file)\n",
        "# select only two columns for analysis: url and text\n",
        "df=df[['url','text']]\n",
        "# Print the DataFrame\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Join all pages from a domain to an entry in the analysis. To do this, add a new column which will contain only the main domain name."
      ],
      "metadata": {
        "id": "Rr6hiPQ-4z5O"
      },
      "id": "Rr6hiPQ-4z5O"
    },
    {
      "cell_type": "code",
      "source": [
        "# function to extract the main domain from the url in the dataset\n",
        "def extract_main_domain(url):\n",
        "    if not isinstance(str(url), str):\n",
        "        print('NOT VALID',url)\n",
        "        return None\n",
        "    match = re.findall('(?:\\w+\\.)*\\w+\\.\\w*', str(url)) #'www\\.?([^/]+)'\n",
        "    return match[0].lstrip('www.') if match else None"
      ],
      "metadata": {
        "id": "_Px9Aoim4-pq"
      },
      "id": "_Px9Aoim4-pq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47db9deb-8836-47fb-9f74-28a023bcb5d7",
      "metadata": {
        "id": "47db9deb-8836-47fb-9f74-28a023bcb5d7"
      },
      "outputs": [],
      "source": [
        "# Load the list of domains from a csv file:\n",
        "cc_column = cc+' domains'\n",
        "#print(cc_column)\n",
        "\n",
        "urls = pd.read_csv(gdrive_path+cc+'_urls.csv')[cc_column].values.tolist()\n",
        "\n",
        "# Extract main domains from nl_urls\n",
        "domains = {extract_main_domain(url) for url in urls if extract_main_domain(url) is not None}\n",
        "\n",
        "# Check if main domains in list_of_links match any domain in nl_domains\n",
        "matching_links = [link for link in df.url if extract_main_domain(link) in domains]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc9d2331-fe01-4e94-b984-00ac834a771c",
      "metadata": {
        "id": "cc9d2331-fe01-4e94-b984-00ac834a771c"
      },
      "outputs": [],
      "source": [
        "# Add a new column 'domain' and fill it by applying the extract_main_domain function to the 'url' column\n",
        "df['domain'] = df['url'].apply(extract_main_domain)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08144921-d7be-45ed-8795-1c085fb2640b",
      "metadata": {
        "id": "08144921-d7be-45ed-8795-1c085fb2640b"
      },
      "source": [
        "## Understand meaningful words collocations"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preparing the text (stopwords, lemmatization, etc)"
      ],
      "metadata": {
        "id": "inCepASp67ru"
      },
      "id": "inCepASp67ru"
    },
    {
      "cell_type": "code",
      "source": [
        "# make all stopword files stored in github available in this notebook:\n",
        "!wget 'https://raw.githubusercontent.com/jazoza/cultural-data-analysis/refs/heads/main/stopwords_archive/NL.txt'\n",
        "!wget 'https://raw.githubusercontent.com/jazoza/cultural-data-analysis/refs/heads/main/stopwords_archive/UK.txt'\n",
        "!wget 'https://raw.githubusercontent.com/jazoza/cultural-data-analysis/refs/heads/main/stopwords_archive/DE.txt'\n",
        "!wget 'https://raw.githubusercontent.com/jazoza/cultural-data-analysis/refs/heads/main/stopwords_archive/FR.txt'"
      ],
      "metadata": {
        "id": "rKDwducX7kqD"
      },
      "id": "rKDwducX7kqD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load a list of 'stopwords' in the language you are analyzing\n",
        "def get_stopwords_list(stop_file_path):\n",
        "    \"\"\"load stop words \"\"\"\n",
        "    with open(stop_file_path, 'r', encoding=\"utf-8\") as f:\n",
        "        stopwords = f.readlines()\n",
        "        stop_set = set(m.strip() for m in stopwords)\n",
        "        return list(frozenset(stop_set))\n",
        "stopwords_path = cc+\".txt\"\n",
        "stopwords = get_stopwords_list(stopwords_path)"
      ],
      "metadata": {
        "id": "_bCxFJzY69_b"
      },
      "id": "_bCxFJzY69_b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extend the stopwords list with any other words you want to exclude from analysis\n",
        "special_stop_words = ['nbsp', 'www', ' ', '', '—', '\\’s', 'ii', 'iii', 'iiii', 'l\\’']\n",
        "stopwords_ext = stopwords+special_stop_words"
      ],
      "metadata": {
        "id": "HgNkZORH75QF"
      },
      "id": "HgNkZORH75QF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to remove non-ascii characters\n",
        "def _removeNonAscii(s): return \"\".join(i for i in s if ord(i)<128)"
      ],
      "metadata": {
        "id": "0SuPUQ_4VrfT"
      },
      "id": "0SuPUQ_4VrfT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m spacy download nl_core_news_sm"
      ],
      "metadata": {
        "id": "aSu2nqTUp_as"
      },
      "id": "aSu2nqTUp_as",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load spacy\n",
        "\n",
        "nlp = spacy.load(\"fr_core_news_sm\")\n",
        "#nlp = spacy.load('fr_core_web_sm')"
      ],
      "metadata": {
        "id": "8lCObUihV8Yj"
      },
      "id": "8lCObUihV8Yj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to clean and lemmatize comments\n",
        "def clean_documents(text):\n",
        "    #remove punctuations\n",
        "    regex = re.compile('[' + re.escape(string.punctuation) + '\\\\r\\\\t\\\\n]')\n",
        "    nopunct = regex.sub(\" \", str(text))\n",
        "    #use spacy to lemmatize comments\n",
        "    doc = nlp(nopunct, disable=['parser','ner'])\n",
        "    lemma = [token.lemma_ for token in doc]\n",
        "    return lemma"
      ],
      "metadata": {
        "id": "yeR7duSYWD9p"
      },
      "id": "yeR7duSYWD9p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#apply function to clean and lemmatize comments\n",
        "lemmatized = df.text.map(clean_documents)\n",
        "#make sure to lowercase everything\n",
        "lemmatized = lemmatized.map(lambda x: [word.lower() for word in x])\n",
        "lemmatized.head()"
      ],
      "metadata": {
        "id": "7VsB2yq1WObL"
      },
      "id": "7VsB2yq1WObL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unlist_documents = [item for items in lemmatized for item in items]"
      ],
      "metadata": {
        "id": "IS3V0pl3Xksv"
      },
      "id": "IS3V0pl3Xksv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save these outputs for later\n",
        "with open(gdrive_path+'jar/'+cc+'_lemmatized.pickle', 'wb') as handle_l:\n",
        "    pickle.dump(lemmatized, handle_l, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open(gdrive_path+'jar/'+cc+'_unlist_documents.pickle', 'wb') as handle_u:\n",
        "    pickle.dump(unlist_documents, handle_u, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "VGoew_tHZLpQ"
      },
      "id": "VGoew_tHZLpQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load lemmatized text data as document lists\n",
        "\n",
        "You can see how to produce lemmatized text data in week 4:\n",
        "- [4. HH Narratives](https://colab.research.google.com/github/jazoza/cultural-data-analysis/blob/main/04_CDA_HH_narratives.ipynb)"
      ],
      "metadata": {
        "id": "Z5Ni3BHXtE1-"
      },
      "id": "Z5Ni3BHXtE1-"
    },
    {
      "cell_type": "code",
      "source": [
        "# load saved pickles\n",
        "with open(gdrive_path+'jar/'+cc+'_lemmatized.pickle', 'rb') as handle_l:\n",
        "    lemmatized = pickle.load(handle_l)\n",
        "\n",
        "with open(gdrive_path+'jar/'+cc+'_unlist_documents.pickle', 'rb') as handle_u:\n",
        "    unlist_documents = pickle.load(handle_u)"
      ],
      "metadata": {
        "id": "5aBr85g3ajFB"
      },
      "id": "5aBr85g3ajFB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "id": "Hbkku34SfpU1"
      },
      "id": "Hbkku34SfpU1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initiate bigrams and trigrams\n",
        "bigrams = nltk.collocations.BigramAssocMeasures()\n",
        "trigrams = nltk.collocations.TrigramAssocMeasures()"
      ],
      "metadata": {
        "id": "3IaXW0GXX4X2"
      },
      "id": "3IaXW0GXX4X2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# identify all collocations in the flat list of words from all documents\n",
        "bigramFinder = nltk.collocations.BigramCollocationFinder.from_words(unlist_documents)\n",
        "trigramFinder = nltk.collocations.TrigramCollocationFinder.from_words(unlist_documents)"
      ],
      "metadata": {
        "id": "6L2vI56MX9ps"
      },
      "id": "6L2vI56MX9ps",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute basic bigram fequency\n",
        "bigramFreqTable = pd.DataFrame(list(bigramFinder.ngram_fd.items()), columns=['bigram','freq']).sort_values(by='freq', ascending=False)\n",
        "# compute basic tri fequency\n",
        "trigramFreqTable = pd.DataFrame(list(trigramFinder.ngram_fd.items()), columns=['trigram','freq']).sort_values(by='freq', ascending=False)"
      ],
      "metadata": {
        "id": "4dCLatTKfJDe"
      },
      "id": "4dCLatTKfJDe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find meaningful bi- and tri-grams by filtering adjectives and nouns based on an nltk functionality"
      ],
      "metadata": {
        "id": "W8JjofCAYNdC"
      },
      "id": "W8JjofCAYNdC"
    },
    {
      "cell_type": "code",
      "source": [
        "#function to filter for ADJ/NN bigrams\n",
        "def rightTypes(ngram):\n",
        "    for word in ngram:\n",
        "        _removeNonAscii(word)\n",
        "        if word in stopwords_ext:\n",
        "            return False\n",
        "        if len(word) <= 2:\n",
        "            return False\n",
        "    acceptable_types = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n",
        "    second_type = ('NN', 'NNS', 'NNP', 'NNPS')\n",
        "    tags = nltk.pos_tag(ngram)\n",
        "    if tags[0][1] in acceptable_types and tags[1][1] in second_type:\n",
        "        return True\n",
        "    else:\n",
        "        return False"
      ],
      "metadata": {
        "id": "OryUdGaYYMGs"
      },
      "id": "OryUdGaYYMGs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#filter bigrams\n",
        "filtered_bi = bigramFreqTable[bigramFreqTable.bigram.map(lambda x: rightTypes(x))]\n",
        "filtered_bi[:20]"
      ],
      "metadata": {
        "id": "a6HFZOAafcqP"
      },
      "id": "a6HFZOAafcqP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Search for bigrams containing specific words"
      ],
      "metadata": {
        "id": "Ab8vHOoauHfP"
      },
      "id": "Ab8vHOoauHfP"
    },
    {
      "cell_type": "code",
      "source": [
        "# define the search term by changing the word 'château' to a word you want to explore\n",
        "search_term = 'château'\n",
        "# Filter for bigrams containing the word 'visite'\n",
        "search_bigrams = filtered_bi[filtered_bi['bigram'].apply(lambda x: search_term in x)]\n",
        "\n",
        "# print 20 most frequent collocations of the searc_term\n",
        "# if invalid, try changing the number 20 to a smaller value\n",
        "search_bigrams.sort_index(ascending=False)[:20]"
      ],
      "metadata": {
        "id": "vVjv_jxwuEdY"
      },
      "id": "vVjv_jxwuEdY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rightTypesTri(ngram):\n",
        "    for word in ngram:\n",
        "        _removeNonAscii(word)\n",
        "        if word in stopwords_ext:\n",
        "            return False\n",
        "        if len(word) <= 2:\n",
        "            return False\n",
        "    first_type = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n",
        "    third_type = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n",
        "    tags = nltk.pos_tag(ngram)\n",
        "    if tags[0][1] in first_type and tags[2][1] in third_type:\n",
        "        return True\n",
        "    else:\n",
        "        return False"
      ],
      "metadata": {
        "id": "x_P8w-6-jbJj"
      },
      "id": "x_P8w-6-jbJj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#filter trigrams\n",
        "filtered_tri = trigramFreqTable[trigramFreqTable.trigram.map(lambda x: rightTypes(x))]\n",
        "filtered_tri[:20]"
      ],
      "metadata": {
        "id": "VyQRFtyBjp6M"
      },
      "id": "VyQRFtyBjp6M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the search term by changing the word 'château' to a word you want to explore\n",
        "search_term = 'château'\n",
        "# Filter for bigrams containing the word 'visite'\n",
        "search_trigrams = filtered_tri[filtered_tri['trigram'].apply(lambda x: search_term in x)]\n",
        "\n",
        "# print 20 most frequent collocations of the searc_term\n",
        "# if invalid, try changing the number 20 to a smaller value\n",
        "search_trigrams.sort_index(ascending=False)[:20]"
      ],
      "metadata": {
        "id": "RYhtt60cvn3U"
      },
      "id": "RYhtt60cvn3U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic Modeling\n",
        "\n",
        "Use [Latent-dirichlet allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) to extract topics from the text based on a statistical model"
      ],
      "metadata": {
        "id": "TF6gJ1PpgrG8"
      },
      "id": "TF6gJ1PpgrG8"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n",
        "\n",
        "NUM_TOPICS = 10\n",
        "\n",
        "# data to work with: list of tweets\n",
        "documents = df['text'].tolist()\n",
        "\n",
        "vectorizer = CountVectorizer(min_df=5, max_df=0.9, stop_words=stopwords_ext, lowercase=True, token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')\n",
        "documents_vectorized = vectorizer.fit_transform(documents)\n",
        "# Build a Latent Dirichlet Allocation Model\n",
        "lda_model = LatentDirichletAllocation(n_components=NUM_TOPICS, max_iter=10, learning_method='online')\n",
        "lda_Z = lda_model.fit_transform(documents_vectorized)\n",
        "print(lda_Z.shape)  # (NO_DOCUMENTS, NO_TOPICS)\n",
        "(393104, 10)\n",
        "def print_topics(model, vectorizer, top_n=10):\n",
        "    for idx, topic in enumerate(model.components_):\n",
        "        print(\"Topic %d:\" % (idx))\n",
        "        print([(vectorizer.get_feature_names_out()[i], topic[i])\n",
        "                        for i in topic.argsort()[:-top_n - 1:-1]])\n",
        "\n",
        "print(\"LDA Model:\")\n",
        "print_topics(lda_model, vectorizer)"
      ],
      "metadata": {
        "id": "LX_B4pBbmJ7g"
      },
      "id": "LX_B4pBbmJ7g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "def display_topics(H, W, feature_names, documents, no_top_words, no_top_documents):\n",
        "    for topic_idx, topic in enumerate(H):\n",
        "        print(f\"Topic {topic_idx}:\")\n",
        "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
        "\n",
        "documents = df['text'].tolist()\n",
        "\n",
        "vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words=stopwords_ext) # Modify stopwords if needed\n",
        "dtm = vectorizer.fit_transform(documents)\n",
        "\n",
        "num_topics = 10\n",
        "lda = LatentDirichletAllocation(n_components=num_topics, random_state=0)\n",
        "lda.fit(dtm)\n",
        "\n",
        "\n",
        "no_top_words = 10\n",
        "no_top_documents = 10\n",
        "\n",
        "display_topics(lda.components_, lda.transform(dtm), vectorizer.get_feature_names_out(), documents, no_top_words, no_top_documents)"
      ],
      "metadata": {
        "id": "y0V0l_03ouQW"
      },
      "id": "y0V0l_03ouQW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visualize topics"
      ],
      "metadata": {
        "id": "8WPGBwAnqDEU"
      },
      "id": "8WPGBwAnqDEU"
    },
    {
      "cell_type": "code",
      "source": [
        "from bokeh.io import push_notebook, show, output_notebook\n",
        "from bokeh.plotting import figure\n",
        "from bokeh.models import ColumnDataSource, LabelSet\n",
        "from bokeh.io import output_notebook\n",
        "output_notebook()\n",
        "\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "documents_vectorized = vectorizer.fit_transform(documents)\n",
        "# plotting documents in 2D\n",
        "svd = TruncatedSVD(n_components=20)\n",
        "documents_2d = svd.fit_transform(documents_vectorized)\n",
        "\n",
        "bok_df = pd.DataFrame(columns=['x', 'y', 'document'])\n",
        "bok_df['x'], bok_df['y'], bok_df['document'] = documents_2d[:,0], documents_2d[:,1], range(len(documents))\n",
        "\n",
        "source = ColumnDataSource(ColumnDataSource.from_df(df))\n",
        "labels = LabelSet(x=\"x\", y=\"y\", text=\"document\", y_offset=8,\n",
        "                  text_font_size=\"8pt\", text_color=\"#555555\",\n",
        "                  source=source, text_align='center')\n",
        "\n",
        "plot=figure(min_width=1000, height=600)\n",
        "#plot = figure(plot_width=1000, plot_height=600)\n",
        "plot.scatter(\"x\", \"y\", size=12, source=source, line_color=\"black\", fill_alpha=0.4)\n",
        "plot.add_layout(labels)\n",
        "show(plot, notebook_handle=True)"
      ],
      "metadata": {
        "id": "GDt1_G9mozQl"
      },
      "id": "GDt1_G9mozQl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorizing the corpus"
      ],
      "metadata": {
        "id": "cRJxcGbsWdv8"
      },
      "id": "cRJxcGbsWdv8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word2Vec model, explore and visualize"
      ],
      "metadata": {
        "id": "8nURhyHN8ymX"
      },
      "id": "8nURhyHN8ymX"
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "4YrTkLDz9hjC"
      },
      "id": "4YrTkLDz9hjC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bca25ef7-5a19-41e3-a199-bdde7304e575",
      "metadata": {
        "id": "bca25ef7-5a19-41e3-a199-bdde7304e575"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# X is a list of tokenized texts (i.e. list of lists of tokens)\n",
        "X = [word_tokenize(item) for item in df.text.tolist()]\n",
        "#print(X[0:3])\n",
        "model = gensim.models.Word2Vec(X, min_count=6, vector_size=200) # min_count: how many times a word appears in the corpus; size: number of dimensions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar(positive=[\"schloss\"], topn=12)"
      ],
      "metadata": {
        "id": "QS_aLSZc87XP"
      },
      "id": "QS_aLSZc87XP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize keywords with t-SNE\n",
        "\n",
        "Choose keywords that correspond to your analysis and visualize how they and their closest terms are distributed in the discourse.\n",
        "Use t-SNE to visualize the relations.\n",
        "\n",
        "* [t-Distributed Stochastic Neighbor Embedding (t-SNE)](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) is a technique for dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets.\n"
      ],
      "metadata": {
        "id": "kPpqk0TYab3j"
      },
      "id": "kPpqk0TYab3j"
    },
    {
      "cell_type": "code",
      "source": [
        "keys_role = ['Man', 'Frau']\n",
        "\n",
        "embedding_clusters = []\n",
        "word_clusters = []\n",
        "for word in keys_role:\n",
        "    embeddings = []\n",
        "    words = []\n",
        "    for similar_word, _ in model.wv.most_similar(word, topn=30):\n",
        "        words.append(similar_word)\n",
        "        embeddings.append(model.wv[similar_word])\n",
        "    embedding_clusters.append(embeddings)\n",
        "    word_clusters.append(words)"
      ],
      "metadata": {
        "id": "Xuyj8RYhXRPz"
      },
      "id": "Xuyj8RYhXRPz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "embedding_clusters = np.array(embedding_clusters)\n",
        "n, m, k = embedding_clusters.shape\n",
        "tsne_model_en_2d = TSNE(perplexity=15, n_components=2, init='pca', n_iter=3500, random_state=32)\n",
        "embeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, 2)"
      ],
      "metadata": {
        "id": "wL-7z_InXSX0"
      },
      "id": "wL-7z_InXSX0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "def tsne_plot_similar_words(title, labels, embedding_clusters, word_clusters, a, filename=None):\n",
        "    plt.figure(figsize=(16, 9))\n",
        "    colors = cm.rainbow(np.linspace(0, 1, len(labels)))\n",
        "    for label, embeddings, words, color in zip(labels, embedding_clusters, word_clusters, colors):\n",
        "        x = embeddings[:, 0]\n",
        "        y = embeddings[:, 1]\n",
        "        plt.scatter(x, y, c=color, alpha=a, label=label)\n",
        "        for i, word in enumerate(words):\n",
        "            plt.annotate(word, alpha=0.5, xy=(x[i], y[i]), xytext=(5, 2),\n",
        "                         textcoords='offset points', ha='right', va='bottom', size=8)\n",
        "    plt.legend(loc=4)\n",
        "    plt.title(title)\n",
        "    plt.grid(True)\n",
        "    if filename:\n",
        "        plt.savefig(filename, format='png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "tsne_plot_similar_words('Similar words from '+cc+' heritage houses website', keys_role, embeddings_en_2d, word_clusters, 0.7,\n",
        "                        'similar_words.png')"
      ],
      "metadata": {
        "id": "5tgZbwmbWdeT"
      },
      "id": "5tgZbwmbWdeT",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}