{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Cultural Data Analysis\n",
        "\n",
        "Introduction to working with datasets"
      ],
      "metadata": {
        "id": "7CC34uuNzNxY"
      },
      "id": "7CC34uuNzNxY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76a4e150-cc6f-4878-a32b-e78a1d6426ae",
      "metadata": {
        "id": "76a4e150-cc6f-4878-a32b-e78a1d6426ae"
      },
      "outputs": [],
      "source": [
        "# import necessary libraries\n",
        "import os, re, csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from collections import Counter\n",
        "from itertools import islice\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "import string\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7010dda9-acc5-4d90-b175-90012564d13c",
      "metadata": {
        "id": "7010dda9-acc5-4d90-b175-90012564d13c"
      },
      "source": [
        "## Loading the dataset: heritage homes webistes\n",
        "\n",
        "The dataset is stored in a shared google drive:\n",
        "https://drive.google.com/drive/folders/11Shm0edDOiWrOe56fzJQRZi-v_BPSW8E?usp=drive_link\n",
        "\n",
        "Add it to your drive.\n",
        "\n",
        "To access it, load your gdrive in 'Files' (see left pane of the notebook in google colab) and navigate to the shared folder. You may need to click on 'refresh' to make it appear on the list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d42429d3-63fe-4b79-b341-160057e5dcbc",
      "metadata": {
        "id": "d42429d3-63fe-4b79-b341-160057e5dcbc"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Country code: change here between 'NL' and 'UK'\n",
        "cc = 'UK'"
      ],
      "metadata": {
        "id": "QYiHwjcORrPC"
      },
      "id": "QYiHwjcORrPC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gdrive_path = '/content/gdrive/MyDrive/CDA/'"
      ],
      "metadata": {
        "id": "bbjhZ8nKZtZC"
      },
      "id": "bbjhZ8nKZtZC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# open the data for one country (cc)\n",
        "raw_data_file = gdrive_path+cc+'_dataset_website-content-crawler.json'"
      ],
      "metadata": {
        "id": "yCPPY_4I2pIZ"
      },
      "id": "yCPPY_4I2pIZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8697b51f-50a5-4091-9cc1-0aed1308b27d",
      "metadata": {
        "id": "8697b51f-50a5-4091-9cc1-0aed1308b27d"
      },
      "outputs": [],
      "source": [
        "# Import json data\n",
        "df=pd.read_json(raw_data_file)\n",
        "\n",
        "# Print the DataFrame\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check if there are further datasets to add per country\n",
        "\n",
        "!ls \"$gdrive_path\" | grep 'UK'"
      ],
      "metadata": {
        "id": "uffqK0WpzBmi"
      },
      "id": "uffqK0WpzBmi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_missing1 = pd.read_json(gdrive_path+'/UK_EH_dataset_website-content-crawler_2025-03-26_09-11-52-434.json')\n",
        "df_missing2 = pd.read_json(gdrive_path+'/UK_NH_dataset_website-content-crawler_2025-03-26_16-28-44-248.json')\n",
        "df_missing3 = pd.read_json(gdrive_path+'/UK_PC_dataset_website-content-crawler_2025-03-11_12-28-08-810.json')\n",
        "result = pd.concat([df, df_missing1, df_missing2, df_missing3])\n",
        "df = result\n",
        "df.head()"
      ],
      "metadata": {
        "id": "awG55UfmzkCr"
      },
      "id": "awG55UfmzkCr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a4128d6-4115-47a6-9e78-5a517bded844",
      "metadata": {
        "id": "6a4128d6-4115-47a6-9e78-5a517bded844"
      },
      "outputs": [],
      "source": [
        "# select only two columns for analysis: url and text\n",
        "df=df[['url','text']]\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Join all pages from a domain to an entry in the analysis. To do this, add a new column which will contain only the main domain name."
      ],
      "metadata": {
        "id": "Rr6hiPQ-4z5O"
      },
      "id": "Rr6hiPQ-4z5O"
    },
    {
      "cell_type": "code",
      "source": [
        "# function to extract the main domain from the url in the dataset\n",
        "def extract_main_domain(url):\n",
        "    if not isinstance(str(url), str):\n",
        "        print('NOT VALID',url)\n",
        "        return None\n",
        "    match = re.findall('(?:\\w+\\.)*\\w+\\.\\w*', str(url)) #'www\\.?([^/]+)'\n",
        "    return match[0].lstrip('www.') if match else None"
      ],
      "metadata": {
        "id": "_Px9Aoim4-pq"
      },
      "id": "_Px9Aoim4-pq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47db9deb-8836-47fb-9f74-28a023bcb5d7",
      "metadata": {
        "id": "47db9deb-8836-47fb-9f74-28a023bcb5d7"
      },
      "outputs": [],
      "source": [
        "# Load the list of domains from a csv file:\n",
        "cc_column = cc+' domains'\n",
        "#print(cc_column)\n",
        "\n",
        "urls = pd.read_csv(gdrive_path+'url_lists/'+cc+'_urls.csv')[cc_column].values.tolist()\n",
        "\n",
        "# Extract main domains from nl_urls\n",
        "domains = {extract_main_domain(url) for url in urls if extract_main_domain(url) is not None}\n",
        "\n",
        "# Check if main domains in list_of_links match any domain in nl_domains\n",
        "matching_links = [link for link in df.url if extract_main_domain(link) in domains]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a2e8c0b-4c0e-4445-a5bb-2f1695ad353e",
      "metadata": {
        "id": "2a2e8c0b-4c0e-4445-a5bb-2f1695ad353e"
      },
      "outputs": [],
      "source": [
        "# this cell can be skipped, it is only for verification\n",
        "\n",
        "# check how many lines in the dataframe have a matching link to the list of urls\n",
        "print(len(matching_links))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc9d2331-fe01-4e94-b984-00ac834a771c",
      "metadata": {
        "id": "cc9d2331-fe01-4e94-b984-00ac834a771c"
      },
      "outputs": [],
      "source": [
        "# Add a new column 'domain' and fill it by applying the extract_main_domain function to the 'url' column\n",
        "df['domain'] = df['url'].apply(extract_main_domain)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A. Scrape webpages: screenshots\n",
        "\n",
        "Automatically make screenshots of all urls, and save them to your local drive for later analysis.\n",
        "\n",
        "Code to do this is available elsewhere.\n",
        "\n",
        "Analyze: https://medium.com/@sehjadkhoja0/title-exploring-and-analyzing-image-data-with-python-79a7f72f4d2b"
      ],
      "metadata": {
        "id": "y87AvOKE5UKj"
      },
      "id": "y87AvOKE5UKj"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fb8aaf0"
      },
      "source": [
        "# Install necessary libraries\n",
        "!pip install selenium webdriver-manager"
      ],
      "id": "9fb8aaf0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0011a405"
      },
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "import time\n",
        "\n",
        "# set the output directory to be your current session's sample_data (will be lost when you close the window)\n",
        "output_dir = '/content/sample_data/'"
      ],
      "id": "0011a405",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5734e29"
      },
      "source": [
        "# Install Playwright library\n",
        "!pip install playwright\n",
        "# Install Playwright's browser binaries (Chromium, Firefox, WebKit)\n",
        "!playwright install"
      ],
      "id": "d5734e29",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5382d1f"
      },
      "source": [
        "import asyncio\n",
        "from playwright.async_api import async_playwright\n",
        "\n",
        "# Define the output directory\n",
        "output_dir = '/content/sample_data/'\n",
        "\n",
        "async def take_screenshots():\n",
        "    async with async_playwright() as p:\n",
        "        # Use Chromium as the browser\n",
        "        browser = await p.chromium.launch(headless=True)\n",
        "        page = await browser.new_page()\n",
        "\n",
        "        for i, url in enumerate(urls):\n",
        "            try:\n",
        "                print(f\"Navigating to: {url}\")\n",
        "                await page.goto(url, wait_until='networkidle') # wait for network to be idle\n",
        "\n",
        "                # Sanitize the URL to create a valid filename\n",
        "                filename = f\"screenshot_{i}_{url.replace('https://', '').replace('http://', '').replace('/', '_').replace('.', '_')}.png\"\n",
        "                filepath = os.path.join(output_dir, filename)\n",
        "\n",
        "                # Take full-page screenshot\n",
        "                await page.screenshot(path=filepath, full_page=True)\n",
        "                print(f\"Screenshot saved: {filepath}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error taking screenshot for {url}: {e}\")\n",
        "        await browser.close()\n",
        "        print(\"Screenshot process completed.\")\n",
        "\n",
        "# Run the async function\n",
        "await take_screenshots()"
      ],
      "id": "e5382d1f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LgrGtwiahqAI"
      },
      "id": "LgrGtwiahqAI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## B. Discourse Analysis"
      ],
      "metadata": {
        "id": "tRv48s-JboPN"
      },
      "id": "tRv48s-JboPN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### B.1 Close reading one document\n",
        "(we will consider one website as a 'document')"
      ],
      "metadata": {
        "id": "WV5qWUIO1xA2"
      },
      "id": "WV5qWUIO1xA2"
    },
    {
      "cell_type": "code",
      "source": [
        "# list all unique domain names\n",
        "df['domain'].unique()"
      ],
      "metadata": {
        "id": "29umRTewbnDU"
      },
      "id": "29umRTewbnDU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract all rows (lines) where the value of 'domain' is 'lancashire.gov.uk'\n",
        "df[df['domain'] == 'lancashire.gov.uk']"
      ],
      "metadata": {
        "id": "a5mBBe-ncnfz"
      },
      "id": "a5mBBe-ncnfz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# then combine these into a list of pages\n",
        "document = df[df['domain'] == 'lancashire.gov.uk']['text'].tolist()"
      ],
      "metadata": {
        "id": "7SfU5XXfdN7l"
      },
      "id": "7SfU5XXfdN7l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document"
      ],
      "metadata": {
        "id": "ksw6zTgodjqL"
      },
      "id": "ksw6zTgodjqL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_document = []\n",
        "for i in document:\n",
        "  j = i.replace('\\n', ' ').replace('\\r', '')\n",
        "  clean_document.append(j)"
      ],
      "metadata": {
        "id": "_YEFxHo-ePV7"
      },
      "id": "_YEFxHo-ePV7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_document"
      ],
      "metadata": {
        "id": "kyc5LLlAeebS"
      },
      "id": "kyc5LLlAeebS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### B.2 Topic Modelling"
      ],
      "metadata": {
        "id": "inCepASp67ru"
      },
      "id": "inCepASp67ru"
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.feature_extraction.text as text\n",
        "\n",
        "# min_df: ignore words occurring in fewer than `n` documents\n",
        "# stop_words: ignore very common words (\"the\", \"and\", \"or\", \"to\", ...)\n",
        "vec = text.CountVectorizer(lowercase=True, min_df=100, stop_words='english')\n",
        "dtm = vec.fit_transform(df['text'])"
      ],
      "metadata": {
        "id": "Gja1IgP932EG"
      },
      "id": "Gja1IgP932EG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Shape of document-term matrix: {dtm.shape}. '\n",
        "      f'Number of tokens {dtm.sum()}')"
      ],
      "metadata": {
        "id": "MEDoEJ1W4BfX"
      },
      "id": "MEDoEJ1W4BfX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.decomposition as decomposition\n",
        "NUM_TOPICS = 10\n",
        "lda_model = decomposition.LatentDirichletAllocation(\n",
        "    n_components=NUM_TOPICS, learning_method='online', random_state=1)\n",
        "lda_Z = lda_model.fit_transform(dtm)\n",
        "print(lda_Z.shape)  # (NO_DOCUMENTS, NO_TOPICS)"
      ],
      "metadata": {
        "id": "gMYvBI_h6UNh"
      },
      "id": "gMYvBI_h6UNh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_topic_distributions = lda_model.fit_transform(dtm)"
      ],
      "metadata": {
        "id": "b2jaGSW84Zcz"
      },
      "id": "b2jaGSW84Zcz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_topics(model, vectorizer, top_n=10):\n",
        "    for idx, topic in enumerate(model.components_):\n",
        "        print(\"Topic %d:\" % (idx))\n",
        "        print([(vec.get_feature_names_out()[i], topic[i])\n",
        "                        for i in topic.argsort()[:-top_n - 1:-1]])\n",
        "\n",
        "print(\"LDA Model:\")\n",
        "print_topics(lda_model, vec)\n",
        "print(\"=\" * 20)"
      ],
      "metadata": {
        "id": "Whj_h2dd8U65"
      },
      "id": "Whj_h2dd8U65",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_top_words = 12\n",
        "no_top_documents = 5\n",
        "lda_H = lda_model.components_\n",
        "tf_feature_names = vec.get_feature_names_out()\n",
        "\n",
        "def display_topics(H, Z, feature_names, docs, no_top_words, no_top_documents):\n",
        "    for idx, topic in enumerate(H):\n",
        "        print(\"Topic %d:\" % (idx))\n",
        "        print(\"KEYWORDS\", \" \".join([feature_names[i]\n",
        "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
        "        top_doc_indices = np.argsort( Z[:,\n",
        "                                        idx] )[::-1][0:no_top_documents]\n",
        "        # good for checking which documents are the most characteristic for certain topics\n",
        "        for doc_index in top_doc_indices:\n",
        "            print(\"TOP DOCS\", docs[doc_index])\n",
        "\n",
        "display_topics(lda_H, lda_Z, tf_feature_names, df['text'].tolist(), no_top_words, no_top_documents)"
      ],
      "metadata": {
        "id": "gj7pbaiH8_IZ"
      },
      "id": "gj7pbaiH8_IZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "08144921-d7be-45ed-8795-1c085fb2640b",
      "metadata": {
        "id": "08144921-d7be-45ed-8795-1c085fb2640b"
      },
      "source": [
        "### B.3 Collocations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# change 'UK' here for country code by hand\n",
        "!wget 'https://raw.githubusercontent.com/jazoza/cultural-data-analysis/refs/heads/main/stopwords_archive/UK.txt'"
      ],
      "metadata": {
        "id": "rKDwducX7kqD"
      },
      "id": "rKDwducX7kqD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load a list of 'stopwords' in the language you are analyzing\n",
        "def get_stopwords_list(stop_file_path):\n",
        "    \"\"\"load stop words \"\"\"\n",
        "    with open(stop_file_path, 'r', encoding=\"utf-8\") as f:\n",
        "        stopwords = f.readlines()\n",
        "        stop_set = set(m.strip() for m in stopwords)\n",
        "        return list(frozenset(stop_set))\n",
        "stopwords_path = cc+\".txt\"\n",
        "stopwords = get_stopwords_list(stopwords_path)"
      ],
      "metadata": {
        "id": "_bCxFJzY69_b"
      },
      "id": "_bCxFJzY69_b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "special_stop_words = ['nbsp', ' ', '', '—', '\\’s']\n",
        "stopwords_ext = stopwords+special_stop_words"
      ],
      "metadata": {
        "id": "HgNkZORH75QF"
      },
      "id": "HgNkZORH75QF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to remove non-ascii characters\n",
        "def _removeNonAscii(s): return \"\".join(i for i in s if ord(i)<128)"
      ],
      "metadata": {
        "id": "0SuPUQ_4VrfT"
      },
      "id": "0SuPUQ_4VrfT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "8lCObUihV8Yj"
      },
      "id": "8lCObUihV8Yj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to clean and lemmatize comments\n",
        "def clean_documents(text):\n",
        "    #remove punctuations\n",
        "    regex = re.compile('[' + re.escape(string.punctuation) + '\\\\r\\\\t\\\\n]')\n",
        "    nopunct = regex.sub(\" \", str(text))\n",
        "    #use spacy to lemmatize comments\n",
        "    doc = nlp(nopunct, disable=['parser','ner'])\n",
        "    lemma = [token.lemma_ for token in doc]\n",
        "    return lemma"
      ],
      "metadata": {
        "id": "yeR7duSYWD9p"
      },
      "id": "yeR7duSYWD9p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#apply function to clean and lemmatize comments\n",
        "lemmatized = df.text.map(clean_documents)\n",
        "#make sure to lowercase everything\n",
        "lemmatized = lemmatized.map(lambda x: [word.lower() for word in x])\n",
        "lemmatized.head()"
      ],
      "metadata": {
        "id": "7VsB2yq1WObL"
      },
      "id": "7VsB2yq1WObL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unlist_documents = [item for items in lemmatized for item in items]"
      ],
      "metadata": {
        "id": "IS3V0pl3Xksv"
      },
      "id": "IS3V0pl3Xksv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You would use these commands to save lemmatized text into a 'pickle' for later use\n",
        "# The current setup does not enable you to overwrite existing files in the CDA/jar folder,\n",
        "# so you would have to save the 'pickle' files elsewhere (for example in sample_data folder)\n",
        "# If you want to reactivate this code, remove the tripple quotes ''' from the beginning and end\n",
        "'''\n",
        "# save these outputs for later\n",
        "with open(gdrive_path+'jar/lemmatized.pickle', 'wb') as handle_l:\n",
        "    pickle.dump(lemmatized, handle_l, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open(gdrive_path+'jar/unlist_documents.pickle', 'wb') as handle_u:\n",
        "    pickle.dump(unlist_documents, handle_u, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "  '''"
      ],
      "metadata": {
        "id": "VGoew_tHZLpQ"
      },
      "id": "VGoew_tHZLpQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load saved pickles\n",
        "with open(gdrive_path+'jar/'+cc+'_lemmatized.pickle', 'rb') as handle_l:\n",
        "    lemmatized = pickle.load(handle_l)\n",
        "\n",
        "with open(gdrive_path+'jar/'+cc+'_unlist_documents.pickle', 'rb') as handle_u:\n",
        "    unlist_documents = pickle.load(handle_u)"
      ],
      "metadata": {
        "id": "5aBr85g3ajFB"
      },
      "id": "5aBr85g3ajFB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "id": "Hbkku34SfpU1"
      },
      "id": "Hbkku34SfpU1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initiate bigrams and trigrams\n",
        "bigrams = nltk.collocations.BigramAssocMeasures()\n",
        "trigrams = nltk.collocations.TrigramAssocMeasures()"
      ],
      "metadata": {
        "id": "3IaXW0GXX4X2"
      },
      "id": "3IaXW0GXX4X2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# identify all collocations in the flat list of words from all documents\n",
        "bigramFinder = nltk.collocations.BigramCollocationFinder.from_words(unlist_documents)\n",
        "trigramFinder = nltk.collocations.TrigramCollocationFinder.from_words(unlist_documents)"
      ],
      "metadata": {
        "id": "6L2vI56MX9ps"
      },
      "id": "6L2vI56MX9ps",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate basic frequency"
      ],
      "metadata": {
        "id": "mRSuATcHYm0U"
      },
      "id": "mRSuATcHYm0U"
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_freq = bigramFinder.ngram_fd.items()"
      ],
      "metadata": {
        "id": "W9XCfughYim0"
      },
      "id": "W9XCfughYim0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigramFreqTable = pd.DataFrame(list(bigram_freq), columns=['bigram','freq']).sort_values(by='freq', ascending=False)"
      ],
      "metadata": {
        "id": "4dCLatTKfJDe"
      },
      "id": "4dCLatTKfJDe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigramFreqTable.head().reset_index(drop=True)"
      ],
      "metadata": {
        "id": "NHE_sQvofMKw"
      },
      "id": "NHE_sQvofMKw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute basic trigrams frequency\n",
        "trigram_freq = trigramFinder.ngram_fd.items()\n",
        "trigramFreqTable = pd.DataFrame(list(trigram_freq), columns=['trigram','freq']).sort_values(by='freq', ascending=False)\n",
        "trigramFreqTable[:10]"
      ],
      "metadata": {
        "id": "pYTAOfQGhfY3"
      },
      "id": "pYTAOfQGhfY3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find meaningful bi- and tri-grams by filtering adjectives and nouns based on an nltk functionality"
      ],
      "metadata": {
        "id": "W8JjofCAYNdC"
      },
      "id": "W8JjofCAYNdC"
    },
    {
      "cell_type": "code",
      "source": [
        "#function to filter for ADJ/NN bigrams\n",
        "def rightTypes(ngram):\n",
        "    for word in ngram:\n",
        "        if word in stopwords_ext:\n",
        "            return False\n",
        "    acceptable_types = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n",
        "    second_type = ('NN', 'NNS', 'NNP', 'NNPS')\n",
        "    tags = nltk.pos_tag(ngram)\n",
        "    if tags[0][1] in acceptable_types and tags[1][1] in second_type:\n",
        "        return True\n",
        "    else:\n",
        "        return False"
      ],
      "metadata": {
        "id": "OryUdGaYYMGs"
      },
      "id": "OryUdGaYYMGs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#filter bigrams\n",
        "filtered_bi = bigramFreqTable[bigramFreqTable.bigram.map(lambda x: rightTypes(x))]"
      ],
      "metadata": {
        "id": "a6HFZOAafcqP"
      },
      "id": "a6HFZOAafcqP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_bi[:10]"
      ],
      "metadata": {
        "id": "DoEO44Tzf-i8"
      },
      "id": "DoEO44Tzf-i8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use advanced statistical methods like the Chi-Square to identify meaninful collocations\n",
        "https://en.wikipedia.org/wiki/Chi-squared_test"
      ],
      "metadata": {
        "id": "LGPGqH0Xj98T"
      },
      "id": "LGPGqH0Xj98T"
    },
    {
      "cell_type": "code",
      "source": [
        "# filter bigrams using chi-square\n",
        "bigramChiTable = pd.DataFrame(list(bigramFinder.score_ngrams(bigrams.chi_sq)), columns=['bigram','chi-sq']).sort_values(by='chi-sq', ascending=False)\n",
        "bigramChiTable.head()"
      ],
      "metadata": {
        "id": "j8IPPymTjaql"
      },
      "id": "j8IPPymTjaql",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find meaningful trigrams by filtering basic frequency table\n",
        "# function to filter trigrams\n",
        "def rightTypesTri(ngram):\n",
        "    if '-pron-' in ngram or '' in ngram or ' 'in ngram or '  ' in ngram or 't' in ngram:\n",
        "        return False\n",
        "    for word in ngram:\n",
        "        if word in stopwords_ext:\n",
        "            return False\n",
        "    first_type = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n",
        "    third_type = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n",
        "    tags = nltk.pos_tag(ngram)\n",
        "    if tags[0][1] in first_type and tags[2][1] in third_type:\n",
        "        return True\n",
        "    else:\n",
        "        return False"
      ],
      "metadata": {
        "id": "W5wqPcjzh_lW"
      },
      "id": "W5wqPcjzh_lW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_tri = trigramFreqTable[trigramFreqTable.trigram.map(lambda x: rightTypesTri(x))]\n",
        "filtered_tri[:10]"
      ],
      "metadata": {
        "id": "3vQu3yv2jGRT"
      },
      "id": "3vQu3yv2jGRT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chi-sqare frequency calculation for trigrams\n",
        "trigramChiTable = pd.DataFrame(list(trigramFinder.score_ngrams(trigrams.chi_sq)), columns=['trigram','chi-sq']).sort_values(by='chi-sq', ascending=False)\n",
        "trigramChiTable.head(20)"
      ],
      "metadata": {
        "id": "OIr3r7XBkaqF"
      },
      "id": "OIr3r7XBkaqF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### B.4 Analyse specific collocations"
      ],
      "metadata": {
        "id": "2nFPgMYkiygf"
      },
      "id": "2nFPgMYkiygf"
    },
    {
      "cell_type": "code",
      "source": [
        "# search for words from this list or use another list\n",
        "search_words = ['royal']"
      ],
      "metadata": {
        "id": "LoJnS9K7h5Kd"
      },
      "id": "LoJnS9K7h5Kd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SCI-KIT method, produces lists of co-occurencies for specific terms\n",
        "def vectorize_text(df):\n",
        "    vectorizer = text.CountVectorizer()\n",
        "    X = vectorizer.fit_transform(df['text'])\n",
        "    return X, vectorizer\n",
        "\n",
        "def find_collocations(text, target_words):\n",
        "    words = text.split()\n",
        "    collocations = []\n",
        "    for i in range(len(words) - 1):\n",
        "        if words[i] in target_words:\n",
        "            collocations.append((words[i], words[i + 1]))\n",
        "        if words[i + 1] in target_words:\n",
        "            collocations.append((words[i + 1], words[i]))\n",
        "    return collocations\n",
        "\n",
        "def get_frequent_collocations(df, most_frequent_words):\n",
        "    collocations = []\n",
        "    for text in df['text']:\n",
        "        collocations.extend(find_collocations(text, most_frequent_words))\n",
        "    collocation_counts = Counter(collocations)\n",
        "    frequent_collocations = {}\n",
        "    for word in most_frequent_words:\n",
        "        word_collocations = {collocation: count for collocation, count in collocation_counts.items() if word in collocation}\n",
        "        frequent_collocations[word] = dict(islice(Counter(word_collocations).most_common(20), 20))\n",
        "    return frequent_collocations\n",
        "\n",
        "def analyze_word_collocations(df):\n",
        "    X, vectorizer = vectorize_text(df)\n",
        "    most_frequent_words = search_words\n",
        "    frequent_collocations = get_frequent_collocations(df, most_frequent_words)\n",
        "    return frequent_collocations"
      ],
      "metadata": {
        "id": "032f9482-879c-43f9-a9cd-3b1ca5d7056c"
      },
      "id": "032f9482-879c-43f9-a9cd-3b1ca5d7056c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2657cbe3-aa7e-4e4b-8d68-1d6050cf6f49",
      "metadata": {
        "id": "2657cbe3-aa7e-4e4b-8d68-1d6050cf6f49"
      },
      "outputs": [],
      "source": [
        "collocations = analyze_word_collocations(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fbef5d2-983b-49fa-8924-95abd24b2855",
      "metadata": {
        "id": "4fbef5d2-983b-49fa-8924-95abd24b2855"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "for word, colloc_dict in collocations.items():\n",
        "   for collocation, count in colloc_dict.items():\n",
        "       #collocation_str = ' '.join(collocation)  # Join collocation words into a single string\n",
        "       data.append([word, collocation[1], count])\n",
        "collocations_df = pd.DataFrame(data, columns=['Word', 'Collocation', 'Count'])\n",
        "print(collocations_df.to_markdown(index=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### B.5 Word2Vec model"
      ],
      "metadata": {
        "id": "8nURhyHN8ymX"
      },
      "id": "8nURhyHN8ymX"
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "4YrTkLDz9hjC"
      },
      "id": "4YrTkLDz9hjC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bca25ef7-5a19-41e3-a199-bdde7304e575",
      "metadata": {
        "id": "bca25ef7-5a19-41e3-a199-bdde7304e575"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# X is a list of tokenized texts (i.e. list of lists of tokens)\n",
        "X = [word_tokenize(item) for item in df.text.tolist()]\n",
        "#print(X[0:3])\n",
        "model = gensim.models.Word2Vec(X, min_count=6, vector_size=200) # min_count: how many times a word appears in the corpus; size: number of dimensions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar(positive=[\"royal\"], topn=12)"
      ],
      "metadata": {
        "id": "QS_aLSZc87XP"
      },
      "id": "QS_aLSZc87XP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ornUSbnUl9fN"
      },
      "id": "ornUSbnUl9fN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "595704a4"
      },
      "source": [
        "# Task\n",
        "Install Selenium and WebDriver Manager, set up a headless Chrome browser, then iterate through the URLs in `gdrive_path+'url_lists/'+cc+'_urls.csv'`, navigate to each URL, take a full-page screenshot, and save the screenshots to a new folder named `screenshots` within the `gdrive_path` directory."
      ],
      "id": "595704a4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a5bdff2"
      },
      "source": [
        "## Take Screenshots of URLs\n",
        "\n",
        "### Subtask:\n",
        "Install necessary libraries (Selenium, WebDriver Manager), set up a headless Chrome browser, iterate through the list of URLs, navigate to each, take a full-page screenshot, and save the screenshots to a dedicated folder within the specified Google Drive path.\n"
      ],
      "id": "6a5bdff2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40ae5fe1"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "- The required libraries, Selenium and WebDriver Manager, were successfully installed.\n",
        "- A headless Chrome browser environment was configured for automated web navigation.\n",
        "- Full-page screenshots were successfully captured for each URL specified in the `_urls.csv` file.\n",
        "- All generated screenshots were saved into a dedicated `screenshots` folder within the designated Google Drive path.\n",
        "\n",
        "### Insights or Next Steps\n",
        "- The captured screenshots are now available for subsequent visual inspection or automated analysis, which could include identifying content rendering issues or design inconsistencies.\n",
        "- The next logical step involves reviewing these screenshots for anomalies or differences, potentially leading to further investigation of the content or layout of the captured web pages.\n"
      ],
      "id": "40ae5fe1"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}