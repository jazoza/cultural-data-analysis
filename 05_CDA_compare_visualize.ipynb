{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Cultural Data Analysis\n",
        "\n",
        "Introduction to working with datasets"
      ],
      "metadata": {
        "id": "7CC34uuNzNxY"
      },
      "id": "7CC34uuNzNxY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76a4e150-cc6f-4878-a32b-e78a1d6426ae",
      "metadata": {
        "id": "76a4e150-cc6f-4878-a32b-e78a1d6426ae"
      },
      "outputs": [],
      "source": [
        "# import necessary libraries\n",
        "import os, re, csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gensim, nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from collections import Counter\n",
        "from itertools import islice\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "import string\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "Dx2o0IVXXqbu"
      },
      "id": "Dx2o0IVXXqbu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "7010dda9-acc5-4d90-b175-90012564d13c",
      "metadata": {
        "id": "7010dda9-acc5-4d90-b175-90012564d13c"
      },
      "source": [
        "## Loading the dataset: heritage homes webistes\n",
        "\n",
        "The dataset is stored in a shared google drive:\n",
        "https://drive.google.com/drive/folders/11Shm0edDOiWrOe56fzJQRZi-v_BPSW8E?usp=drive_link\n",
        "\n",
        "Add it to your drive.\n",
        "\n",
        "To access it, load your gdrive in 'Files' (see left pane of the notebook in google colab) and navigate to the shared folder. You may need to click on 'refresh' to make it appear on the list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d42429d3-63fe-4b79-b341-160057e5dcbc",
      "metadata": {
        "id": "d42429d3-63fe-4b79-b341-160057e5dcbc"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Country code: change here between 'NL' and 'UK'\n",
        "cc = 'UK'"
      ],
      "metadata": {
        "id": "QYiHwjcORrPC"
      },
      "id": "QYiHwjcORrPC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gdrive_path = '/content/gdrive/MyDrive/CDA/'"
      ],
      "metadata": {
        "id": "bbjhZ8nKZtZC"
      },
      "id": "bbjhZ8nKZtZC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data_file = gdrive_path+cc+'_dataset_website-content-crawler.json'"
      ],
      "metadata": {
        "id": "yCPPY_4I2pIZ"
      },
      "id": "yCPPY_4I2pIZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8697b51f-50a5-4091-9cc1-0aed1308b27d",
      "metadata": {
        "id": "8697b51f-50a5-4091-9cc1-0aed1308b27d"
      },
      "outputs": [],
      "source": [
        "# Import json data from Aipfy scraping\n",
        "df=pd.read_json(raw_data_file)\n",
        "\n",
        "# Print the DataFrame\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a4128d6-4115-47a6-9e78-5a517bded844",
      "metadata": {
        "id": "6a4128d6-4115-47a6-9e78-5a517bded844"
      },
      "outputs": [],
      "source": [
        "# select only two columns for analysis: url and text\n",
        "df=df[['url','text']]\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Join all pages from a domain to an entry in the analysis. To do this, add a new column which will contain only the main domain name."
      ],
      "metadata": {
        "id": "Rr6hiPQ-4z5O"
      },
      "id": "Rr6hiPQ-4z5O"
    },
    {
      "cell_type": "code",
      "source": [
        "# function to extract the main domain from the url in the dataset\n",
        "def extract_main_domain(url):\n",
        "    if not isinstance(str(url), str):\n",
        "        print('NOT VALID',url)\n",
        "        return None\n",
        "    match = re.findall('(?:\\w+\\.)*\\w+\\.\\w*', str(url)) #'www\\.?([^/]+)'\n",
        "    return match[0].lstrip('www.') if match else None"
      ],
      "metadata": {
        "id": "_Px9Aoim4-pq"
      },
      "id": "_Px9Aoim4-pq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47db9deb-8836-47fb-9f74-28a023bcb5d7",
      "metadata": {
        "id": "47db9deb-8836-47fb-9f74-28a023bcb5d7"
      },
      "outputs": [],
      "source": [
        "# Load the list of domains from a csv file:\n",
        "cc_column = cc+' domains'\n",
        "#print(cc_column)\n",
        "\n",
        "urls = pd.read_csv(gdrive_path+cc+'_urls.csv')[cc_column].values.tolist()\n",
        "\n",
        "# Extract main domains from nl_urls\n",
        "domains = {extract_main_domain(url) for url in urls if extract_main_domain(url) is not None}\n",
        "\n",
        "# Check if main domains in list_of_links match any domain in nl_domains\n",
        "matching_links = [link for link in df.url if extract_main_domain(link) in domains]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a2e8c0b-4c0e-4445-a5bb-2f1695ad353e",
      "metadata": {
        "id": "2a2e8c0b-4c0e-4445-a5bb-2f1695ad353e"
      },
      "outputs": [],
      "source": [
        "# this cell can be skipped, it is only for verification\n",
        "\n",
        "# check how many lines in the dataframe have a matching link to the list of urls\n",
        "print(len(matching_links))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc9d2331-fe01-4e94-b984-00ac834a771c",
      "metadata": {
        "id": "cc9d2331-fe01-4e94-b984-00ac834a771c"
      },
      "outputs": [],
      "source": [
        "# Add a new column 'domain' and fill it by applying the extract_main_domain function to the 'url' column\n",
        "df['domain'] = df['url'].apply(extract_main_domain)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Close reading one document\n",
        "(we will consider one website as a 'document')"
      ],
      "metadata": {
        "id": "tRv48s-JboPN"
      },
      "id": "tRv48s-JboPN"
    },
    {
      "cell_type": "code",
      "source": [
        "# list all unique domain names\n",
        "df['domain'].unique()"
      ],
      "metadata": {
        "id": "29umRTewbnDU"
      },
      "id": "29umRTewbnDU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract all rows (lines) where the value of 'domain' is 'lancashire.gov.uk'\n",
        "df[df['domain'] == 'lancashire.gov.uk']"
      ],
      "metadata": {
        "id": "a5mBBe-ncnfz"
      },
      "id": "a5mBBe-ncnfz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# then combine these into a list of pages\n",
        "document = df[df['domain'] == 'lancashire.gov.uk']['text'].tolist()"
      ],
      "metadata": {
        "id": "7SfU5XXfdN7l"
      },
      "id": "7SfU5XXfdN7l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document"
      ],
      "metadata": {
        "id": "ksw6zTgodjqL"
      },
      "id": "ksw6zTgodjqL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_document = []\n",
        "for i in document:\n",
        "  j = i.replace('\\n', ' ').replace('\\r', '')\n",
        "  clean_document.append(j)"
      ],
      "metadata": {
        "id": "_YEFxHo-ePV7"
      },
      "id": "_YEFxHo-ePV7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_document"
      ],
      "metadata": {
        "id": "kyc5LLlAeebS"
      },
      "id": "kyc5LLlAeebS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Working with text computationally"
      ],
      "metadata": {
        "id": "inCepASp67ru"
      },
      "id": "inCepASp67ru"
    },
    {
      "cell_type": "code",
      "source": [
        "# change 'UK' here for country code by hand\n",
        "!wget 'https://raw.githubusercontent.com/jazoza/cultural-data-analysis/refs/heads/main/stopwords_archive/UK.txt'"
      ],
      "metadata": {
        "id": "rKDwducX7kqD"
      },
      "id": "rKDwducX7kqD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load a list of 'stopwords' in the language you are analyzing\n",
        "def get_stopwords_list(stop_file_path):\n",
        "    \"\"\"load stop words \"\"\"\n",
        "    with open(stop_file_path, 'r', encoding=\"utf-8\") as f:\n",
        "        stopwords = f.readlines()\n",
        "        stop_set = set(m.strip() for m in stopwords)\n",
        "        return list(frozenset(stop_set))\n",
        "stopwords_path = cc+\".txt\"\n",
        "stopwords = get_stopwords_list(stopwords_path)"
      ],
      "metadata": {
        "id": "_bCxFJzY69_b"
      },
      "id": "_bCxFJzY69_b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "special_stop_words = ['nbsp', ' ', '', '—', '\\’s']\n",
        "stopwords_ext = stopwords+special_stop_words"
      ],
      "metadata": {
        "id": "HgNkZORH75QF"
      },
      "id": "HgNkZORH75QF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to remove non-ascii characters\n",
        "def _removeNonAscii(s): return \"\".join(i for i in s if ord(i)<128)"
      ],
      "metadata": {
        "id": "0SuPUQ_4VrfT"
      },
      "id": "0SuPUQ_4VrfT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "8lCObUihV8Yj"
      },
      "id": "8lCObUihV8Yj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to clean and lemmatize comments\n",
        "def clean_documents(text):\n",
        "    #remove punctuations\n",
        "    regex = re.compile('[' + re.escape(string.punctuation) + '\\\\r\\\\t\\\\n]')\n",
        "    nopunct = regex.sub(\" \", str(text))\n",
        "    #use spacy to lemmatize comments\n",
        "    doc = nlp(nopunct, disable=['parser','ner'])\n",
        "    lemma = [token.lemma_ for token in doc]\n",
        "    return lemma"
      ],
      "metadata": {
        "id": "yeR7duSYWD9p"
      },
      "id": "yeR7duSYWD9p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#apply function to clean and lemmatize comments\n",
        "lemmatized = df.text.map(clean_documents)\n",
        "#make sure to lowercase everything\n",
        "lemmatized = lemmatized.map(lambda x: [word.lower() for word in x])\n",
        "lemmatized.head()"
      ],
      "metadata": {
        "id": "7VsB2yq1WObL"
      },
      "id": "7VsB2yq1WObL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unlist_documents = [item for items in lemmatized for item in items]"
      ],
      "metadata": {
        "id": "IS3V0pl3Xksv"
      },
      "id": "IS3V0pl3Xksv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save these outputs for later\n",
        "with open(gdrive_path+'jar/lemmatized.pickle', 'wb') as handle_l:\n",
        "    pickle.dump(lemmatized, handle_l, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open(gdrive_path+'jar/unlist_documents.pickle', 'wb') as handle_u:\n",
        "    pickle.dump(unlist_documents, handle_u, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "VGoew_tHZLpQ"
      },
      "id": "VGoew_tHZLpQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load saved pickles\n",
        "with open(gdrive_path+'jar/lemmatized.pickle', 'rb') as handle_l:\n",
        "    lemmatized = pickle.load(handle_l)\n",
        "\n",
        "with open(gdrive_path+'jar/unlist_documents.pickle', 'rb') as handle_u:\n",
        "    unlist_documents = pickle.load(handle_u)"
      ],
      "metadata": {
        "id": "5aBr85g3ajFB"
      },
      "id": "5aBr85g3ajFB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "08144921-d7be-45ed-8795-1c085fb2640b",
      "metadata": {
        "id": "08144921-d7be-45ed-8795-1c085fb2640b"
      },
      "source": [
        "## 3. Collocations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "id": "Hbkku34SfpU1"
      },
      "id": "Hbkku34SfpU1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initiate bigrams and trigrams\n",
        "bigrams = nltk.collocations.BigramAssocMeasures()\n",
        "trigrams = nltk.collocations.TrigramAssocMeasures()"
      ],
      "metadata": {
        "id": "3IaXW0GXX4X2"
      },
      "id": "3IaXW0GXX4X2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# identify all collocations in the flat list of words from all documents\n",
        "bigramFinder = nltk.collocations.BigramCollocationFinder.from_words(unlist_documents)\n",
        "trigramFinder = nltk.collocations.TrigramCollocationFinder.from_words(unlist_documents)"
      ],
      "metadata": {
        "id": "6L2vI56MX9ps"
      },
      "id": "6L2vI56MX9ps",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate basic frequency"
      ],
      "metadata": {
        "id": "mRSuATcHYm0U"
      },
      "id": "mRSuATcHYm0U"
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_freq = bigramFinder.ngram_fd.items()"
      ],
      "metadata": {
        "id": "W9XCfughYim0"
      },
      "id": "W9XCfughYim0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigramFreqTable = pd.DataFrame(list(bigram_freq), columns=['bigram','freq']).sort_values(by='freq', ascending=False)"
      ],
      "metadata": {
        "id": "4dCLatTKfJDe"
      },
      "id": "4dCLatTKfJDe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigramFreqTable.head().reset_index(drop=True)"
      ],
      "metadata": {
        "id": "NHE_sQvofMKw"
      },
      "id": "NHE_sQvofMKw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute basic trigrams frequency\n",
        "trigram_freq = trigramFinder.ngram_fd.items()\n",
        "trigramFreqTable = pd.DataFrame(list(trigram_freq), columns=['trigram','freq']).sort_values(by='freq', ascending=False)\n",
        "trigramFreqTable[:10]"
      ],
      "metadata": {
        "id": "pYTAOfQGhfY3"
      },
      "id": "pYTAOfQGhfY3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find meaningful bi- and tri-grams by filtering adjectives and nouns based on an nltk functionality"
      ],
      "metadata": {
        "id": "W8JjofCAYNdC"
      },
      "id": "W8JjofCAYNdC"
    },
    {
      "cell_type": "code",
      "source": [
        "#function to filter for ADJ/NN bigrams\n",
        "def rightTypes(ngram):\n",
        "    for word in ngram:\n",
        "        if word in stopwords_ext:\n",
        "            return False\n",
        "    acceptable_types = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n",
        "    second_type = ('NN', 'NNS', 'NNP', 'NNPS')\n",
        "    tags = nltk.pos_tag(ngram)\n",
        "    if tags[0][1] in acceptable_types and tags[1][1] in second_type:\n",
        "        return True\n",
        "    else:\n",
        "        return False"
      ],
      "metadata": {
        "id": "OryUdGaYYMGs"
      },
      "id": "OryUdGaYYMGs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#filter bigrams\n",
        "filtered_bi = bigramFreqTable[bigramFreqTable.bigram.map(lambda x: rightTypes(x))]"
      ],
      "metadata": {
        "id": "a6HFZOAafcqP"
      },
      "id": "a6HFZOAafcqP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_bi[:10]"
      ],
      "metadata": {
        "id": "DoEO44Tzf-i8"
      },
      "id": "DoEO44Tzf-i8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use advanced statistical methods like the Chi-Square to identify meaninful collocations\n",
        "https://en.wikipedia.org/wiki/Chi-squared_test"
      ],
      "metadata": {
        "id": "LGPGqH0Xj98T"
      },
      "id": "LGPGqH0Xj98T"
    },
    {
      "cell_type": "code",
      "source": [
        "# filter bigrams using chi-square\n",
        "bigramChiTable = pd.DataFrame(list(bigramFinder.score_ngrams(bigrams.chi_sq)), columns=['bigram','chi-sq']).sort_values(by='chi-sq', ascending=False)\n",
        "bigramChiTable.head()"
      ],
      "metadata": {
        "id": "j8IPPymTjaql"
      },
      "id": "j8IPPymTjaql",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find meaningful trigrams by filtering basic frequency table\n",
        "# function to filter trigrams\n",
        "def rightTypesTri(ngram):\n",
        "    if '-pron-' in ngram or '' in ngram or ' 'in ngram or '  ' in ngram or 't' in ngram:\n",
        "        return False\n",
        "    for word in ngram:\n",
        "        if word in stopwords_ext:\n",
        "            return False\n",
        "    first_type = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n",
        "    third_type = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n",
        "    tags = nltk.pos_tag(ngram)\n",
        "    if tags[0][1] in first_type and tags[2][1] in third_type:\n",
        "        return True\n",
        "    else:\n",
        "        return False"
      ],
      "metadata": {
        "id": "W5wqPcjzh_lW"
      },
      "id": "W5wqPcjzh_lW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_tri = trigramFreqTable[trigramFreqTable.trigram.map(lambda x: rightTypesTri(x))]\n",
        "filtered_tri[:10]"
      ],
      "metadata": {
        "id": "3vQu3yv2jGRT"
      },
      "id": "3vQu3yv2jGRT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chi-sqare frequency calculation for trigrams\n",
        "trigramChiTable = pd.DataFrame(list(trigramFinder.score_ngrams(trigrams.chi_sq)), columns=['trigram','chi-sq']).sort_values(by='chi-sq', ascending=False)\n",
        "trigramChiTable.head(20)"
      ],
      "metadata": {
        "id": "OIr3r7XBkaqF"
      },
      "id": "OIr3r7XBkaqF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Analyse specific collocations"
      ],
      "metadata": {
        "id": "2nFPgMYkiygf"
      },
      "id": "2nFPgMYkiygf"
    },
    {
      "cell_type": "code",
      "source": [
        "# search for words from this list or use another list\n",
        "search_words = ['royal']"
      ],
      "metadata": {
        "id": "LoJnS9K7h5Kd"
      },
      "id": "LoJnS9K7h5Kd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SCI-KIT method, produces lists of co-occurencies for specific terms\n",
        "def vectorize_text(df):\n",
        "    vectorizer = CountVectorizer()\n",
        "    X = vectorizer.fit_transform(df['text'])\n",
        "    return X, vectorizer\n",
        "\n",
        "def find_collocations(text, target_words):\n",
        "    words = text.split()\n",
        "    collocations = []\n",
        "    for i in range(len(words) - 1):\n",
        "        if words[i] in target_words:\n",
        "            collocations.append((words[i], words[i + 1]))\n",
        "        if words[i + 1] in target_words:\n",
        "            collocations.append((words[i + 1], words[i]))\n",
        "    return collocations\n",
        "\n",
        "def get_frequent_collocations(df, most_frequent_words):\n",
        "    collocations = []\n",
        "    for text in df['text']:\n",
        "        collocations.extend(find_collocations(text, most_frequent_words))\n",
        "    collocation_counts = Counter(collocations)\n",
        "    frequent_collocations = {}\n",
        "    for word in most_frequent_words:\n",
        "        word_collocations = {collocation: count for collocation, count in collocation_counts.items() if word in collocation}\n",
        "        frequent_collocations[word] = dict(islice(Counter(word_collocations).most_common(20), 20))\n",
        "    return frequent_collocations\n",
        "\n",
        "def analyze_word_collocations(df):\n",
        "    X, vectorizer = vectorize_text(df)\n",
        "    most_frequent_words = search_words\n",
        "    frequent_collocations = get_frequent_collocations(df, most_frequent_words)\n",
        "    return frequent_collocations"
      ],
      "metadata": {
        "id": "032f9482-879c-43f9-a9cd-3b1ca5d7056c"
      },
      "id": "032f9482-879c-43f9-a9cd-3b1ca5d7056c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2657cbe3-aa7e-4e4b-8d68-1d6050cf6f49",
      "metadata": {
        "id": "2657cbe3-aa7e-4e4b-8d68-1d6050cf6f49"
      },
      "outputs": [],
      "source": [
        "collocations = analyze_word_collocations(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fbef5d2-983b-49fa-8924-95abd24b2855",
      "metadata": {
        "id": "4fbef5d2-983b-49fa-8924-95abd24b2855"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "for word, colloc_dict in collocations.items():\n",
        "   for collocation, count in colloc_dict.items():\n",
        "       #collocation_str = ' '.join(collocation)  # Join collocation words into a single string\n",
        "       data.append([word, collocation[1], count])\n",
        "collocations_df = pd.DataFrame(data, columns=['Word', 'Collocation', 'Count'])\n",
        "print(collocations_df.to_markdown(index=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word2Vec model"
      ],
      "metadata": {
        "id": "8nURhyHN8ymX"
      },
      "id": "8nURhyHN8ymX"
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "4YrTkLDz9hjC"
      },
      "id": "4YrTkLDz9hjC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bca25ef7-5a19-41e3-a199-bdde7304e575",
      "metadata": {
        "id": "bca25ef7-5a19-41e3-a199-bdde7304e575"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# X is a list of tokenized texts (i.e. list of lists of tokens)\n",
        "X = [word_tokenize(item) for item in df.text.tolist()]\n",
        "#print(X[0:3])\n",
        "model = gensim.models.Word2Vec(X, min_count=6, vector_size=200) # min_count: how many times a word appears in the corpus; size: number of dimensions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar(positive=[\"royal\"], topn=12)"
      ],
      "metadata": {
        "id": "QS_aLSZc87XP"
      },
      "id": "QS_aLSZc87XP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic Modeling"
      ],
      "metadata": {
        "id": "xU6301CNmIAC"
      },
      "id": "xU6301CNmIAC"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ornUSbnUl9fN"
      },
      "id": "ornUSbnUl9fN",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}