{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jazoza/cultural-data-analysis/blob/main/03_CDA_HH_dataset_representations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cultural Data Analysis\n",
        "\n",
        "Introduction to working with datasets"
      ],
      "metadata": {
        "id": "7CC34uuNzNxY"
      },
      "id": "7CC34uuNzNxY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76a4e150-cc6f-4878-a32b-e78a1d6426ae",
      "metadata": {
        "id": "76a4e150-cc6f-4878-a32b-e78a1d6426ae"
      },
      "outputs": [],
      "source": [
        "# import necessary libraries\n",
        "import os, re, csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from collections import Counter\n",
        "from itertools import islice"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7010dda9-acc5-4d90-b175-90012564d13c",
      "metadata": {
        "id": "7010dda9-acc5-4d90-b175-90012564d13c"
      },
      "source": [
        "## Loading the datasets: heritage homes webistes\n",
        "\n",
        "The dataset is stored in a shared google drive:\n",
        "https://drive.google.com/drive/folders/11Shm0edDOiWrOe56fzJQRZi-v_BPSW8E?usp=drive_link\n",
        "\n",
        "Add it to your drive.\n",
        "\n",
        "To access it, load your gdrive in 'Files' (see left pane of the notebook in google colab) and navigate to the shared folder. You may need to click on 'refresh' to make it appear on the list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d42429d3-63fe-4b79-b341-160057e5dcbc",
      "metadata": {
        "id": "d42429d3-63fe-4b79-b341-160057e5dcbc"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A. Working witn one country"
      ],
      "metadata": {
        "id": "K2_05SuKPd2y"
      },
      "id": "K2_05SuKPd2y"
    },
    {
      "cell_type": "code",
      "source": [
        "# Country code: change here between 'NL' and 'UK'\n",
        "cc = 'NL'"
      ],
      "metadata": {
        "id": "QYiHwjcORrPC"
      },
      "execution_count": null,
      "outputs": [],
      "id": "QYiHwjcORrPC"
    },
    {
      "cell_type": "code",
      "source": [
        "gdrive_path = '/content/gdrive/MyDrive/CDA/'"
      ],
      "metadata": {
        "id": "bbjhZ8nKZtZC"
      },
      "execution_count": null,
      "outputs": [],
      "id": "bbjhZ8nKZtZC"
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data_file = gdrive_path+cc+'_dataset_website-content-crawler.json'"
      ],
      "metadata": {
        "id": "XltmToHfTDzn"
      },
      "execution_count": null,
      "outputs": [],
      "id": "XltmToHfTDzn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8697b51f-50a5-4091-9cc1-0aed1308b27d",
      "metadata": {
        "id": "8697b51f-50a5-4091-9cc1-0aed1308b27d"
      },
      "outputs": [],
      "source": [
        "# Import json data from Aipfy scraping\n",
        "df=pd.read_json(raw_data_file)\n",
        "\n",
        "# Print the DataFrame\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check if there are further datasets to add per country\n",
        "\n",
        "!ls \"$gdrive_path\" | grep \"$cc\""
      ],
      "metadata": {
        "id": "xBveTQoTgJbS"
      },
      "id": "xBveTQoTgJbS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_missing1 = pd.read_json(gdrive_path+'/NL_LG_dataset_website-content-crawler_2025-02-06_09-40-33-880.json')\n",
        "result = pd.concat([df, df_missing1])\n",
        "df = result\n",
        "df.head()"
      ],
      "metadata": {
        "id": "OG8VKhFzgVPq"
      },
      "id": "OG8VKhFzgVPq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a4128d6-4115-47a6-9e78-5a517bded844",
      "metadata": {
        "id": "6a4128d6-4115-47a6-9e78-5a517bded844"
      },
      "outputs": [],
      "source": [
        "# select only two columns for analysis: url and text\n",
        "df=df[['url','text']]\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Join all pages from a domain to an entry in the analysis. To do this, add a new column which will contain only the main domain name."
      ],
      "metadata": {
        "id": "Rr6hiPQ-4z5O"
      },
      "id": "Rr6hiPQ-4z5O"
    },
    {
      "cell_type": "code",
      "source": [
        "# function to extract the main domain from the url in the dataset\n",
        "def extract_main_domain(url):\n",
        "    if not isinstance(str(url), str):\n",
        "        print('NOT VALID',url)\n",
        "        return None\n",
        "    match = re.findall('(?:\\\\w+\\\\.)*\\\\w+\\\\.\\\\w*', str(url)) #'www\\.?([^/]+)'\n",
        "    return match[0].lstrip('www.') if match else None"
      ],
      "metadata": {
        "id": "_Px9Aoim4-pq"
      },
      "id": "_Px9Aoim4-pq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47db9deb-8836-47fb-9f74-28a023bcb5d7",
      "metadata": {
        "id": "47db9deb-8836-47fb-9f74-28a023bcb5d7"
      },
      "outputs": [],
      "source": [
        "# Load the list of domains from a csv file:\n",
        "cc_column = cc+' domains'\n",
        "#print(cc_column)\n",
        "\n",
        "urls = pd.read_csv(gdrive_path+'url_lists/'+cc+'_urls.csv')[cc_column].values.tolist()\n",
        "\n",
        "# Extract main domains from nl_urls\n",
        "domains = {extract_main_domain(url) for url in urls if extract_main_domain(url) is not None}\n",
        "\n",
        "# Check if main domains in list_of_links match any domain in nl_domains\n",
        "matching_links = [link for link in df.url if extract_main_domain(link) in domains]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a2e8c0b-4c0e-4445-a5bb-2f1695ad353e",
      "metadata": {
        "id": "2a2e8c0b-4c0e-4445-a5bb-2f1695ad353e"
      },
      "outputs": [],
      "source": [
        "print(len(matching_links))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc9d2331-fe01-4e94-b984-00ac834a771c",
      "metadata": {
        "id": "cc9d2331-fe01-4e94-b984-00ac834a771c"
      },
      "outputs": [],
      "source": [
        "# Add a new column 'domain' and fill it by applying the extract_main_domain function to the 'url' column\n",
        "df['domain'] = df['url'].apply(extract_main_domain)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### B. Working with all countries"
      ],
      "metadata": {
        "id": "ukRXJgSvn-yx"
      },
      "id": "ukRXJgSvn-yx"
    },
    {
      "cell_type": "code",
      "source": [
        "# Country code: change here between 'NL' and 'UK'\n",
        "cc_list = ['NL', 'UK', 'DE', 'FR']"
      ],
      "metadata": {
        "id": "Qu6K9f2Vo6i_"
      },
      "id": "Qu6K9f2Vo6i_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gdrive_path = '/content/gdrive/MyDrive/CDA/'"
      ],
      "metadata": {
        "id": "9fnHB7XSo89i"
      },
      "id": "9fnHB7XSo89i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to extract the main domain from the url in the dataset\n",
        "def extract_main_domain(url):\n",
        "    if not isinstance(str(url), str):\n",
        "        print('NOT VALID',url)\n",
        "        return None\n",
        "    match = re.findall('(?:\\\\w+\\\\.)*\\\\w+\\\\.\\\\w*', str(url)) #'www\\.?([^/]+)'\n",
        "    return match[0].lstrip('www.') if match else None"
      ],
      "metadata": {
        "id": "sczGS62Oo8z6"
      },
      "id": "sczGS62Oo8z6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import scraped json data into 4 separate dataframes\n",
        "df0=pd.read_json(gdrive_path+cc_list[0]+'_dataset_website-content-crawler.json')\n",
        "# select only two columns for analysis: url and text\n",
        "df0=df0[['url','text']]\n",
        "\n",
        "df1=pd.read_json(gdrive_path+cc_list[1]+'_dataset_website-content-crawler.json')\n",
        "# select only two columns for analysis: url and text\n",
        "df1=df1[['url','text']]\n",
        "\n",
        "df2=pd.read_json(gdrive_path+cc_list[2]+'_dataset_website-content-crawler.json')\n",
        "# select only two columns for analysis: url and text\n",
        "df2=df2[['url','text']]\n",
        "\n",
        "df3=pd.read_json(gdrive_path+cc_list[3]+'_dataset_website-content-crawler.json')\n",
        "# select only two columns for analysis: url and text\n",
        "df3=df3[['url','text']]\n",
        "\n",
        "df0.head()\n"
      ],
      "metadata": {
        "id": "8K9APfwUo4Up"
      },
      "id": "8K9APfwUo4Up",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Join all pages from a domain to an entry in the analysis. To do this, add a new column which will contain only the main domain name."
      ],
      "metadata": {
        "id": "7yB3vYA7pGpq"
      },
      "id": "7yB3vYA7pGpq"
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a new column 'domain' and fill it by applying the extract_main_domain function to the 'url' column\n",
        "\n",
        "# first, create a mapping of dataframes which could be addressed in a loop\n",
        "df_dict = {'0':df0, '1':df1, '2':df2, '3':df3}\n",
        "\n",
        "# then, loop through the df_dict to update each dataframe\n",
        "for k, v in df_dict.items():\n",
        "  cc_column = cc_list[int(k[-1])]+' domains'\n",
        "  cc = cc_list[int(k[-1])]\n",
        "  # print(cc_column, cc)\n",
        "  urls = pd.read_csv(gdrive_path+'url_lists/'+cc_list[int(k[-1])]+'_urls.csv')[cc_column].values.tolist()\n",
        "  domains = {extract_main_domain(url) for url in urls if extract_main_domain(url) is not None}\n",
        "  matching_links = [link for link in v.url if extract_main_domain(link) in domains]\n",
        "  # update the dataframe\n",
        "  v['domain'] = v['url'].apply(extract_main_domain)\n",
        "\n",
        "# check one of the dataframes\n",
        "df1.head()"
      ],
      "metadata": {
        "id": "dFCVEXuJpF_U"
      },
      "id": "dFCVEXuJpF_U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare the analysis"
      ],
      "metadata": {
        "id": "inCepASp67ru"
      },
      "id": "inCepASp67ru"
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the stopwords list for one language (defined in cc)\n",
        "# Redefine cc as necessary\n",
        "cc = 'NL'\n",
        "# Define the path to the file in github\n",
        "stopwords_cc = \"https://raw.githubusercontent.com/jazoza/cultural-data-analysis/refs/heads/main/stopwords_archive/\"+cc+\".txt\"\n",
        "# Download the file, it should appear under 'sample_data' folder\n",
        "!wget \"$stopwords_cc\""
      ],
      "metadata": {
        "id": "rKDwducX7kqD"
      },
      "id": "rKDwducX7kqD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load a list of 'stopwords' in the language you are analyzing\n",
        "def get_stopwords_list(stop_file_path):\n",
        "    \"\"\"load stop words \"\"\"\n",
        "    with open(stop_file_path, 'r', encoding=\"utf-8\") as f:\n",
        "        stopwords = f.readlines()\n",
        "        stop_set = set(m.strip() for m in stopwords)\n",
        "        return list(frozenset(stop_set))\n",
        "stopwords_path = cc+\".txt\"\n",
        "stopwords = get_stopwords_list(stopwords_path)"
      ],
      "metadata": {
        "id": "_bCxFJzY69_b"
      },
      "id": "_bCxFJzY69_b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# you may need to include additional words which you notice as too frequent\n",
        "special_stop_words = ['nbsp', 'nl', 'the', 'and', 'und', 'we', 'to']\n",
        "stopwords_ext = stopwords+special_stop_words"
      ],
      "metadata": {
        "id": "HgNkZORH75QF"
      },
      "id": "HgNkZORH75QF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b5238d41-a6b8-48db-84d9-f6d3bb1054ad",
      "metadata": {
        "id": "b5238d41-a6b8-48db-84d9-f6d3bb1054ad"
      },
      "source": [
        "## 1. Term frequency\n",
        "\n",
        "The cells below will compute a term-matrix and calculate the frequency of each unique word (token) in the corpus\n",
        "\n",
        "This can be done for ALL words in the corpus, or ALL MEANINGFUL words (without so-called stop-words like 'the' or 'het')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b7d7865-db1f-42bc-97a0-f901e371a5e5",
      "metadata": {
        "id": "3b7d7865-db1f-42bc-97a0-f901e371a5e5"
      },
      "outputs": [],
      "source": [
        "# CALCULATE TERM FREQUENCY OF ALL TERMS\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# convert the text documents into a matrix of token (word) counts\n",
        "cvec_all = CountVectorizer().fit(df.text)\n",
        "df_matrix_all = cvec_all.transform(df.text)\n",
        "df_all = np.sum(df_matrix_all,axis=0)\n",
        "terms = np.squeeze(np.asarray(df_all))\n",
        "# print the 'shape' of the matrix - it should indicate the number of unique terms\n",
        "print(terms.shape)\n",
        "term_freq_df_all = pd.DataFrame([terms],columns=cvec_all.get_feature_names_out()).transpose() #term_freq_df is with stopwords\n",
        "term_freq_df_all.columns = ['terms']\n",
        "# show the first ten words [:10];\n",
        "# change the values in the brackets to show 30th-40th words [30:40]\n",
        "# or show the last ten words [:-10]\n",
        "term_freq_df_all.sort_values(by='terms', ascending=False).iloc[:-10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e317c18-cf12-48e6-a4db-57b3a99fde06",
      "metadata": {
        "id": "4e317c18-cf12-48e6-a4db-57b3a99fde06"
      },
      "outputs": [],
      "source": [
        "# CALCULATE TERM FREQUENCY WITHOUT STOP-WORDS\n",
        "\n",
        "cvec_stopped = CountVectorizer(stop_words=stopwords_ext, token_pattern=r'(?u)\\b[A-Za-z]{2,}\\b') # see above, import frozenset from stopwords_archive in correct language) # see above, import frozenset from stopwords_archive in correct language\n",
        "cvec_stopped.fit(df.text)\n",
        "document_matrix = cvec_stopped.transform(df.text)\n",
        "term_batches = np.linspace(0,document_matrix.shape[0],10).astype(int)\n",
        "i=0\n",
        "df_stopped = []\n",
        "while i < len(term_batches)-1:\n",
        "    batch_result = np.sum(document_matrix[term_batches[i]:term_batches[i+1]].toarray(),axis=0)\n",
        "    df_stopped.append(batch_result)\n",
        "    print(term_batches[i+1],\"entries' term frequency calculated\")\n",
        "    i += 1\n",
        "\n",
        "terms_stopped = np.sum(df_stopped,axis=0)\n",
        "print(terms_stopped.shape)\n",
        "term_freq_df_stopped = pd.DataFrame([terms_stopped],columns=cvec_stopped.get_feature_names_out()).transpose()\n",
        "term_freq_df_stopped.columns = ['terms']\n",
        "term_freq_df_stopped.sort_values(by='terms', ascending=False).iloc[:10]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09be655e-553d-4f2f-8993-9a4fbe63c0aa",
      "metadata": {
        "id": "09be655e-553d-4f2f-8993-9a4fbe63c0aa"
      },
      "source": [
        "### 1.1 TF-IDF vectorization\n",
        "\n",
        "- What is TF/IDF (term frequency / inverse document frequency)? https://en.wikipedia.org/wiki/Tf%E2%80%93idf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "nHaX1Ipo99za"
      },
      "id": "nHaX1Ipo99za",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "002e51e5-ccc0-41c7-9e8c-35d2c794368b",
      "metadata": {
        "id": "002e51e5-ccc0-41c7-9e8c-35d2c794368b"
      },
      "outputs": [],
      "source": [
        "# Initialize the TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(stop_words=stopwords_ext)\n",
        "# Fit and transform the text data\n",
        "tfidf_matrix = vectorizer.fit_transform(df['text'])\n",
        "# Convert the TF-IDF matrix to a DataFrame\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "# Add filenames as index\n",
        "tfidf_df.index = df['domain']\n",
        "# Print the TF-IDF DataFrame\n",
        "tfidf_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58b69f61-d6b4-4413-9c7a-f916939197ec",
      "metadata": {
        "id": "58b69f61-d6b4-4413-9c7a-f916939197ec"
      },
      "outputs": [],
      "source": [
        "# Add column for document frequency aka number of times word appears in all documents\n",
        "tfidf_df.loc['ALL'] = (tfidf_df > 0).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e520f80-a77d-4e4f-af27-53df6eddbd91",
      "metadata": {
        "id": "4e520f80-a77d-4e4f-af27-53df6eddbd91"
      },
      "outputs": [],
      "source": [
        "tfidf_df.head() # first five rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d00361c1-d6b5-46e3-8b66-87423a0dc2b6",
      "metadata": {
        "id": "d00361c1-d6b5-46e3-8b66-87423a0dc2b6"
      },
      "outputs": [],
      "source": [
        "# 10 most frequent words!\n",
        "\n",
        "tfidf_slice = tfidf_df[term_freq_df_stopped.sort_values(by='terms', ascending=False).iloc[:10].index.tolist()]\n",
        "tfidf_slice.sort_index().round(decimals=2).head() # first five rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54b73181-9d99-4d60-a272-b4c1bb07ae8c",
      "metadata": {
        "id": "54b73181-9d99-4d60-a272-b4c1bb07ae8c"
      },
      "outputs": [],
      "source": [
        "# reorganize the DataFrame so that the words are in rows rather than columns\n",
        "tfidf_df = tfidf_df.drop('ALL', errors='ignore')\n",
        "tfidf_df = tfidf_df.stack().reset_index()\n",
        "tfidf_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ed80520-b12a-415e-b1bd-3edef4cd23b4",
      "metadata": {
        "id": "0ed80520-b12a-415e-b1bd-3edef4cd23b4"
      },
      "outputs": [],
      "source": [
        "tfidf_df = tfidf_df.rename(columns={0:'tfidf', 'domain': 'document','level_1': 'term'})\n",
        "tfidf_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea792b30-cfee-431a-bdf4-743edded27d3",
      "metadata": {
        "id": "ea792b30-cfee-431a-bdf4-743edded27d3"
      },
      "outputs": [],
      "source": [
        "tfidf_df.sort_values(by=['document','tfidf'], ascending=[True,False]).groupby(['document']).head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a35a2381-c49e-4218-8e0b-d298d24db2da",
      "metadata": {
        "id": "a35a2381-c49e-4218-8e0b-d298d24db2da"
      },
      "outputs": [],
      "source": [
        "top_tfidf = tfidf_df.sort_values(by=['document','tfidf'], ascending=[True,False]).groupby(['document']).head(10)\n",
        "\n",
        "# Terms in this list will get a red dot in the visualization\n",
        "term_list = ['kasteel', 'huis']\n",
        "\n",
        "# adding a little randomness to break ties in term ranking\n",
        "top_tfidf_plusRand = top_tfidf.copy()\n",
        "top_tfidf_plusRand['tfidf'] = top_tfidf_plusRand['tfidf'] + np.random.rand(top_tfidf.shape[0])*0.0001\n",
        "\n",
        "# base for all visualizations, with rank calculation\n",
        "base = alt.Chart(top_tfidf_plusRand).encode(\n",
        "    x = 'rank:O',\n",
        "    y = 'document:N'\n",
        ").transform_window(\n",
        "    rank = \"rank()\",\n",
        "    sort = [alt.SortField(\"tfidf\", order=\"descending\")],\n",
        "    groupby = [\"document\"],\n",
        ")\n",
        "\n",
        "# heatmap specification\n",
        "heatmap = base.mark_rect().encode(\n",
        "    color = 'tfidf:Q'\n",
        ")\n",
        "\n",
        "# red circle over terms in above list\n",
        "circle = base.mark_circle(size=100).encode(\n",
        "    color = alt.condition(\n",
        "        alt.FieldOneOfPredicate(field='term', oneOf=term_list),\n",
        "        alt.value('red'),\n",
        "        alt.value('#FFFFFF00')\n",
        "    )\n",
        ")\n",
        "\n",
        "# text labels, white for darker heatmap colors\n",
        "text = base.mark_text(baseline='middle').encode(\n",
        "    text = 'term:N',\n",
        "    color = alt.condition(alt.datum.tfidf >= 0.23, alt.value('white'), alt.value('black'))\n",
        ")\n",
        "\n",
        "# display the three superimposed visualizations\n",
        "(heatmap + circle + text).properties(width = 600)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Collocations"
      ],
      "metadata": {
        "id": "2nFPgMYkiygf"
      },
      "id": "2nFPgMYkiygf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Analyze specific collocations"
      ],
      "metadata": {
        "id": "COojp_VZphQW"
      },
      "id": "COojp_VZphQW"
    },
    {
      "cell_type": "code",
      "source": [
        "# search for words from this list or use another list\n",
        "search_words = ['royal']"
      ],
      "metadata": {
        "id": "LoJnS9K7h5Kd"
      },
      "execution_count": null,
      "outputs": [],
      "id": "LoJnS9K7h5Kd"
    },
    {
      "cell_type": "code",
      "source": [
        "# SCI-KIT method, produces lists of co-occurencies for specific terms\n",
        "def vectorize_text(df):\n",
        "    vectorizer = text.CountVectorizer()\n",
        "    X = vectorizer.fit_transform(df['text'])\n",
        "    return X, vectorizer\n",
        "\n",
        "def find_collocations(text, target_words):\n",
        "    words = text.split()\n",
        "    collocations = []\n",
        "    for i in range(len(words) - 1):\n",
        "        if words[i] in target_words:\n",
        "            collocations.append((words[i], words[i + 1]))\n",
        "        if words[i + 1] in target_words:\n",
        "            collocations.append((words[i + 1], words[i]))\n",
        "    return collocations\n",
        "\n",
        "def get_frequent_collocations(df, most_frequent_words):\n",
        "    collocations = []\n",
        "    for text in df['text']:\n",
        "        collocations.extend(find_collocations(text, most_frequent_words))\n",
        "    collocation_counts = Counter(collocations)\n",
        "    frequent_collocations = {}\n",
        "    for word in most_frequent_words:\n",
        "        word_collocations = {collocation: count for collocation, count in collocation_counts.items() if word in collocation}\n",
        "        frequent_collocations[word] = dict(islice(Counter(word_collocations).most_common(20), 20))\n",
        "    return frequent_collocations\n",
        "\n",
        "def analyze_word_collocations(df):\n",
        "    X, vectorizer = vectorize_text(df)\n",
        "    most_frequent_words = search_words\n",
        "    frequent_collocations = get_frequent_collocations(df, most_frequent_words)\n",
        "    return frequent_collocations"
      ],
      "metadata": {
        "id": "032f9482-879c-43f9-a9cd-3b1ca5d7056c"
      },
      "execution_count": null,
      "outputs": [],
      "id": "032f9482-879c-43f9-a9cd-3b1ca5d7056c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2657cbe3-aa7e-4e4b-8d68-1d6050cf6f49"
      },
      "outputs": [],
      "source": [
        "collocations = analyze_word_collocations(df)"
      ],
      "id": "2657cbe3-aa7e-4e4b-8d68-1d6050cf6f49"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fbef5d2-983b-49fa-8924-95abd24b2855"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "for word, colloc_dict in collocations.items():\n",
        "   for collocation, count in colloc_dict.items():\n",
        "       #collocation_str = ' '.join(collocation)  # Join collocation words into a single string\n",
        "       data.append([word, collocation[1], count])\n",
        "collocations_df = pd.DataFrame(data, columns=['Word', 'Collocation', 'Count'])\n",
        "print(collocations_df.to_markdown(index=True))"
      ],
      "id": "4fbef5d2-983b-49fa-8924-95abd24b2855"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08144921-d7be-45ed-8795-1c085fb2640b"
      },
      "source": [
        "### 3. Analyse collocations in sentences"
      ],
      "id": "08144921-d7be-45ed-8795-1c085fb2640b"
    },
    {
      "cell_type": "code",
      "source": [
        "#function to remove non-ascii characters\n",
        "def _removeNonAscii(s): return \"\".join(i for i in s if ord(i)<128)"
      ],
      "metadata": {
        "id": "0SuPUQ_4VrfT"
      },
      "execution_count": null,
      "outputs": [],
      "id": "0SuPUQ_4VrfT"
    },
    {
      "cell_type": "code",
      "source": [
        "# import the advanced Natural Language Processing (NLP) library\n",
        "# which we will use to analze the grammatical structure\n",
        "import spacy"
      ],
      "metadata": {
        "id": "ijzGDgoRAHN3"
      },
      "id": "ijzGDgoRAHN3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download the suitable language pipeline\n",
        "# Dutch: nl_core_news_sm\n",
        "# French: nl_core_news_sm\n",
        "# German: nl_core_news_sm\n",
        "# English is available by default\n",
        "!python -m spacy download nl_core_news_sm"
      ],
      "metadata": {
        "id": "55G_5i9IAs1b"
      },
      "id": "55G_5i9IAs1b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('nl_core_news_sm')"
      ],
      "metadata": {
        "id": "pqCDLM2KAce0"
      },
      "id": "pqCDLM2KAce0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "#function to clean and lemmatize comments\n",
        "def clean_documents(text):\n",
        "    #remove punctuations\n",
        "    regex = re.compile('[' + re.escape(string.punctuation) + '\\\\r\\\\t\\\\n]')\n",
        "    nopunct = regex.sub(\" \", str(text))\n",
        "    #use spacy to lemmatize comments\n",
        "    doc = nlp(nopunct, disable=['parser','ner'])\n",
        "    lemma = [token.lemma_ for token in doc]\n",
        "    return lemma"
      ],
      "metadata": {
        "id": "yeR7duSYWD9p"
      },
      "execution_count": null,
      "outputs": [],
      "id": "yeR7duSYWD9p"
    },
    {
      "cell_type": "code",
      "source": [
        "#apply function to clean and lemmatize comments\n",
        "lemmatized = df.text.map(clean_documents)\n",
        "#make sure to lowercase everything\n",
        "lemmatized = lemmatized.map(lambda x: [word.lower() for word in x])\n",
        "lemmatized.head()"
      ],
      "metadata": {
        "id": "7VsB2yq1WObL"
      },
      "execution_count": null,
      "outputs": [],
      "id": "7VsB2yq1WObL"
    },
    {
      "cell_type": "code",
      "source": [
        "unlist_documents = [item for items in lemmatized for item in items]"
      ],
      "metadata": {
        "id": "IS3V0pl3Xksv"
      },
      "execution_count": null,
      "outputs": [],
      "id": "IS3V0pl3Xksv"
    },
    {
      "cell_type": "code",
      "source": [
        "# You would use these commands to save lemmatized text into a 'pickle' for later use\n",
        "# The current setup does not enable you to overwrite existing files in the CDA/jar folder,\n",
        "# so you would have to save the 'pickle' files elsewhere (for example in sample_data folder)\n",
        "# If you want to reactivate this code, remove the tripple quotes ''' from the beginning and end\n",
        "'''\n",
        "# save these outputs for later\n",
        "with open(gdrive_path+'jar/lemmatized.pickle', 'wb') as handle_l:\n",
        "    pickle.dump(lemmatized, handle_l, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open(gdrive_path+'jar/unlist_documents.pickle', 'wb') as handle_u:\n",
        "    pickle.dump(unlist_documents, handle_u, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "  '''"
      ],
      "metadata": {
        "id": "VGoew_tHZLpQ"
      },
      "execution_count": null,
      "outputs": [],
      "id": "VGoew_tHZLpQ"
    },
    {
      "cell_type": "code",
      "source": [
        "# load saved pickles\n",
        "with open(gdrive_path+'jar/'+cc+'_lemmatized.pickle', 'rb') as handle_l:\n",
        "    lemmatized = pickle.load(handle_l)\n",
        "\n",
        "with open(gdrive_path+'jar/'+cc+'_unlist_documents.pickle', 'rb') as handle_u:\n",
        "    unlist_documents = pickle.load(handle_u)"
      ],
      "metadata": {
        "id": "5aBr85g3ajFB"
      },
      "execution_count": null,
      "outputs": [],
      "id": "5aBr85g3ajFB"
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "id": "Hbkku34SfpU1"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Hbkku34SfpU1"
    },
    {
      "cell_type": "code",
      "source": [
        "# initiate bigrams and trigrams\n",
        "bigrams = nltk.collocations.BigramAssocMeasures()\n",
        "trigrams = nltk.collocations.TrigramAssocMeasures()"
      ],
      "metadata": {
        "id": "3IaXW0GXX4X2"
      },
      "execution_count": null,
      "outputs": [],
      "id": "3IaXW0GXX4X2"
    },
    {
      "cell_type": "code",
      "source": [
        "# identify all collocations in the flat list of words from all documents\n",
        "bigramFinder = nltk.collocations.BigramCollocationFinder.from_words(unlist_documents)\n",
        "trigramFinder = nltk.collocations.TrigramCollocationFinder.from_words(unlist_documents)"
      ],
      "metadata": {
        "id": "6L2vI56MX9ps"
      },
      "execution_count": null,
      "outputs": [],
      "id": "6L2vI56MX9ps"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate basic frequency"
      ],
      "metadata": {
        "id": "mRSuATcHYm0U"
      },
      "id": "mRSuATcHYm0U"
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_freq = bigramFinder.ngram_fd.items()"
      ],
      "metadata": {
        "id": "W9XCfughYim0"
      },
      "execution_count": null,
      "outputs": [],
      "id": "W9XCfughYim0"
    },
    {
      "cell_type": "code",
      "source": [
        "bigramFreqTable = pd.DataFrame(list(bigram_freq), columns=['bigram','freq']).sort_values(by='freq', ascending=False)"
      ],
      "metadata": {
        "id": "4dCLatTKfJDe"
      },
      "execution_count": null,
      "outputs": [],
      "id": "4dCLatTKfJDe"
    },
    {
      "cell_type": "code",
      "source": [
        "bigramFreqTable.head().reset_index(drop=True)"
      ],
      "metadata": {
        "id": "NHE_sQvofMKw"
      },
      "execution_count": null,
      "outputs": [],
      "id": "NHE_sQvofMKw"
    },
    {
      "cell_type": "code",
      "source": [
        "# compute basic trigrams frequency\n",
        "trigram_freq = trigramFinder.ngram_fd.items()\n",
        "trigramFreqTable = pd.DataFrame(list(trigram_freq), columns=['trigram','freq']).sort_values(by='freq', ascending=False)\n",
        "trigramFreqTable[:10]"
      ],
      "metadata": {
        "id": "pYTAOfQGhfY3"
      },
      "execution_count": null,
      "outputs": [],
      "id": "pYTAOfQGhfY3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find meaningful bi- and tri-grams by filtering adjectives and nouns based on an nltk functionality"
      ],
      "metadata": {
        "id": "W8JjofCAYNdC"
      },
      "id": "W8JjofCAYNdC"
    },
    {
      "cell_type": "code",
      "source": [
        "#function to filter for ADJ/NN bigrams\n",
        "def rightTypes(ngram):\n",
        "    for word in ngram:\n",
        "        if word in stopwords_ext:\n",
        "            return False\n",
        "    acceptable_types = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n",
        "    second_type = ('NN', 'NNS', 'NNP', 'NNPS')\n",
        "    tags = nltk.pos_tag(ngram)\n",
        "    if tags[0][1] in acceptable_types and tags[1][1] in second_type:\n",
        "        return True\n",
        "    else:\n",
        "        return False"
      ],
      "metadata": {
        "id": "OryUdGaYYMGs"
      },
      "execution_count": null,
      "outputs": [],
      "id": "OryUdGaYYMGs"
    },
    {
      "cell_type": "code",
      "source": [
        "#filter bigrams\n",
        "filtered_bi = bigramFreqTable[bigramFreqTable.bigram.map(lambda x: rightTypes(x))]"
      ],
      "metadata": {
        "id": "a6HFZOAafcqP"
      },
      "execution_count": null,
      "outputs": [],
      "id": "a6HFZOAafcqP"
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_bi[:10]"
      ],
      "metadata": {
        "id": "DoEO44Tzf-i8"
      },
      "execution_count": null,
      "outputs": [],
      "id": "DoEO44Tzf-i8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use advanced statistical methods like the Chi-Square to identify meaninful collocations\n",
        "https://en.wikipedia.org/wiki/Chi-squared_test"
      ],
      "metadata": {
        "id": "LGPGqH0Xj98T"
      },
      "id": "LGPGqH0Xj98T"
    },
    {
      "cell_type": "code",
      "source": [
        "# filter bigrams using chi-square\n",
        "bigramChiTable = pd.DataFrame(list(bigramFinder.score_ngrams(bigrams.chi_sq)), columns=['bigram','chi-sq']).sort_values(by='chi-sq', ascending=False)\n",
        "bigramChiTable.head()"
      ],
      "metadata": {
        "id": "j8IPPymTjaql"
      },
      "execution_count": null,
      "outputs": [],
      "id": "j8IPPymTjaql"
    },
    {
      "cell_type": "code",
      "source": [
        "# find meaningful trigrams by filtering basic frequency table\n",
        "# function to filter trigrams\n",
        "def rightTypesTri(ngram):\n",
        "    if '-pron-' in ngram or '' in ngram or ' 'in ngram or '  ' in ngram or 't' in ngram:\n",
        "        return False\n",
        "    for word in ngram:\n",
        "        if word in stopwords_ext:\n",
        "            return False\n",
        "    first_type = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n",
        "    third_type = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n",
        "    tags = nltk.pos_tag(ngram)\n",
        "    if tags[0][1] in first_type and tags[2][1] in third_type:\n",
        "        return True\n",
        "    else:\n",
        "        return False"
      ],
      "metadata": {
        "id": "W5wqPcjzh_lW"
      },
      "execution_count": null,
      "outputs": [],
      "id": "W5wqPcjzh_lW"
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_tri = trigramFreqTable[trigramFreqTable.trigram.map(lambda x: rightTypesTri(x))]\n",
        "filtered_tri[:10]"
      ],
      "metadata": {
        "id": "3vQu3yv2jGRT"
      },
      "execution_count": null,
      "outputs": [],
      "id": "3vQu3yv2jGRT"
    },
    {
      "cell_type": "code",
      "source": [
        "# Chi-sqare frequency calculation for trigrams\n",
        "trigramChiTable = pd.DataFrame(list(trigramFinder.score_ngrams(trigrams.chi_sq)), columns=['trigram','chi-sq']).sort_values(by='chi-sq', ascending=False)\n",
        "trigramChiTable.head(20)"
      ],
      "metadata": {
        "id": "OIr3r7XBkaqF"
      },
      "execution_count": null,
      "outputs": [],
      "id": "OIr3r7XBkaqF"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}