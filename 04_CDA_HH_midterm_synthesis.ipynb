{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jazoza/cultural-data-analysis/blob/main/04_CDA_HH_midterm_synthesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cultural Data Analysis\n",
        "\n",
        "Introduction to working with datasets"
      ],
      "metadata": {
        "id": "7CC34uuNzNxY"
      },
      "id": "7CC34uuNzNxY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76a4e150-cc6f-4878-a32b-e78a1d6426ae",
      "metadata": {
        "id": "76a4e150-cc6f-4878-a32b-e78a1d6426ae"
      },
      "outputs": [],
      "source": [
        "# import necessary libraries\n",
        "import os, re, csv\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7010dda9-acc5-4d90-b175-90012564d13c",
      "metadata": {
        "id": "7010dda9-acc5-4d90-b175-90012564d13c"
      },
      "source": [
        "## Loading the datasets: heritage homes webistes\n",
        "\n",
        "The dataset is stored in a shared google drive:\n",
        "https://drive.google.com/drive/folders/11Shm0edDOiWrOe56fzJQRZi-v_BPSW8E?usp=drive_link\n",
        "\n",
        "Add it to your drive.\n",
        "\n",
        "To access it, load your gdrive in 'Files' (see left pane of the notebook in google colab) and navigate to the shared folder. You may need to click on 'refresh' to make it appear on the list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d42429d3-63fe-4b79-b341-160057e5dcbc",
      "metadata": {
        "id": "d42429d3-63fe-4b79-b341-160057e5dcbc"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the github repository files where this notebook is stored to the available files. This will make it easier to import stopwords, url lists and other additional data we need."
      ],
      "metadata": {
        "id": "_WYOlRWhaBwf"
      },
      "id": "_WYOlRWhaBwf"
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/jazoza/cultural-data-analysis"
      ],
      "metadata": {
        "id": "F-MaBtlTaAgI"
      },
      "id": "F-MaBtlTaAgI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import all datasets (4 countries)\n",
        "\n",
        "You will have all datasets available for analysis and comparison, mapped in the following way:\n",
        "\n",
        "> df0 - Dutch dataset\n",
        "\n",
        "> df1 - UK dataset\n",
        "\n",
        "> df2 - German dataset\n",
        "\n",
        "> df3 - French dataset"
      ],
      "metadata": {
        "id": "ukRXJgSvn-yx"
      },
      "id": "ukRXJgSvn-yx"
    },
    {
      "cell_type": "code",
      "source": [
        "# Country code: change here between 'NL' and 'UK'\n",
        "cc_list = ['NL', 'UK', 'DE', 'FR']"
      ],
      "metadata": {
        "id": "Qu6K9f2Vo6i_"
      },
      "id": "Qu6K9f2Vo6i_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gdrive_path = '/content/gdrive/MyDrive/CDA/'"
      ],
      "metadata": {
        "id": "9fnHB7XSo89i"
      },
      "id": "9fnHB7XSo89i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import scraped json data into 4 separate dataframes\n",
        "df0=pd.read_json(gdrive_path+cc_list[0]+'_dataset_website-content-crawler.json')\n",
        "# select columns for analysis: url, text, metadata\n",
        "df0=df0[['url','text','metadata']]\n",
        "\n",
        "df1=pd.read_json(gdrive_path+cc_list[1]+'_dataset_website-content-crawler.json')\n",
        "# select columns for analysis: url, text, metadata\n",
        "df1=df1[['url','text','metadata']]\n",
        "\n",
        "df2=pd.read_json(gdrive_path+cc_list[2]+'_dataset_website-content-crawler.json')\n",
        "# select columns for analysis: url, text, metadata\n",
        "df2=df2[['url','text','metadata']]\n",
        "\n",
        "df3=pd.read_json(gdrive_path+cc_list[3]+'_dataset_website-content-crawler.json')\n",
        "# select columns for analysis: url, text, metadata\n",
        "df3=df3[['url','text','metadata']]\n",
        "\n",
        "df0.head()\n"
      ],
      "metadata": {
        "id": "8K9APfwUo4Up"
      },
      "id": "8K9APfwUo4Up",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Join all pages from a domain to an entry in the analysis. To do this, add a new column which will contain only the main domain name."
      ],
      "metadata": {
        "id": "7yB3vYA7pGpq"
      },
      "id": "7yB3vYA7pGpq"
    },
    {
      "cell_type": "code",
      "source": [
        "# function to extract the main domain from the url in the dataset\n",
        "def extract_main_domain(url):\n",
        "    if not isinstance(str(url), str):\n",
        "        print('NOT VALID',url)\n",
        "        return None\n",
        "    match = re.findall('(?:\\\\w+\\\\.)*\\\\w+\\\\.\\\\w*', str(url)) #'www\\.?([^/]+)'\n",
        "    return match[0].lstrip('www.') if match else None"
      ],
      "metadata": {
        "id": "sczGS62Oo8z6"
      },
      "id": "sczGS62Oo8z6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a new column 'domain' and fill it by applying the extract_main_domain function to the 'url' column\n",
        "\n",
        "# first, create a mapping of dataframes which could be addressed in a loop\n",
        "df_dict = {'0':df0, '1':df1, '2':df2, '3':df3}\n",
        "\n",
        "# then, loop through the df_dict to update each dataframe\n",
        "for k, v in df_dict.items():\n",
        "  cc_column = cc_list[int(k[-1])]+' domains'\n",
        "  cc = cc_list[int(k[-1])]\n",
        "  # print(cc_column, cc)\n",
        "  urls = pd.read_csv(gdrive_path+'url_lists/'+cc_list[int(k[-1])]+'_urls.csv')[cc_column].values.tolist()\n",
        "  domains = {extract_main_domain(url) for url in urls if extract_main_domain(url) is not None}\n",
        "  matching_links = [link for link in v.url if extract_main_domain(link) in domains]\n",
        "  # update the dataframe\n",
        "  v['domain'] = v['url'].apply(extract_main_domain)\n",
        "\n",
        "# check one of the dataframes\n",
        "df1.head()"
      ],
      "metadata": {
        "id": "dFCVEXuJpF_U"
      },
      "id": "dFCVEXuJpF_U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare the analysis\n",
        "\n",
        "Import stopwords dictionaries for the 4 langauges we work with.\n",
        "It is good to import all of them in our case, because many websites have sections is English, German or French even when this is not the main language of the website."
      ],
      "metadata": {
        "id": "inCepASp67ru"
      },
      "id": "inCepASp67ru"
    },
    {
      "cell_type": "code",
      "source": [
        "# load a list of 'stopwords' function\n",
        "def get_stopwords_list(stop_file_path):\n",
        "    \"\"\"load stop words \"\"\"\n",
        "    with open(stop_file_path, 'r', encoding=\"utf-8\") as f:\n",
        "        stopwords = f.readlines()\n",
        "        stop_set = set(m.strip() for m in stopwords)\n",
        "        return list(frozenset(stop_set))"
      ],
      "metadata": {
        "id": "_bCxFJzY69_b"
      },
      "id": "_bCxFJzY69_b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the stopwords list for all languages (using cc_list previously defined)\n",
        "# cc_list = ['NL', 'UK', 'DE', 'FR'] # remove the hashtag from this line to uncomment this code and make it run\n",
        "\n",
        "stopwords = [] # empty list to which a list of stopwords will be appended in loop\n",
        "\n",
        "for i in range(len(cc_list)):\n",
        "  stopwords_cc_path = \"/content/cultural-data-analysis/stopwords_archive/\"+cc_list[i]+\".txt\"\n",
        "  stopwords_cc = get_stopwords_list(stopwords_cc_path)\n",
        "  #print(len(stopwords_cc)) # print how many words are in the list\n",
        "  stopwords.extend(stopwords_cc)\n",
        "\n",
        "#print(len(stopwords)) # print how many words are in all stopwords lists"
      ],
      "metadata": {
        "id": "kP-WH-JMXzeM"
      },
      "id": "kP-WH-JMXzeM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# you may need to include additional words which you notice as too frequent\n",
        "special_stop_words = ['nbsp', 'nl', 'fr', 'de', 'uk', 'com', 'www', 'lit'] # these might appear frequently as 'terms' in the corpus, so it's good to filter them\n",
        "stopwords_ext = stopwords+special_stop_words"
      ],
      "metadata": {
        "id": "HgNkZORH75QF"
      },
      "id": "HgNkZORH75QF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b5238d41-a6b8-48db-84d9-f6d3bb1054ad",
      "metadata": {
        "id": "b5238d41-a6b8-48db-84d9-f6d3bb1054ad"
      },
      "source": [
        "## 1. Term frequency\n",
        "\n",
        "The cells below will compute a term-matrix and calculate the frequency of each unique word (token) in the corpus\n",
        "\n",
        "This can be done for ALL words in the corpus, or ALL MEANINGFUL words (without so-called stop-words like 'the' or 'het')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b7d7865-db1f-42bc-97a0-f901e371a5e5",
      "metadata": {
        "id": "3b7d7865-db1f-42bc-97a0-f901e371a5e5"
      },
      "outputs": [],
      "source": [
        "# CALCULATE TERM FREQUENCY OF ALL TERMS\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# convert the text documents into a matrix of token (word) counts\n",
        "cvec_all = CountVectorizer().fit(df0.text) #### CHANGE df0 TO THE DATAFRAME YOU ANALYSE\n",
        "df_matrix_all = cvec_all.transform(df0.text) #### CHANGE df0 TO THE DATAFRAME YOU ANALYSE\n",
        "df_all = np.sum(df_matrix_all,axis=0)\n",
        "terms = np.squeeze(np.asarray(df_all))\n",
        "# print the 'shape' of the matrix - it should indicate the number of unique terms\n",
        "print(terms.shape)\n",
        "term_freq_df_all = pd.DataFrame([terms],columns=cvec_all.get_feature_names_out()).transpose() #term_freq_df is with stopwords\n",
        "term_freq_df_all.columns = ['terms']\n",
        "# show the first ten words [:10];\n",
        "# change the values in the brackets to show 30th-40th words [30:40]\n",
        "# or show the last ten words [:-10]\n",
        "term_freq_df_all.sort_values(by='terms', ascending=False).iloc[:-10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e317c18-cf12-48e6-a4db-57b3a99fde06",
      "metadata": {
        "id": "4e317c18-cf12-48e6-a4db-57b3a99fde06"
      },
      "outputs": [],
      "source": [
        "# CALCULATE TERM FREQUENCY WITHOUT STOP-WORDS\n",
        "\n",
        "#cvec_stopped = CountVectorizer(max_df=0.5, token_pattern=r'(?u)\\b[A-Za-z]{2,}\\b') # max_df could in theory automatically filter stopwords\n",
        "cvec_stopped = CountVectorizer(stop_words=stopwords_ext, token_pattern=r'(?u)\\b[A-Za-z]{2,}\\b') # token pattern recognizes only words which are made of letters, and longer than 1 character\n",
        "cvec_stopped.fit(df0.text) #### CHANGE df0 TO THE DATAFRAME YOU ANALYSE\n",
        "document_matrix = cvec_stopped.transform(df0.text) #### CHANGE df0 TO THE DATAFRAME YOU ANALYSE\n",
        "term_batches = np.linspace(0,document_matrix.shape[0],10).astype(int)\n",
        "i=0\n",
        "df_stopped = []\n",
        "while i < len(term_batches)-1:\n",
        "    batch_result = np.sum(document_matrix[term_batches[i]:term_batches[i+1]].toarray(),axis=0)\n",
        "    df_stopped.append(batch_result)\n",
        "    print(term_batches[i+1],\"entries' term frequency calculated\")\n",
        "    i += 1\n",
        "\n",
        "terms_stopped = np.sum(df_stopped,axis=0)\n",
        "#print(terms_stopped.shape)\n",
        "term_freq_df_stopped = pd.DataFrame([terms_stopped],columns=cvec_stopped.get_feature_names_out()).transpose()\n",
        "term_freq_df_stopped.columns = ['terms']\n",
        "term_freq_df_stopped.sort_values(by='terms', ascending=False).iloc[:10]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Term frequency for specific terms"
      ],
      "metadata": {
        "id": "WxJd9MXcViMI"
      },
      "id": "WxJd9MXcViMI"
    },
    {
      "cell_type": "code",
      "source": [
        "search_word = 'kasteel' # Change this to the word you want to search for\n",
        "\n",
        "if search_word in term_freq_df_stopped.index: # check if the words exists;\n",
        "    frequency = term_freq_df_stopped.loc[search_word, 'terms']\n",
        "    print(f\"The word '{search_word}' appears {frequency} times in the current corpus.\")\n",
        "elif search_word in stopwords_ext:\n",
        "    # If not found in stopped, maybe it was a stop word, so check all terms\n",
        "    frequency = term_freq_df_all.loc[search_word, 'terms']\n",
        "    print(f\"The word '{search_word}' was filtered out as a stopword. Its total frequency is {frequency} times.\")\n",
        "else:\n",
        "    print(f\"The word '{search_word}' was not found in the corpus.\")"
      ],
      "metadata": {
        "id": "sPRXo8GKUXcA"
      },
      "id": "sPRXo8GKUXcA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "09be655e-553d-4f2f-8993-9a4fbe63c0aa",
      "metadata": {
        "id": "09be655e-553d-4f2f-8993-9a4fbe63c0aa"
      },
      "source": [
        "### 1.2 TF-IDF vectorization\n",
        "\n",
        "- What is TF/IDF (term frequency / inverse document frequency)? https://en.wikipedia.org/wiki/Tf%E2%80%93idf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "002e51e5-ccc0-41c7-9e8c-35d2c794368b",
      "metadata": {
        "id": "002e51e5-ccc0-41c7-9e8c-35d2c794368b"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize the TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(stop_words=stopwords_ext, token_pattern=r'(?u)\\b[A-Za-z]{2,}\\b')\n",
        "# Fit and transform the text data\n",
        "tfidf_matrix = vectorizer.fit_transform(df0['text']) #### CHANGE df0 TO THE DATAFRAME YOU ANALYSE\n",
        "# Convert the TF-IDF matrix to a DataFrame\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "# Add filenames as index\n",
        "tfidf_df.index = df0['domain'] #### CHANGE df0 TO THE DATAFRAME YOU ANALYSE\n",
        "# Print the TF-IDF DataFrame\n",
        "tfidf_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54b73181-9d99-4d60-a272-b4c1bb07ae8c",
      "metadata": {
        "id": "54b73181-9d99-4d60-a272-b4c1bb07ae8c"
      },
      "outputs": [],
      "source": [
        "# Function to transform the wide TF-IDF DataFrame to a long format\n",
        "def create_long_tfidf_df_efficiently(tfidf_wide_df):\n",
        "    data = []\n",
        "    for domain, row in tfidf_wide_df.iterrows():\n",
        "        # Get non-zero TF-IDF scores and their corresponding terms to reduce data processing\n",
        "        active_terms = row[row > 0]\n",
        "        for term, tfidf_score in active_terms.items():\n",
        "            data.append({'document': domain, 'term': term, 'tfidf': tfidf_score})\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Reorganize the DataFrame from wide to long format using the efficient function\n",
        "tfidf_df = create_long_tfidf_df_efficiently(tfidf_df)\n",
        "tfidf_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import altair as alt\n",
        "\n",
        "# Terms in this list will get a red dot in the visualization\n",
        "term_list = ['kasteel', 'huis', 'children'] # write key terms here"
      ],
      "metadata": {
        "id": "aHP_PD1Z9IBX"
      },
      "id": "aHP_PD1Z9IBX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4308bb87"
      },
      "source": [
        "import altair as alt\n",
        "\n",
        "# Calculate top 10 TF-IDF terms per domain\n",
        "top_tfidf_plus = tfidf_df.groupby('document').apply(lambda x: x.nlargest(10, 'tfidf')).reset_index(drop=True)\n",
        "\n",
        "# Add 'rank' based on tfidf score within each document\n",
        "top_tfidf_plus['rank'] = top_tfidf_plus.groupby('document')['tfidf'].rank(method='first', ascending=False).astype(int)\n",
        "\n",
        "# adding a little randomness to break ties in term ranking\n",
        "top_tfidf_plusRand = top_tfidf_plus.copy()\n",
        "top_tfidf_plusRand['tfidf'] = top_tfidf_plusRand['tfidf'] + np.random.rand(top_tfidf_plus.shape[0])*0.0001\n",
        "\n",
        "# Define the base Altair chart\n",
        "base = alt.Chart(top_tfidf_plusRand).encode(\n",
        "    x=alt.X('rank:O', axis=alt.Axis(title='Rank (Top 10 Terms)')),\n",
        "    y=alt.Y('document:N', axis=alt.Axis(title='Domain'))\n",
        ").transform_window(\n",
        "    rank=\"rank()\",\n",
        "    sort=[alt.SortField(\"tfidf\", order=\"descending\")],\n",
        "    groupby=[\"document\"]\n",
        ")\n",
        "\n",
        "# Create the heatmap layer\n",
        "heatmap = base.mark_rect().encode(\n",
        "    color=alt.Color('tfidf:Q', scale=alt.Scale(scheme='yellowgreenblue'), title='TF-IDF Score')\n",
        ")\n",
        "\n",
        "# Create the text layer with white text\n",
        "text = base.mark_text(baseline='middle').encode(\n",
        "    text='term:N',\n",
        "    color=alt.value('white') # Explicitly set text color to white\n",
        ")\n",
        "\n",
        "# Combine the heatmap and text layers and set properties\n",
        "chart = (heatmap + text).properties(\n",
        "    title='Top 10 TF-IDF Terms per Domain',\n",
        "    width=600,\n",
        "    height=alt.Step(25) # Adjust height based on number of documents\n",
        ")\n",
        "\n",
        "chart"
      ],
      "id": "4308bb87",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86cca17c"
      },
      "source": [
        "#inspect problems by printing the entire website (document)\n",
        "\n",
        "document = 'artland.top' # replace with one of the website domains on the left\n",
        "\n",
        "print(\"TF-IDF entries for \", document, \": \")\n",
        "display(tfidf_df[tfidf_df['document'] == document].sort_values(by='tfidf', ascending=False))\n",
        "\n",
        "print(\"\\nOriginal text entries for\", document, \": \")\n",
        "# Filter the original DataFrame 'df' for the domain 'kasteeltuinen.nl'\n",
        "domain_text = df0[df0['domain'] == document]['text'].str.cat(sep=' ')\n",
        "print(domain_text)"
      ],
      "id": "86cca17c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Word2Vec model\n",
        "\n",
        "Vectorizing the corpus with word2vec model\n",
        "https://en.wikipedia.org/wiki/Word2vec"
      ],
      "metadata": {
        "id": "xr96f76CWx4Y"
      },
      "id": "xr96f76CWx4Y"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "4RIgZXmuW6Tt"
      },
      "id": "4RIgZXmuW6Tt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "GsDntAtyX6VW"
      },
      "id": "GsDntAtyX6VW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# X is a list of tokenized texts (i.e. list of lists of tokens)\n",
        "X = [word_tokenize(item) for item in df0.text.tolist()] # replace df0 with a dataframe you are analysing\n",
        "#print(X[0:3])\n",
        "model = gensim.models.Word2Vec(X, min_count=6, vector_size=200) # min_count: how many times a word appears in the corpus; size: number of dimensions"
      ],
      "metadata": {
        "id": "Uhm_Fz0eXjjQ"
      },
      "id": "Uhm_Fz0eXjjQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observe keywords that may be characteristic in the corpus on heritage homes, such as 'castle', 'garden', 'party', 'princess'; try also words related to less obvious themes, like 'servant'\n",
        "\n",
        "You can ask for 'negative' or 'positive' similarity, and explore how these bring up terms that are opposite to the meaning in a variety of ways.\n"
      ],
      "metadata": {
        "id": "TxgCXzxKX_7M"
      },
      "id": "TxgCXzxKX_7M"
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar(positive=[\"kasteel\"], topn=12)"
      ],
      "metadata": {
        "id": "v7OWumJgX1E5"
      },
      "id": "v7OWumJgX1E5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar(positive=[\"tuin\"], topn=12)"
      ],
      "metadata": {
        "id": "MsfSD5tNYIns"
      },
      "id": "MsfSD5tNYIns",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar(negative=[\"baron\"], topn=12)"
      ],
      "metadata": {
        "id": "6VVnnjwFYNF6"
      },
      "id": "6VVnnjwFYNF6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Collocations"
      ],
      "metadata": {
        "id": "2nFPgMYkiygf"
      },
      "id": "2nFPgMYkiygf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Analyze specific collocations"
      ],
      "metadata": {
        "id": "COojp_VZphQW"
      },
      "id": "COojp_VZphQW"
    },
    {
      "cell_type": "code",
      "source": [
        "# define vectorization functions\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from collections import Counter\n",
        "from itertools import islice\n",
        "\n",
        "# SCI-KIT method, produces lists of co-occurencies for specific terms\n",
        "def vectorize_text(df):\n",
        "    vectorizer = CountVectorizer()\n",
        "    X = vectorizer.fit_transform(df['text'])\n",
        "    return X, vectorizer\n",
        "\n",
        "def find_collocations(text, target_words):\n",
        "    words = text.split()\n",
        "    collocations = []\n",
        "    for i in range(len(words) - 1):\n",
        "        if words[i] in target_words:\n",
        "            collocations.append((words[i], words[i + 1]))\n",
        "        if words[i + 1] in target_words:\n",
        "            collocations.append((words[i + 1], words[i]))\n",
        "    return collocations\n",
        "\n",
        "def get_frequent_collocations(df, most_frequent_words):\n",
        "    collocations = []\n",
        "    for text in df['text']:\n",
        "        collocations.extend(find_collocations(text, most_frequent_words))\n",
        "    collocation_counts = Counter(collocations)\n",
        "    frequent_collocations = {}\n",
        "    for word in most_frequent_words:\n",
        "        word_collocations = {collocation: count for collocation, count in collocation_counts.items() if word in collocation}\n",
        "        frequent_collocations[word] = dict(islice(Counter(word_collocations).most_common(20), 20)) # change these two values to get more or less terms\n",
        "    return frequent_collocations\n",
        "\n",
        "def analyze_word_collocations(df):\n",
        "    X, vectorizer = vectorize_text(df)\n",
        "    most_frequent_words = search_words\n",
        "    frequent_collocations = get_frequent_collocations(df, most_frequent_words)\n",
        "    return frequent_collocations"
      ],
      "metadata": {
        "id": "032f9482-879c-43f9-a9cd-3b1ca5d7056c"
      },
      "execution_count": null,
      "outputs": [],
      "id": "032f9482-879c-43f9-a9cd-3b1ca5d7056c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2657cbe3-aa7e-4e4b-8d68-1d6050cf6f49"
      },
      "outputs": [],
      "source": [
        "collocations = analyze_word_collocations(df0) # CHANGE df0 TO DATAFRAME YOU ARE ANALYSING"
      ],
      "id": "2657cbe3-aa7e-4e4b-8d68-1d6050cf6f49"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the search term here, analyse whether it appears in the corpus and next to which words (excluding stopwords)"
      ],
      "metadata": {
        "id": "NGX6nGd5YpOH"
      },
      "id": "NGX6nGd5YpOH"
    },
    {
      "cell_type": "code",
      "source": [
        "# search for words from this list or use another list\n",
        "search_words = ['kasteel']"
      ],
      "metadata": {
        "id": "LoJnS9K7h5Kd"
      },
      "execution_count": null,
      "outputs": [],
      "id": "LoJnS9K7h5Kd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fbef5d2-983b-49fa-8924-95abd24b2855"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "for word, colloc_dict in collocations.items():\n",
        "   for collocation, count in colloc_dict.items():\n",
        "       #collocation_str = ' '.join(collocation)  # Join collocation words into a single string\n",
        "       data.append([word, collocation[1], count])\n",
        "collocations_df = pd.DataFrame(data, columns=['Word', 'Collocation', 'Count'])\n",
        "print(collocations_df.to_markdown(index=True))"
      ],
      "id": "4fbef5d2-983b-49fa-8924-95abd24b2855"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.1.1. Analyze collocation in page titles"
      ],
      "metadata": {
        "id": "EP4227iyZM7m"
      },
      "id": "EP4227iyZM7m"
    },
    {
      "cell_type": "code",
      "source": [
        "# add a column 'page_title' to th dataframe (df0 or df1-3) extracting the value of 'title' key in metadata dictionary in each entry (lambda function)\n",
        "\n",
        "df0['page_title'] = df0['metadata'].apply(lambda x: x.get('title'))\n",
        "df0['page_title'].head()"
      ],
      "metadata": {
        "id": "lwu955g2ZT1e"
      },
      "id": "lwu955g2ZT1e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_term = 'baron'\n",
        "total_occurrences = 0\n",
        "\n",
        "print(\"Searching for\", search_term, \":\")\n",
        "for index, title in df0['page_title'].items():\n",
        "    if isinstance(title, str):\n",
        "        # Convert to lowercase for case-insensitive search\n",
        "        title_lower = title.lower()\n",
        "        # Count occurrences in the current title\n",
        "        occurrences_in_title = title_lower.count(search_term)\n",
        "        if occurrences_in_title > 0:\n",
        "            print(title)\n",
        "            total_occurrences += occurrences_in_title\n",
        "\n",
        "print(\"\\nTotal occurrences of\", search_term, \"across all titles: \", total_occurrences)"
      ],
      "metadata": {
        "id": "VFJU-i7BZbJN"
      },
      "id": "VFJU-i7BZbJN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08144921-d7be-45ed-8795-1c085fb2640b"
      },
      "source": [
        "### 2.2 Analyse collocations in sentences"
      ],
      "id": "08144921-d7be-45ed-8795-1c085fb2640b"
    },
    {
      "cell_type": "code",
      "source": [
        "#function to remove non-ascii characters\n",
        "def _removeNonAscii(s): return \"\".join(i for i in s if ord(i)<128)"
      ],
      "metadata": {
        "id": "0SuPUQ_4VrfT"
      },
      "execution_count": null,
      "outputs": [],
      "id": "0SuPUQ_4VrfT"
    },
    {
      "cell_type": "code",
      "source": [
        "# import the advanced Natural Language Processing (NLP) library\n",
        "# which we will use to analze the grammatical structure\n",
        "import spacy"
      ],
      "metadata": {
        "id": "ijzGDgoRAHN3"
      },
      "id": "ijzGDgoRAHN3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download the suitable language pipeline\n",
        "# Dutch: nl_core_news_sm\n",
        "# French: fr_core_news_sm\n",
        "# German: de_core_news_sm\n",
        "# English: en_core_web_sm (available by default)\n",
        "!python -m spacy download nl_core_news_sm"
      ],
      "metadata": {
        "id": "55G_5i9IAs1b"
      },
      "id": "55G_5i9IAs1b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('nl_core_news_sm') # change to FR/DE/EN code module, see names above"
      ],
      "metadata": {
        "id": "pqCDLM2KAce0"
      },
      "id": "pqCDLM2KAce0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "#function to clean and lemmatize comments\n",
        "def clean_documents(text):\n",
        "    #remove punctuations\n",
        "    regex = re.compile('[' + re.escape(string.punctuation) + '\\\\r\\\\t\\\\n]')\n",
        "    nopunct = regex.sub(\" \", str(text))\n",
        "    #use spacy to lemmatize comments\n",
        "    doc = nlp(nopunct, disable=['parser','ner'])\n",
        "    lemma = [token.lemma_ for token in doc]\n",
        "    return lemma"
      ],
      "metadata": {
        "id": "yeR7duSYWD9p"
      },
      "execution_count": null,
      "outputs": [],
      "id": "yeR7duSYWD9p"
    },
    {
      "cell_type": "code",
      "source": [
        "#apply function to clean and lemmatize comments\n",
        "lemmatized = df.text.map(clean_documents)\n",
        "#make sure to lowercase everything\n",
        "lemmatized = lemmatized.map(lambda x: [word.lower() for word in x])\n",
        "lemmatized.head()"
      ],
      "metadata": {
        "id": "7VsB2yq1WObL"
      },
      "execution_count": null,
      "outputs": [],
      "id": "7VsB2yq1WObL"
    },
    {
      "cell_type": "code",
      "source": [
        "unlist_documents = [item for items in lemmatized for item in items]"
      ],
      "metadata": {
        "id": "IS3V0pl3Xksv"
      },
      "execution_count": null,
      "outputs": [],
      "id": "IS3V0pl3Xksv"
    },
    {
      "cell_type": "code",
      "source": [
        "# You would use these commands to save lemmatized text into a 'pickle' for later use\n",
        "# The current setup does not enable you to overwrite existing files in the CDA/jar folder,\n",
        "# so you would have to save the 'pickle' files elsewhere (for example in sample_data folder)\n",
        "# If you want to reactivate this code, remove the tripple quotes ''' from the beginning and end\n",
        "'''\n",
        "# save these outputs for later\n",
        "with open(gdrive_path+'jar/lemmatized.pickle', 'wb') as handle_l:\n",
        "    pickle.dump(lemmatized, handle_l, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open(gdrive_path+'jar/unlist_documents.pickle', 'wb') as handle_u:\n",
        "    pickle.dump(unlist_documents, handle_u, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "  '''"
      ],
      "metadata": {
        "id": "VGoew_tHZLpQ"
      },
      "execution_count": null,
      "outputs": [],
      "id": "VGoew_tHZLpQ"
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# load saved pickles\n",
        "with open(gdrive_path+'jar/'+cc+'_lemmatized.pickle', 'rb') as handle_l:\n",
        "    lemmatized = pickle.load(handle_l)\n",
        "\n",
        "with open(gdrive_path+'jar/'+cc+'_unlist_documents.pickle', 'rb') as handle_u:\n",
        "    unlist_documents = pickle.load(handle_u)\n",
        "'''"
      ],
      "metadata": {
        "id": "5aBr85g3ajFB"
      },
      "execution_count": null,
      "outputs": [],
      "id": "5aBr85g3ajFB"
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "id": "Hbkku34SfpU1"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Hbkku34SfpU1"
    },
    {
      "cell_type": "code",
      "source": [
        "# initiate bigrams and trigrams\n",
        "bigrams = nltk.collocations.BigramAssocMeasures()\n",
        "trigrams = nltk.collocations.TrigramAssocMeasures()"
      ],
      "metadata": {
        "id": "3IaXW0GXX4X2"
      },
      "execution_count": null,
      "outputs": [],
      "id": "3IaXW0GXX4X2"
    },
    {
      "cell_type": "code",
      "source": [
        "# identify all collocations in the flat list of words from all documents\n",
        "bigramFinder = nltk.collocations.BigramCollocationFinder.from_words(unlist_documents)\n",
        "trigramFinder = nltk.collocations.TrigramCollocationFinder.from_words(unlist_documents)"
      ],
      "metadata": {
        "id": "6L2vI56MX9ps"
      },
      "execution_count": null,
      "outputs": [],
      "id": "6L2vI56MX9ps"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate basic frequency"
      ],
      "metadata": {
        "id": "mRSuATcHYm0U"
      },
      "id": "mRSuATcHYm0U"
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_freq = bigramFinder.ngram_fd.items()"
      ],
      "metadata": {
        "id": "W9XCfughYim0"
      },
      "execution_count": null,
      "outputs": [],
      "id": "W9XCfughYim0"
    },
    {
      "cell_type": "code",
      "source": [
        "bigramFreqTable = pd.DataFrame(list(bigram_freq), columns=['bigram','freq']).sort_values(by='freq', ascending=False)"
      ],
      "metadata": {
        "id": "4dCLatTKfJDe"
      },
      "execution_count": null,
      "outputs": [],
      "id": "4dCLatTKfJDe"
    },
    {
      "cell_type": "code",
      "source": [
        "bigramFreqTable.head().reset_index(drop=True)"
      ],
      "metadata": {
        "id": "NHE_sQvofMKw"
      },
      "execution_count": null,
      "outputs": [],
      "id": "NHE_sQvofMKw"
    },
    {
      "cell_type": "code",
      "source": [
        "# compute basic trigrams frequency\n",
        "trigram_freq = trigramFinder.ngram_fd.items()\n",
        "trigramFreqTable = pd.DataFrame(list(trigram_freq), columns=['trigram','freq']).sort_values(by='freq', ascending=False)\n",
        "trigramFreqTable[:10]"
      ],
      "metadata": {
        "id": "pYTAOfQGhfY3"
      },
      "execution_count": null,
      "outputs": [],
      "id": "pYTAOfQGhfY3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find meaningful bi- and tri-grams by filtering adjectives and nouns based on an nltk functionality"
      ],
      "metadata": {
        "id": "W8JjofCAYNdC"
      },
      "id": "W8JjofCAYNdC"
    },
    {
      "cell_type": "code",
      "source": [
        "#function to filter for ADJ/NN bigrams\n",
        "def rightTypes(ngram):\n",
        "    for word in ngram:\n",
        "        if word in stopwords_ext:\n",
        "            return False\n",
        "    acceptable_types = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n",
        "    second_type = ('NN', 'NNS', 'NNP', 'NNPS')\n",
        "    tags = nltk.pos_tag(ngram)\n",
        "    if tags[0][1] in acceptable_types and tags[1][1] in second_type:\n",
        "        return True\n",
        "    else:\n",
        "        return False"
      ],
      "metadata": {
        "id": "OryUdGaYYMGs"
      },
      "execution_count": null,
      "outputs": [],
      "id": "OryUdGaYYMGs"
    },
    {
      "cell_type": "code",
      "source": [
        "#filter bigrams\n",
        "filtered_bi = bigramFreqTable[bigramFreqTable.bigram.map(lambda x: rightTypes(x))]"
      ],
      "metadata": {
        "id": "a6HFZOAafcqP"
      },
      "execution_count": null,
      "outputs": [],
      "id": "a6HFZOAafcqP"
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_bi[:10]"
      ],
      "metadata": {
        "id": "DoEO44Tzf-i8"
      },
      "execution_count": null,
      "outputs": [],
      "id": "DoEO44Tzf-i8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use advanced statistical methods like the Chi-Square to identify meaninful collocations\n",
        "https://en.wikipedia.org/wiki/Chi-squared_test"
      ],
      "metadata": {
        "id": "LGPGqH0Xj98T"
      },
      "id": "LGPGqH0Xj98T"
    },
    {
      "cell_type": "code",
      "source": [
        "# filter bigrams using chi-square\n",
        "bigramChiTable = pd.DataFrame(list(bigramFinder.score_ngrams(bigrams.chi_sq)), columns=['bigram','chi-sq']).sort_values(by='chi-sq', ascending=False)\n",
        "bigramChiTable.head()"
      ],
      "metadata": {
        "id": "j8IPPymTjaql"
      },
      "execution_count": null,
      "outputs": [],
      "id": "j8IPPymTjaql"
    },
    {
      "cell_type": "code",
      "source": [
        "# find meaningful trigrams by filtering basic frequency table\n",
        "# function to filter trigrams\n",
        "def rightTypesTri(ngram):\n",
        "    if '-pron-' in ngram or '' in ngram or ' 'in ngram or '  ' in ngram or 't' in ngram:\n",
        "        return False\n",
        "    for word in ngram:\n",
        "        if word in stopwords_ext:\n",
        "            return False\n",
        "    first_type = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n",
        "    third_type = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n",
        "    tags = nltk.pos_tag(ngram)\n",
        "    if tags[0][1] in first_type and tags[2][1] in third_type:\n",
        "        return True\n",
        "    else:\n",
        "        return False"
      ],
      "metadata": {
        "id": "W5wqPcjzh_lW"
      },
      "execution_count": null,
      "outputs": [],
      "id": "W5wqPcjzh_lW"
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_tri = trigramFreqTable[trigramFreqTable.trigram.map(lambda x: rightTypesTri(x))]\n",
        "filtered_tri[:10]"
      ],
      "metadata": {
        "id": "3vQu3yv2jGRT"
      },
      "execution_count": null,
      "outputs": [],
      "id": "3vQu3yv2jGRT"
    },
    {
      "cell_type": "code",
      "source": [
        "# Chi-sqare frequency calculation for trigrams\n",
        "trigramChiTable = pd.DataFrame(list(trigramFinder.score_ngrams(trigrams.chi_sq)), columns=['trigram','chi-sq']).sort_values(by='chi-sq', ascending=False)\n",
        "trigramChiTable.head(20)"
      ],
      "metadata": {
        "id": "OIr3r7XBkaqF"
      },
      "execution_count": null,
      "outputs": [],
      "id": "OIr3r7XBkaqF"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}