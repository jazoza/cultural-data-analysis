{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Cultural Data Analysis\n",
        "\n",
        "Introduction to working with datasets"
      ],
      "metadata": {
        "id": "7CC34uuNzNxY"
      },
      "id": "7CC34uuNzNxY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76a4e150-cc6f-4878-a32b-e78a1d6426ae",
      "metadata": {
        "id": "76a4e150-cc6f-4878-a32b-e78a1d6426ae"
      },
      "outputs": [],
      "source": [
        "# import necessary libraries\n",
        "import os, re, csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gensim, nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from collections import Counter\n",
        "from itertools import islice"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7010dda9-acc5-4d90-b175-90012564d13c",
      "metadata": {
        "id": "7010dda9-acc5-4d90-b175-90012564d13c"
      },
      "source": [
        "## Loading the dataset: heritage homes webistes\n",
        "\n",
        "The dataset is stored in a shared google drive:\n",
        "https://drive.google.com/drive/folders/11Shm0edDOiWrOe56fzJQRZi-v_BPSW8E?usp=drive_link\n",
        "\n",
        "Add it to your drive.\n",
        "\n",
        "To access it, load your gdrive in 'Files' (see left pane of the notebook in google colab) and navigate to the shared folder. You may need to click on 'refresh' to make it appear on the list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d42429d3-63fe-4b79-b341-160057e5dcbc",
      "metadata": {
        "id": "d42429d3-63fe-4b79-b341-160057e5dcbc"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Country code: change here between 'NL' and 'UK'\n",
        "cc = 'NL'"
      ],
      "metadata": {
        "id": "QYiHwjcORrPC"
      },
      "execution_count": null,
      "outputs": [],
      "id": "QYiHwjcORrPC"
    },
    {
      "cell_type": "code",
      "source": [
        "gdrive_path = '/content/gdrive/MyDrive/CDA/'"
      ],
      "metadata": {
        "id": "bbjhZ8nKZtZC"
      },
      "execution_count": null,
      "outputs": [],
      "id": "bbjhZ8nKZtZC"
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data_file = gdrive_path+cc+'_dataset_website-content-crawler.json'"
      ],
      "metadata": {
        "id": "9lTKfHVaSZqs"
      },
      "execution_count": null,
      "outputs": [],
      "id": "9lTKfHVaSZqs"
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data_file = '/content/gdrive/MyDrive/CDA/NL_dataset_website-content-crawler.json'"
      ],
      "metadata": {
        "id": "yCPPY_4I2pIZ"
      },
      "id": "yCPPY_4I2pIZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8697b51f-50a5-4091-9cc1-0aed1308b27d",
      "metadata": {
        "id": "8697b51f-50a5-4091-9cc1-0aed1308b27d"
      },
      "outputs": [],
      "source": [
        "# Import json data from Aipfy scraping\n",
        "df=pd.read_json(raw_data_file)\n",
        "\n",
        "# Print the DataFrame\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a4128d6-4115-47a6-9e78-5a517bded844",
      "metadata": {
        "id": "6a4128d6-4115-47a6-9e78-5a517bded844"
      },
      "outputs": [],
      "source": [
        "# select only two columns for analysis: url and text\n",
        "df=df[['url','text']]\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Join all pages from a domain to an entry in the analysis. To do this, add a new column which will contain only the main domain name."
      ],
      "metadata": {
        "id": "Rr6hiPQ-4z5O"
      },
      "id": "Rr6hiPQ-4z5O"
    },
    {
      "cell_type": "code",
      "source": [
        "# function to extract the main domain from the url in the dataset\n",
        "def extract_main_domain(url):\n",
        "    if not isinstance(str(url), str):\n",
        "        print('NOT VALID',url)\n",
        "        return None\n",
        "    match = re.findall('(?:\\w+\\.)*\\w+\\.\\w*', str(url)) #'www\\.?([^/]+)'\n",
        "    return match[0].lstrip('www.') if match else None"
      ],
      "metadata": {
        "id": "_Px9Aoim4-pq"
      },
      "id": "_Px9Aoim4-pq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47db9deb-8836-47fb-9f74-28a023bcb5d7",
      "metadata": {
        "id": "47db9deb-8836-47fb-9f74-28a023bcb5d7"
      },
      "outputs": [],
      "source": [
        " Load the list of domains from a csv file:\n",
        "cc_column = cc+' domains'\n",
        "#print(cc_column)\n",
        "\n",
        "urls = pd.read_csv(gdrive_path+cc+'_urls.csv')[cc_column].values.tolist()\n",
        "\n",
        "# Extract main domains from nl_urls\n",
        "domains = {extract_main_domain(url) for url in urls if extract_main_domain(url) is not None}\n",
        "\n",
        "# Check if main domains in list_of_links match any domain in nl_domains\n",
        "matching_links = [link for link in df.url if extract_main_domain(link) in domains]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a2e8c0b-4c0e-4445-a5bb-2f1695ad353e",
      "metadata": {
        "id": "2a2e8c0b-4c0e-4445-a5bb-2f1695ad353e"
      },
      "outputs": [],
      "source": [
        "print(len(matching_links))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc9d2331-fe01-4e94-b984-00ac834a771c",
      "metadata": {
        "id": "cc9d2331-fe01-4e94-b984-00ac834a771c"
      },
      "outputs": [],
      "source": [
        "# Add a new column 'domain' and fill it by applying the extract_main_domain function to the 'url' column\n",
        "df['domain'] = df['url'].apply(extract_main_domain)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Working with text"
      ],
      "metadata": {
        "id": "inCepASp67ru"
      },
      "id": "inCepASp67ru"
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/jazoza/cultural-data-analysis/refs/heads/main/stopwords_archive/dutch.txt"
      ],
      "metadata": {
        "id": "rKDwducX7kqD"
      },
      "id": "rKDwducX7kqD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load a list of 'stopwords' in the language you are analyzing\n",
        "def get_stopwords_list(stop_file_path):\n",
        "    \"\"\"load stop words \"\"\"\n",
        "    with open(stop_file_path, 'r', encoding=\"utf-8\") as f:\n",
        "        stopwords = f.readlines()\n",
        "        stop_set = set(m.strip() for m in stopwords)\n",
        "        return list(frozenset(stop_set))\n",
        "stopwords_path = \"NL.txt\"\n",
        "stopwords = get_stopwords_list(stopwords_path)"
      ],
      "metadata": {
        "id": "_bCxFJzY69_b"
      },
      "id": "_bCxFJzY69_b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "special_stop_words = ['nbsp', 'the', 'and']\n",
        "stopwords_ext = stopwords+special_stop_words"
      ],
      "metadata": {
        "id": "HgNkZORH75QF"
      },
      "id": "HgNkZORH75QF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b5238d41-a6b8-48db-84d9-f6d3bb1054ad",
      "metadata": {
        "id": "b5238d41-a6b8-48db-84d9-f6d3bb1054ad"
      },
      "source": [
        "### Calculate term frequency, all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b7d7865-db1f-42bc-97a0-f901e371a5e5",
      "metadata": {
        "id": "3b7d7865-db1f-42bc-97a0-f901e371a5e5"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cvec_all = CountVectorizer().fit(df.text)\n",
        "df_matrix_all = cvec_all.transform(df.text)\n",
        "df_all = np.sum(df_matrix_all,axis=0)\n",
        "terms = np.squeeze(np.asarray(df_all))\n",
        "print(terms.shape)\n",
        "term_freq_df_all = pd.DataFrame([terms],columns=cvec_all.get_feature_names_out()).transpose() #term_freq_df is with stopwords\n",
        "term_freq_df_all.columns = ['terms']\n",
        "term_freq_df_all.sort_values(by='terms', ascending=False).iloc[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c00bf257-bb19-4481-b513-31ac70340084",
      "metadata": {
        "id": "c00bf257-bb19-4481-b513-31ac70340084"
      },
      "source": [
        "### Calculate term frequency without stop words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e317c18-cf12-48e6-a4db-57b3a99fde06",
      "metadata": {
        "id": "4e317c18-cf12-48e6-a4db-57b3a99fde06"
      },
      "outputs": [],
      "source": [
        "cvec_stopped = CountVectorizer(stop_words=stopwords_ext) # see above, import frozenset from stopwords_archive in correct language\n",
        "cvec_stopped.fit(df.text)\n",
        "document_matrix = cvec_stopped.transform(df.text)\n",
        "term_batches = np.linspace(0,document_matrix.shape[0],10).astype(int)\n",
        "i=0\n",
        "df_stopped = []\n",
        "while i < len(term_batches)-1:\n",
        "    batch_result = np.sum(document_matrix[term_batches[i]:term_batches[i+1]].toarray(),axis=0)\n",
        "    df_stopped.append(batch_result)\n",
        "    print(term_batches[i+1],\"entries' term frequency calculated\")\n",
        "    i += 1\n",
        "\n",
        "terms_stopped = np.sum(df_stopped,axis=0)\n",
        "print(terms_stopped.shape)\n",
        "term_freq_df_stopped = pd.DataFrame([terms_stopped],columns=cvec_stopped.get_feature_names_out()).transpose()\n",
        "term_freq_df_stopped.columns = ['terms']\n",
        "term_freq_df_stopped.sort_values(by='terms', ascending=False).iloc[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zipf's law"
      ],
      "metadata": {
        "id": "RGijZAqY0Bqu"
      },
      "id": "RGijZAqY0Bqu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52d2cbe9-ed7f-41c8-96e9-e8289f453ed0",
      "metadata": {
        "id": "52d2cbe9-ed7f-41c8-96e9-e8289f453ed0"
      },
      "outputs": [],
      "source": [
        "# Plot Zipf's law table\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('fivethirtyeight')\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "from pylab import *\n",
        "counts = term_freq_df_all.terms\n",
        "tokens = term_freq_df_all.index\n",
        "ranks = arange(1, len(counts)+1)\n",
        "indices = argsort(-counts)\n",
        "frequencies = counts[indices]\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.rc('font', size=14)\n",
        "plt.ylim(1,10**6)\n",
        "plt.xlim(1,10**6)\n",
        "loglog(ranks, frequencies, marker=\".\")\n",
        "plt.plot([1,frequencies[0]],[frequencies[0],1],color='r')\n",
        "title(\"Zipf plot for tweets tokens\")\n",
        "xlabel(\"Frequency rank of token\")\n",
        "ylabel(\"Absolute frequency of token\")\n",
        "grid(True)\n",
        "for n in list(logspace(-0.5, log10(len(counts)-2), 25).astype(int)):\n",
        "    dummy = text(ranks[n], frequencies[n], \" \" + tokens[indices[n]],\n",
        "                 verticalalignment=\"bottom\",\n",
        "                 horizontalalignment=\"left\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}